#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{bussproofs}
#+LATEX_HEADER: \usepackage{stmaryrd}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{newfloat}
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{lscape}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+latex_header: \usepackage[citestyle=authoryear-icomp,bibstyle=authoryear, hyperref=true,backref=true,maxcitenames=3,url=true,backend=biber,natbib=true] {biblatex}
#+latex_header: \addbibresource{./references.bib}
#+LATEX_HEADER: \lstset{
#+LATEX_HEADER:  basicstyle=\ttfamily,
#+LATEX_HEADER:   mathescape
#+LATEX_HEADER: }
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \setmonofont{DejaVu Sans Mono}
#+LATEX_HEADER: \newenvironment{scprooftree}[1]%
#+LATEX_HEADER:  {\gdef\scalefactor{#1}\begin{center}\proofSkipAmount \leavevmode}%
#+LATEX_HEADER:  {\scalebox{\scalefactor}{\DisplayProof}\proofSkipAmount \end{center} }
#+LATEX_HEADER: \usepackage{esvect}
#+LATEX_HEADER:\usepackage{geometry}
#+LATEX_HEADER: \geometry{
#+LATEX_HEADER: a4paper,
#+LATEX_HEADER:   total={170mm,257mm},
#+LATEX_HEADER:   left=20mm,
#+LATEX_HEADER:   top=20mm,
#+LATEX_HEADER: }
#+LATEX_HEADER: \newcommand{\id}[1]{\text{id}_{#1}}
#+LATEX_HEADER: \newcommand{\rec}{\text{rec}}
#+LATEX_HEADER: \newcommand{\corec}{\text{corec}}
#+LATEX_HEADER: \newcommand{\TyCtx}{\enskip\textbf{TyCtx}}
#+LATEX_HEADER: \newcommand{\Ctx}{\enskip\textbf{Ctx}}
#+LATEX_HEADER: \newcommand{\D}{\AxiomC{$\mathcal{D}$}\noLine}
#+LATEX_HEADER: \newcommand{\Di}[1]{\AxiomC{$\mathcal{D}_{#1}$}\noLine}
#+LATEX_HEADER: \newcommand{\topI}[1]{\RightLabel{\textbf{($\top$-I)}}\UnaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\TyVarI}[1]{\RightLabel{\textbf{TyVar-I}}\BinaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\TyVarWeak}[1]{\RightLabel{\textbf{(TyVar-Weak)}}\BinaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\TyWeak}[1]{\RightLabel{\textbf{(Ty-Weak)}}\BinaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\TyInst}[1]{\RightLabel{\textbf{(Ty-Inst)}}\BinaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\ParamAbstr}[1]{\RightLabel{\textbf{(Param-Abstr)}}\UnaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\FPTy}{\RightLabel{\textbf{(FP-Ty)}}}
#+LATEX_HEADER: \newcommand{\Inst}[1]{\RightLabel{\textbf{(Inst)}}\BinaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\Conv}[1]{\RightLabel{\textbf{(Conv)}}\BinaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\Proj}[1]{\RightLabel{\textbf{(Proj)}}\UnaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\TermWeak}[1]{\RightLabel{\textbf{(Term-Weak)}}\BinaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\IndI}[1]{\RightLabel{\textbf{(Ind-I)}}\UnaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\CoindE}[1]{\RightLabel{\textbf{(Coind-E)}}\UnaryInfC{#1}}
#+LATEX_HEADER: \newcommand{\IndE}{\RightLabel{\textbf{(Ind-E)}}}
#+LATEX_HEADER: \newcommand{\CoindI}{\RightLabel{\textbf{(Coind-I)}}}
#+LATEX_HEADER: \newcommand{\rat}{\rightarrowtriangle}
#+LATEX_HEADER: \newenvironment{changemargin}[2]{%
#+LATEX_HEADER: \begin{list}{}{%
#+LATEX_HEADER: \setlength{\topsep}{0pt}%
#+LATEX_HEADER: \setlength{\leftmargin}{#1}%
#+LATEX_HEADER: \setlength{\rightmargin}{#2}%
#+LATEX_HEADER: \setlength{\listparindent}{\parindent}%
#+LATEX_HEADER: \setlength{\itemindent}{\parindent}%
#+LATEX_HEADER: \setlength{\parsep}{\parskip}%
#+LATEX_HEADER: }%
#+LATEX_HEADER: \item[]}{\end{list}}
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}
#+LATEX_HEADER: \newtheorem{definition}{Definition}
#+TITLE: Implementation of Type Theory based on dependent Inductive and Coinductive Types
#+AUTHOR: Florian Engel

#+begin_src elisp :exports none
  (setq org-latex-listings 'minted)
#+end_src

#+RESULTS:
: minted

#+begin_abstract
  Dependent types are a useful tool to restrict possible type even further then
  types of strongly typed languages like Haskell. This gives us further type
  safety. With them we can also proof theorems. Coinductive types allow us to
  define types by their observations rather then by their constructors. This is
  useful for infinite types like streams. In many common dependently typed
  languages , like coq and agda, we can define inductive types which depend on
  values and coinductive types but not coinductive types, which depend on values

  In this work I will first give a survey of coinductive types in this languages
  and then implement the type theory from cite:basold2016type. This type theory
  has both dependent inductive types and dependent coinductive types. In this
  type theory the dependent function space becomes definable. This leads to a
  more symmetrical approach of coinduction in dependently typed languages.
#+end_abstract

* Introduction
  Through this work I will explain coinductive types at the examples of streams
  and functions. They will be generalized to partial streams and the Pi type in
  dependently typed languages. Streams are lists which are infinitely long. They
  are useful to modelling many IO interaction. For example a chat of a text
  messenger might be infinitely long. We can never know if the chat is finished.
  This is of course limited by the hardware, but we are interested in abstract
  models. Functions are used every where in functional programming. In most of
  this languages they are first-class objects. But in languages with coinductive
  types we can define them. If we only have types which are defined through
  induction or coinduction, we get a symmetrical language. This is useful,
  because than we can change a inductive type to a coinductive one and vice
  versa. It is straight forward to add function which destruct an inductive type
  by pattern matching on the constructor. But it is hard to add a new
  constructor. We then have to add this constructor to every pattern matching
  on that type. For coinductive types its the other way around. For more on this
  see cite:binder2019decomposition

  In section [[Coinductive Types]] we will see how coinductive can be defined. hen,
  in section [[*Implementation]] we will m

* Coinductive Types
  Inductive types are defined via their constructors.  Coinductive types on
  the other hand are defined via their destructors.  In the paper cite:abel2013copatterns
  functions, which have coinductive types as their output, are implemented via
  copattern matching.  In this paper streams are defined like the following

  #+begin_example
  record Stream A = { head : A,
                      tail : Stream A }
  #+end_example

  The A in the definition should be a concrete type. The type system in the
  paper don't has dependent types. What differentiate this from regular record
  types (for example in Haskell), is the recursive field tail. So they call it a
  recursive record. In a strict language without inductive types we could never
  instantiate such a type, because to do this we already need something of type
  ~Stream A~ to fill in the field ~tail~. To remedy this the paper defines
  copattern matching. With the help of copattern matching we can define function
  which outputs expressions of type ~Stream A~. As an example we look at the
  definition of repeat. This function takes in an a of type Nat and generates an
  stream which just infinitely repeats it.

  #+begin_example
  repeat : Nat -> Stream Nat
  head (repeat x) = x
  tail (repeat x) = repeat x
  #+end_example

  As you can see copattern matching works via observations i.e. we define what
  should be the output of the fields applied to the function. This fields are
  also called observers, because we observe parts of the type. Because
  inhabitants of ~Stream~ are infinitely long we can't print out a stream.
  Because of this we also consider each expression with a coinductive type as a
  value. To get a subpart of this value we have to use observers . For example
  we can look at the third value of ~repeat 2~ via ~head (tail (tail (repeat
  2)))~ which should evaluate to 2. We can also implement a function which looks
  at the nth. value. Here it is.

  #+begin_example
  nth : Nat -> Stream A -> A
  nth 0     x = head x
  nth (S n) x = nth n (tail x)
  #+end_example

  As you can see we use ordinary pattern matching on the left hand side and
  observers on the right hand side. `nth 3 (repeat 2)` will output 2 as expected.
  Functions can also be defined via a recursive record.  It is defined like the
  following.

  #+begin_example
  record A -> B = { apply : A ~> B }
  #+end_example

  Here we differentiate between our defined function ~A -> B~ and ~~>~ in the
  destructor. Constructor application or, as is the case here, destructor
  application is not the same as function application, like in Haskell. In the
  paper ~f x~ means ~apply f x~. We will also use this convention in the
  following. In fact we already used in the definitions of the functions
  ~repeat~ and ~nth~. ~nth 0 x~ is just a nested copattern. We can also write it
  with `apply` like so: ~apply (apply nth 0) x = head x~. Here we use currying.
  So first apply is the sole observer of type ~Stream A -> A~ and the second of
  type ~Nat -> (Stream A -> A)~.

* Coinductive Types in dependent languages
  In this section we will look how coinductive types are implemented in
  dependently typed language. In dependently typed languages types can depend on
  values. The classical example for such a type is the vector. Vectors are like
  list, except their length is contained in their type. For example a vector of
  natural numbers of length 2 has type ~Vec Nat 2~. This type depends on two
  things. Namely the type ~Nat~ and the value ~2~, which is itself of type ~Nat~.
  We can define vectors in coq like follows.
  #+begin_src coq
  Inductive Vec (A : Set) : nat -> Set :=
  | Nil : Vec A 0
  | Cons : forall {k : nat}, A -> Vec A k -> Vec A (S k).
  #+end_src
  A Vector has two constructors.  One for the empty vector called ~Nil~ and one to append a
  element at the front of a vector called ~Cons~.  The difference to list is the second argument
  to the type constructor ~Vec~. It is 0 for ~Nil~.  And ~Cons~ gets an ~A~ and a vector of length ~k~.  It
  returns a vector of length ~S k~ (~S~ is just the successor of k).
  They can also be defined in agda like follows.
  #+begin_src agda
  data Vec (A : Set) : ℕ → Set where
    Nil : Vec A 0
    Cons : {k : ℕ} → A → Vec A k → Vec A (suc k)
  #+end_src
  One advantage over of vectors over list is that we can define a total function
  (a function which is defined for every input) which takes the head of a
  vector. This function can't be total for lists, because we can't know if the
  input list is empty. A empty list has no head. For vectors we can enforce in
  coq like follow.
  #+begin_src coq
  Definition hd {A : Set} {k : nat} (v : Vec A (S k)) : A :=
    match v with
    | Cons _ x _ => x
    end.
  #+end_src
  We just pattern match on ~v~.  The only patter is for the ~Cons~ constructor.  The ~Nil~ constructor
  is a vector of length 0.  But ~v~ has type ~Vec A (S k)~.  So it can't be a vector of length 0.
  In agda the function looks like follow.
  #+begin_src agda
  hd : {A : Set} {k : ℕ} → Vec A (suc k) → A
  hd (cons x _) = x
  #+end_src
  That terms can occur in types makes it necessary to ensure that function
  terminate. Otherwise type checking wouldn't be decidable. If we have a
  function ~f : Nat -> Nat~ and we want to check a value ~a~ against a type ~Vec
  (f 1)~ we have to know what ~f 1~ evaluates to. So ~f~ has to terminate.  We check
  termination in coq via a structural decreasing argument.  A argument is structural decreasing, if
  it is structural smaller in a recursive call.  Structural smaller means it is a recursive occurrence
  in a constructor.  As an example we look at the definition of the natural numbers and the add function
  on them.  We define the natural numbers in coq like follows.
  #+begin_src coq
  Inductive nat : Set :=
  | O : nat
  | S : nat -> nat.
  #+end_src
  ~O~ is the constructor for 0 and ~S~ is the successor of its argument. Here
  the recursive argument to ~S~ is structural smaller than S applied to it i.e.
  ~n~ is structural smaller than ~S n~. Then we can define addition like follows
  #+begin_src coq
  Fixpoint add (n m:nat) : nat :=
  match n with
  | O => m
  | S p => S (add p m)
  end
  #+end_src
  In the recursive call the first argument is structural decreasing. ~p~ is
  smaller than ~s p~. So coq accepts this definition.  The classical example
  for a function where a argument is decreasing, but not structural decreasing
  is quicksort.  A naive implementation would be the following.
  #+begin_src coq
  Fixpoint quicksort (l : list nat) : list nat :=
  match l with
  | nil => nil
  | cons x xs => match split x xs with
                | (lower, upper) => app (quicksort lower) (cons x (quicksort upper))
                end
  end.
  #+end_src
  Here ~split~ is just a function which gets a number and a list of numbers.
  It gives back a pair of two lists where the left list are all elements of
  the input list which are smaller than the input number and the right this
  which are bigger.  It is clear that this lists can't be longer than the
  input list.  So ~lower~ and ~upper~ can't be longer than ~xs~.  Here ~xs~ is
  structural smaller than the input ~cons x xs~.  So ~lower~ and ~upper~ are smaller
  than the input.  Therefore we know that ~quicksort~ is terminating.  But coq won't
  accept our code, because no argument is structural decreasing.


  For coinductive types termination means that functions which produce them
  should be productive. If a function is productive it produces in each step a
  new part of the infinitely large coinductive type.

  In section [[Coinductive Types in Coq]] we will look at the implementation in coq.
  There are two ways to define them. The older way uses positive coinductive
  types. This is known to violate subject reduction. Therefore it is highly
  discouraged to use them. To fix this the new way uses negative coinductive
  types the new way uses negative coinductive types. In section [[Coinductive
  Types in Agda]] we look at the implementation in agda. Agda also has the two
  ways of defining such types. On special thing about it, is that it implements
  copattern matching. To help agda with termination checking we can use sized
  types.
** Coinductive Types in Coq
   There are two approaches to define coinductive types in coq. The older one is
   described in [[Postive Coinductive Types]]. It works over constructors. Therefore
   they are called positive coinductive types. The newer and recommended one is
   described in section [[Negative Coinductive Types]]. They are defined over
   primitive records (a relatively new feature of coq). Therefore they are
   called negative coinductive Types.

*** Postive Coinductive Types
   Positive coinductive types are defined over constructors in coq.  The keyword
   ~CoInductive~ is used to indicate that we about to define a coinductive type.
   This is the only syntactical difference from the definition of inductive
   types. For example streams are defined like the following.

   #+begin_src coq
     CoInductive Stream (A:Set): Set :=
       Cons : A -> Stream A -> Stream A.
   #+end_src

   If this was a inductive type we couldn't generate of this type.  To generate values
   of coinductive types coq uses guarded recursion.  This checks if the recursive call
   to the function occurs as a argument to a coinductive constructor.  In addition to the
   guard condition the constructor can only nested in other constructors, fun or match
   expressions.  With all of this in mind we can define
   ~repeat~ like the following.

   #+begin_src coq
     CoFixpoint repeat (A:Set) (x:A) : Stream A := Cons A x (repeat x).
   #+end_src

   Then we can produce the constant zero stream with ~repeat nat 0~. If we used
   a normal coq function i.e. write ~Fixpoint~ instead of ~CoFixpoint~ coq
   wouldn't except our code. It rejects it, because there is no argument which
   is structural decreasing. ~x~ stays always the same. ~CoFixpoint~ on the
   other hand only checks the previously mentioned conditions. It sees the
   recursive call ~repeat A x~ occurs as an argument to constructor ~LCons~ of
   the coinductive type ~Stream~. This constructor is also not nested. So our
   definition is accepted.

   We can use the normal pattern matching of coq to destruct a coinductive type.
   We define ~nth~ like the following.

   #+begin_src coq
     Fixpoint nth (A:Set) (n:Nat) (s:Stream A) {struct n} : A :=
       match l with
         Cons _ a l' =>
         match n with 0 => a | S p => nth A p l' end
       end.
   #+end_src

   The guard condition is necessary to ensure every expression is terminating.
   If we didn't have the guard condition we could define the following.

   #+begin_src coq
     CoFixpoint loop (A : Set) : Stream A = loop A.
   #+end_src

   Here the recursive call doesn't occur in a constructor.  So the guard
   condition is violated.  With this definition the expression ~nth 0 loop~
   wouldn't terminate.  ~nth~ would try to pattern match on ~loop~.  But to
   succeed in that ~loop~ has to come has to unfold to something of the form
   ~Cons a ?~ which it never does.  So ~nth 0 loop~ will never evaluate to a
   value.  This would lead to undecidable type checking.

   We illustrate the purpose of the other conditions on a example taken from
   cite:chlipala2013certified.  First we implement the function ~tl~ like so.

   #+begin_src coq
     Definition tl A (s : Stream A) : Stream A :=
       match s with
       | Cons _ _ s' => s'
       end.
   #+end_src

   This is just one normal pattern match on ~Stream~.  If we didn't had the
   other condition we could define the following.

   #+begin_src coq
     CoFixpoint bad : Stream nat := tl nat (Cons nat 0 bad).
   #+end_src

   This doesn't violate the guard condition.  The recursive call ~bad~ is a
   argument to the constructor ~Cons~.  But the constructor is nested in a
   function.  If we would allow this, ~nth 0 bad~ would loop forever.  To
   understand why, we first unfold ~tl~ in ~bad~.  So we get

   #+begin_src coq
     nth 0 (cofix bad : Stream nat :=
              match (Cons 0 bad) with
              | Cons _ s' => s'
              end)
   #+end_src

   We can now simplify this to just

   #+begin_src coq
     nth 0 (cofix bad : Stream nat := bad)
   #+end_src

   After that ~bad~ isn't anymore an argument to a constructor.  Here we can also
   see easily that the expression ~cofix bad : Stream nat := bad~ loops for ever.
   So we never get the value at position ~0~.

   An important property of typed languages is subject reduction. Subject
   reduction says if we evaluate a expression $e_1$ of type $t$ to a expression
   $e_2$, $e_2$ should also be of type $t$. With positive coinductive types subject
   reduction is no longer valid. We illustrate this by Oury's counterexample
   cite:oury2008. First we define the codata type ~U~ as follows

   #+begin_src coq
    CoInductive U : Set := In : U -> U.
   #+end_src

   We can now define a value of u with the following ~Cofixpoint~ like so

   #+begin_src coq
     CoFixpoint u : U := In u.
   #+end_src

   This generates an infinite succession of ~In~.  We use the function ~force~
   to force ~U~ to evaluate one step i.e. ~x~ becomes ~In y~

   #+begin_src coq
     Definition force (x: U) : U :=
       match x with
         In y => In y
       end.
   #+end_src

   The same trick will be used to define ~eq~ which sates that ~x~ is
   definitional equal to ~force x~

   #+begin_src coq
     Definition eq (x : U) : x = force x :=
       match x with
         In y => eq_refl
       end.
   #+end_src

   This first matches on x to force it, to reduce to ~In y~. Then the new goal
   becomes ~In y = force (In y)~. ~force (In y)~ evaluates to just ~In y~, as it
   is just pattern matching on ~In y~. So the final goal is ~In y = In y~ which
   can be shown by ~eq_refl~. ~eq_refl~ is a constructor for ~=~, where both
   sides of ~=~ are exactly the same. If we now instantiate ~eq~ with ~u~ we
   become ~eq u~

   #+begin_src coq
     Definition eq_u : u = In u := eq u
   #+end_src

   But ~u~ is not definitional equal to ~In u~.  As mentioned above expression
   with a coinductive type are always values to prevent inifinite evaluation.
   So ~In u~ is a value and ~u~ is also a value.  But values are only
   definitional equal, if they are exactly the same.  The next section will
   solve this problem through negative coinductive types.

*** Negative Coinductive Types
    In coq 8.5. primitive record were introduced.
    With this it is now possible to define types over there destructors.  So we
    can have negative , especially negative coinductive, types in coq.  With
    primitive records we can define streams like the following

    #+begin_src coq
      CoInductive Stream (A : Set) : Set :=
        Seq { hd : A; tl : Stream A }
    #+end_src

    Now we cant define ~repeat~ over the fields of ~Stream~

    #+begin_src coq
      CoFixpoint repeat (A:Set) (x:A) : Stream A :=
        {| hd := x; tl := repeat A x|}.
    #+end_src

    To define ~repeat~ we must define what is the head of the constructed stream
    and what it is tail.  The guard conditions say now that corecursive
    occurrences must be guarded by a record field.  We can see that the
    corecursive call ~repeat~ is a direct argument to the field ~tl~ of the
    corecursive type ~Stream A~.  This means coq accepts the above definition.
    If we want to access parts of a stream we use the destructors ~hd~ and
    ~tl~.  With them we can define nth again for the negative stream.

    #+begin_src coq
      Fixpoint nth' (A : Set) (n : nat) (s : Stream' A) : list A :=
        match n with
        | 0 => nil
        | S n' => s.(hd' A) :: nth' A n' s.(tl' A)
        end.
    #+end_src

    With negative coinductive types we can't form the above mentioned
    counterexample to subject reduction anymore, because we can't pattern match
    on negative types. Oury's example becomes.

    #+begin_src coq
      CoInductive U := { out : U }.
    #+end_src

    ~U~ is now defined over its destructor ~out~, instead of its constructor ~in~.
     Then ~in~ becomes just a function.  In Fact its just a definition, because
     we don't recurse or corecurse on it.

    #+begin_src coq
      Definition In (y : U) : U := {| out := y |}
    #+end_src

    We define it over the only field ~out~.  When we put a ~y~ in then we get
    the same ~y~ out.  We can also again define ~u~

    #+begin_src coq
      CoFixpoint u : U := {| out := u |}
    #+end_src

    ~u~ With coinductive types it is know possible to define the pi type.

    #+begin_src coq
      CoInductive Pi (A : Set) (B : A -> Set) := { Apply (x : A) : B x }.
    #+end_src

    The Pi type is defined over its destructor ~Apply~.  If we evaluate ~Apply~
    on a value of Pi (which is a function) and an argument, we get the result
    i.e. we apply the value to the function.  It looks like the Pi type becomes definable
    in coq.  But we are cheating.  The type of ~Apply~ is already a Pi type.  This is because
    we identify constructors and destructors with functions.  We will see that the theory of
    the paper avoids this identification. To define a function we use
    ~CoFixpoint~.  As a simple non recursive, non dependent example we use the
    function ~plus2~.

    #+begin_src coq
      CoFixpoint plus2 : Pi nat (fun _ => nat) :=
        {| Apply x  := S (S x) |}.
    #+end_src

    If we apply (i.e. call the destructor ~Apply~) an ~x~ to plus2 we give back
    ~S (S x)~.  Which is twice the successor on ~x~.  So we add 2 to ~x~.  We
    use ~_~ here because ~plus2~ is not a dependent function i.e. the result
    type ~nat~ doesn't depend on the input value.  To define function with more
    than one argument we just use currying i.e. we use the type ~Pi~ as the
    second argument ~Pi~. For example a 2-ary non-dependent function from ~A~
    and ~B~ to ~C~ would have type ~Pi A (fun _ => Pi B (fun _ => C))~.  It
    would be fortunate if we could define ~plus~ like the following.

    #+begin_src coq
      CoFixpoint plus : Pi nat (fun _ => Pi nat (fun _ => nat)) :=
        {| Apply := fun (n : nat)  =>
             match n with
             | O => {| Apply (m : nat) := m |}
             | S n' => {| Apply m := S (Apply _ _ (Apply _ _  plus n') m) |}
             end
        |}.
    #+end_src

    But coq doesn't accept this definition.  The guard condition is violated.
    ~plus n'~ is not a direct argument of the field ~Apply~.  The definition
    should terminate because we are decreasing ~n~ and the case for ~0~ is
    accepted.  In the case for ~0~, there is no recursive call.

    We can also define a dependent function.  We define append2Units like
    follows
    #+begin_src coq
    CoFixpoint append2Units : Pi nat (fun n => Pi (Vec unit n) (fun _ => Vec unit (S (S n)))) :=
      {| Apply n := {| Apply v := Cons _ tt (Cons _ tt v) |} |}.
    #+end_src
    This just appends 2 units at a vector of length ~n~.

** Coinductive Types in Agda
   In agda coinductive types where first also introduced as positive types.
   In the section [[Positive Coinductive Types in Agda]] we will look at them in
   detail.  In section [[Negative Coinductive Types in Agda]] we describe the
   correct way to implement coinductive types in agda. There are function which
   terminate but are rejected by the type checker. In fact in any total language
   there have to be such functions. We can show that by trying to list all
   total functions. The following table lists functions per row. The columns say
   what the output of the functions to the given input is
   |          |        1 |        2 |        3 |        4 | $\dots$  |
   |----------+----------+----------+----------+----------+----------|
   | $f_1$    |        2 |        7 |        8 |        6 | $\dots$  |
   | $f_2$    |        4 |        4 |        6 |       19 | $\dots$  |
   | $f_3$    |        6 |      257 |        1 |        2 | $\dots$  |
   | $f_4$    |        7 |      121 |    23188 |     2313 | $\dots$  |
   | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\ddots$ |
   We can now define a function $g(n)=f_n(n)+1$ this function is total and not
   in the list, because it is different to any function in the list for at least
   on input To allow more functions we can use a unique feature of agda, sized
   types. They are described in section [[Termination Checking with Sized Types]].

*** Positive Coinductive Types in Agda
   Agda doesn't has a special keyword to define coinductive types like coq.  It
   uses the symbol $\infty$ to mark arguments to constructors as coinductive.
   This symbol says that the computation of arguments of this type are suspended.
   $\infty$ is just a type constructor.  So agda ensures productivity over type
   checking. We define streams like so

   #+begin_src agda
     data Stream (A : Set) : Set where
       cons : A → ∞ (Stream A) → Stream A
   #+end_src

   Here the second argument to cons is marked with $\infty$. This is the tail of
   the stream. Because it is infinitely long (we don't have a constructor of an
   empty stream) we can't compute it completely, so we suspend the computation.
   We can delay a computation with the constructor $\sharp$ and force it with
   the function $\flat$. They're types are given below

   #+begin_src agda
     ♯_ : ∀ {a} {A : Set a} → A → ∞ A
     ♭  : ∀ {a} {A : Set a} → ∞ A → A
     #+end_src

   We can now again define our usual functions.  We begin with ~repeat~

   #+begin_src agda
     repeat : {A : Set} → A → Stream A
     repeat x = cons x (♯ (repeat x))
   #+end_src

   We first apply ~Cons~ to ~x~. So the head of the stream is ~x~. We then apply
   it to the corecursive call ~repeat~. So the tail will be a repetition of xs.
   We have to call the ~repeat~ with $\sharp$ to suspend the computation.
   Otherwise the code doesn't type check. If we would write this function
   without $\sharp$ on a stream which has no $\infty$ on the second argument of
   ~cons~, the function would run forever. In fact the termination checker won't
   allow us to write such an function. We can also write ~nth~ again, which
   consumes a stream

   #+begin_src agda
     nth : {A : Set} → ℕ → Stream A → A
     nth 0       (cons x _)  = x
     nth (suc n) (cons _ xs) = nth n (♭ xs)
   #+end_src

   Here we have to use $\flat$ on the right hand side of the second case, to
   force the computation of the tail of the input stream.  We have to do that
   because ~nth~ wants a stream.  It doesn't want a suspended stream.
   Productivity on coinductive types like stream is checked by only allowing non
   decreasing recursive calls behind the $\sharp$ constructor
**** TODO Look up and cite it

*** Negative Coinductive Types in Agda
    In agda we can also define negative coinductive types.  This is the
    recommended way.  Agda implements the previously mentioned copattern matching.
    We can define a record with the keyword ~record~.  We use the keyword ~coinductive~
    to make it possible to define recursive fields.  Stream is defined like the
    following.

    #+begin_src agda
      record Stream (A : Set) : Set where
        coinductive
        field
          hd : A
          tl : Stream A
    #+end_src

    A Stream has 2 field. ~hd~ is the head of the stream. It has type ~A~. ~tl~
    is the tail of the stream. It is another stream, so it has type ~Stream A~.
    ~tl~ is a recursive field. So agda wouldn't accept the definition without
    ~coinductive~. Stream can never be empty. Every stream has a head (a field
    ~hd~) and an empty stream wouldn't have an head. So the tail of a stream can
    never be empty. Therefor every stream is infinitely long. We can now define
    ~repeat~ with copattern matching.

    #+begin_src agda
      repeat : ∀ {A : Set} → A → Stream A
      hd (repeat x) = x
      tl (repeat x) = repeat x
    #+end_src

    We have to copattern match on every field of ~Stream~, namely ~hd~ and ~tl~.
    Because agda is total it won't accept non-exhaustive (co)pattern matches
    like Haskell.  First we define what the head of ~repeat x~ is.  We just
    repeat ~x~ infinitely often.  So every element of the steam is ~x~, including
    the head.  Therefor we just write ~x~.  In the second and last copattern we
    define what the tail of the stream is.  The tail is just ~repeat x~.
    Infinitely often repeated ~x~ is the same as x and then infinitely repeated
    ~x~.  We can use normal pattern matchings and the destructors for functions
    which consume streams.  We define ~nth~ like the following.

   #+begin_src agda
     nth : ∀ {A : Set} → ℕ → Stream A → A
     nth zero s = hd s
     nth (suc n) s = nth n (tl s)
   #+end_src

   Here we just pattern match on the first argument (excluding the implicit
   argument of the type).  If it is zero the result is just the head of the
   stream.  If it is $n+1$ the result is the recursive call of ~nth~ on ~n~ and
   ~tl s~.  Agda accepts this code, because it is structural decreasing on the
   first (or second if we count the implicit) argument.

   We can also define the Pi type.  We use ~_$_~ as the apply operator.  This
   operator is taken from Haskell.

   #+begin_src agda
   record Pi (A : Set) (B : A → Set) : Set where
     field _$_ : (x : A) → B x
     infixl 20 _$_
   open Pi
   #+end_src

   like in coq we are using the first-class pi type to define the pi type. We
   can also define a function which adds 2 to a number ~plus2~ in agda.

   #+begin_src agda
    plus2 : ℕ →' ℕ
    plus2 $ x = suc (suc x)
   #+end_src

   We just use copattern matching to define it. If we apply an ~x~ to ~plus2~ we
   get ~suc (suc x)~. ~_→'_~ is just the non-dependent function it is defined
   using our pi type. Here it is

   #+begin_src agda
     _→'_ : Set → Set → Set
     A →' B = Pi A (λ _ → B)
     infixr 20 _→'_
   #+end_src

   In agda it becomes possible to define plus. We just use nested copattern
   matching.

   #+begin_src agda
    plus : ℕ →' ℕ →' ℕ
    plus $ 0       $ m = m
    plus $ (suc n) $ m = suc (plus $ n $ m)
   #+end_src

   If we change ~→'~ to ~→~ and remove ~$~ we get the standard definition for
   plus in agda.  We can also define a dependent function ~repeatUnit~ like follow
   #+begin_src agda
   repeatUnit : Pi ℕ (λ n → Vec ⊤ n)
   repeatUnit $ 0     = nil
   repeatUnit $ suc n = tt :: (repeatUnit $ n)
   #+end_src
   This function gives back a vector with the length of the input, where every element
   is unit.

*** Termination Checking with Sized Types
    They are many function, which are total but are not accepted by agda's
    termination checker.  For example we could try to define  division with
    rest on natural numbers like the following.

   #+begin_src agda
   _/_ :  ℕ → ℕ → ℕ
   zero / y = zero
   suc x / y = suc ( (x - y) / y)
   #+end_src

   The problem with this definition is that agda doesn't know that $x-y$ is
   smaller than $x+1$, which is clearly the case (x and y are positive).  This
   definition would work perfectly fine in a language without termination
   checking (like haskell).

   To remedy this problem sized types where introduced first to mini-agda (a
   language specifcilly develobed to expolore them) by abel.  Later they got
   introduced to agda itself.

   Sized types allow us to annote data with their size.  Functions can use this
   sizes to check termination and productivity.

   Termination Checking with sized types: https://agda.readthedocs.io/en/latest/language/sized-types.html

* Coinductive Types in other languages
* Type Theory base of dependent Inductive and Coinductive Types
  In the paper cite:basold2016type a type theory, where inductive types and
  coinductive types can depend on values, is developed.  For example we can, in
  contrast to the coinductive types of coq and agda, define streams which
  depend on their defintion length.  The theory differentiates types from terms.
  We don't have infinite universes, where a term in universe $n$ has a type
  in universe $n+1$(This is how it is done in coq cite:sozeau2014universe  and
  agda cite:agdadocuniverselevels).  Therefor types can only depend on values,
  not on other types.  We only have funcitions on the type level.  We will see
  that functions are definable on the term level.  We will give the rules for the
  theory in section [[Typing rules]] along with the

*** Type Action

* Implementation
  In this section we look at the implementation details

  In section [[Abstract Syntax]] we will develop the abstract syntax of our language
  from the rare syntax in the paper.  Then we rewrite the typing rules in [[*Typing rules]]
  At last we look at the implementation of the type action in [[*Type Actions]]
** Abstract Syntax
   In the following we will scratch out the abstract syntax. We will give every
   inductive and coinductive type a name. They will be defined via statements.
   We will also be able to bind expressions to names. This will be described in
   section [[Statements]] . In section [[Expressions]] we will define the syntax of
   expressions. This will mostly be in one to one correspondence with the syntax of
   the paper. Note however that we use the names of the constructors instead of anonymous
   constructors together with their type and number.  Also the order of the matches in
   rec and corec is irrelevant.  We use the names of the Con/Destructors to identify them.
   Here we can't write anonymous inductive and coinductive types.  We have to refer to the
   previously defined types.  In the following section [[Examples]] we will see how the examples
   from the paper look in our syntax.
*** Statements
    The abstract syntax is given in figure [[syntax-for-statements]].
    In the syntax "/Name/" , "$Constr_1\dots Contr_m$" and 
    "$Destr_1\dots Destr_m$" are arbitrary distinct names. With the
    keywords data and codata we define inductive and coinductive types
    respectively. After that we will write the name. Behind that we can give a
    parameter context. This is a type context. This types are not polymorphic.
    They are merely macros to make the code more precise. If we want to use this
    Type we have to fully instantiate this context. This types can occur
    everywhere in the definition where a type is expected. A (co)inductive Type
    can have a context, which is written before an arrow. ~Set~ stands for type
    (or * in the paper). If a type don't has a context we omit the arrow. We
    will also give names to every constructor and destructor. Constructors and
    destructors also have contexts. Additionally they have one argument which
    can has a recursive occurrence of the type we are defining. A constructor
    gives back a value of the type, where its context is instantiated. This
    instantiation corresponds to the sigmas in the paper. If we write a name
    before a equal sign we can bind the following expression to the name. Every
    such defined name can depend on a parameter context and an argument context.
    We write the parameter context like in the case for data types behind the
    name. After that we can give a term context between round parenthesis.

    #+name: syntax-for-statements
    \begin{figure}
    \begin{lstlisting}
    statement =
      data Name<$C_1 : \Gamma_1$ -> *,$\dots$ ,$ C_n : \Gamma_n$ -> *> : $(x_1 : B_1,\dots,x_n : B_n)$ -> Set where
        $Constr_1$ : $(x_{1_1}:B_{1_1},\dots,x_{n_1}: B_{n_1})$ -> $A_1[Name/X]$ -> Name $\sigma_{1_1}\dots \sigma_{1_n}$
               $\vdots$                $\vdots$             $\vdots$            $\vdots$
        $Constr_m$ : $(x_{1_m}:B_{1_m},\dots,x_{n_m}: B_{n_m})$ -> $A_i[Name/X]$ -> Name $\sigma_{m_1}\dots \sigma_{m_n}$
     | codata Name<$C_1 : \Gamma_1$ -> *,$\dots$ ,$ C_n : \Gamma_n$ -> *> : $(x_1 : B_1,\dots,x_n : B_n)$ -> Set where
        $Destr_1$ : $(x_{1_1}:B_{i_1},\dots,x_{n_1}: B_{n_1})$ -> Name $\sigma_{1_1}\dots \sigma_{1_n}$ -> $A_1[Name/X]$
               $\vdots$                $\vdots$             $\vdots$            $\vdots$
        $Destr_m$ : $(x_{1_m}:B_{1_m},\dots,x_{n_m}: B_{n_m})$ -> Name $\sigma_{m_1}\dots \sigma_{m_n}$ -> $A_i[Name/X]$
     | name<$C_1 : Gamma_1$ -> *,$\dots$ ,$ C_n : Gamma_n$ -> *> $(x_1:A_1,\dots,x_n:A_n)$ = expr
    \end{lstlisting}
    \caption{Syntax for statements}
    \end{figure}


    The statements in Figure [[syntax-for-statements]] correspond to $\rho(X:\Gamma\rat*;\vv\sigma;\vv{A}):\Gamma\rat*$ as follows.
    + $x_1: B_1,\dots,x_n: B_n$ is $\Gamma$
    + /Name/ is X
    + $Constr_1,\dots, Contr_m$ stands for
      $\alpha_1^{\mu(X:\Gamma\rat *;\vv\sigma;\vv A)},\dots,\alpha_m^{\mu(X:\Gamma\rat *;\vv\sigma;\vv A)}$
    + $Destr_1,\dots, Destr_m$ stands for
      $\xi_1^{\mu(X:\Gamma\rat *;\vv\sigma;\vv A)},\dots,\xi_m^{\mu(X:\Gamma\rat *;\vv\sigma;\vv A)}$
    + $Name_i$ is $A_i[\Gamma/X]$
    + $(x_{1_1}:B_{1_1},\dots,x_{n_1}: B_{n_1}),\dots,(x_{1_m}:B_{1_m},\dots,x_{n_m}:B_{n_m})$
      stands for $\Gamma_1,\dots,\Gamma_m$
    + <$C_1 : Gamma_1$ -> *,$\dots$ ,$ C_n : Gamma_n$ -> *> are the parameter contexts.
      If we call a constructor we have to give this types, to relate the right type to it.

    To parse the abstract syntax we use megaparsec. The parser generates an
    abstract syntax tree, which is given for statements in Listing
    [[Abstract Syntax Tree for Statements]]. The field ~ty~ in ~ExprDef~ is used later in
    type checking. The parser just fills them in with ~Nothing~. data and codata
    definitions are both saved in ~TypeDef~. The haskell type ~OpenDuctive~ contains all the
    information for inductive and coinductive types. It corresponds to $\mu$ and`
    $\nu$ in the paper. We use an ~OpenDuctive~ where the field ~inOrCoin~ is ~IsIn~ 
    for $ \mu$ and an ~OpenDuctive~ where the field ~inOrCoin~ is ~IsCoin~ for
    $\nu$.  The haskell type ~StrDef~ ensures that the sigmas as and gamma1s have the
    same length.  We omit the implementation details for the parser, because we
    are manly focused on type checking.

    #+caption: Abstract Syntax Tree for Statements
    #+NAME: Abstract Syntax Tree for Statements
    #+begin_src haskell
      data Statement = ExprDef { name :: Text
                               , tyParameterCtx :: TyCtx
                               , exprParameterCtx :: Ctx
                               , expr :: Expr
                               , ty :: Maybe Type
                               }
                     | TypeDef OpenDuctive
                     | Expression Expr

      data OpenDuctive = OpenDuctive { nameDuc :: Text
                                     , inOrCoin :: InOrCoin
                                     , parameterCtx :: TyCtx
                                     , gamma :: Ctx
                                     , strDefs :: [StrDef]
                                     }

      data StrDef = StrDef { sigma :: [Expr]
                           , a :: TypeExpr
                           , gamma1 :: Ctx
                           , strName :: Text
                           }
    #+end_src
*** Expressions
    The abstract syntax for expression is given in figure [[syntax-for-expressions]]
    We will separate expression in expressions for terms and in expressions for
    types.  There are given as regular expressions in ~expr~ and ~typeExpr~ respectively.

    An ~expr~ is either the unit expression ~()~, an con/destructor, an
    application ~@~, an ~rec~ or an ~corec~. All con/destructors have to be
    instantiate with all variables in the parameter contexts of their types.
    This is done by given types of the expected kinds separated by ',' enclosed
    in '<' and '>'. With the keyword ~rec~ we can destruct an inductive types.
    We write ~Type to typeExrp~, where ~Type~ is an previously defined inductive
    type after ~rec~ to facilitate type checking. It says we want to destruct a
    inductive type to some other type . We have to list all the constructor
    above one another. For each constructor we write an expression behind the
    equal sign, which should be of type ~TypeExpr~ which we have given above. In
    this expression we can use variables given in the match expression. The last
    one is the recursive occurrence. With the keyword ~corec~ we can do the same
    thing to construct a coinductive type. Here we have to swap the ~Type~ and
    the ~TypeExpr~ and list the destructors. We can also apply a expression to
    another with ~@~. The only primitive expression we have is the unit
    expression ~()~

    The ~typeExpr~ is either the unit type ~Unit~, a lambda abstraction on
    types, an application or a variable. In the lambda expression we have to
    give the type of the variable. We apply a type to a term (types can only
    depend on terms) with ~@~. The unit type is the only primitive type
    expression.

    The generated abstract syntax tree is given in listing
    [[abstract-syntax-tree-for-expressions]]. The variables for expressions are
    separated in ~LocalExprVar~ and ~GlobalExprVar~. ~LocalExprVar~ should refer
    to variables which are only locally defined i.e. in ~Rec~ and ~Corec~. We
    use de-Brujin indexes for them. ~GlobalExprVar~ refers to variables from
    definitions. Here we just use names. We do the same thing for ~LocalTypeVar~
    and ~GlobalTypeVar~. In the abstract syntax tree we use anonymous
    constructors like in the paper. Constructor contains a field ~Ductive~ of
    this type. Here we know that it has to be an inductive type, because we
    don't have constructors for coinductive types. For Destructor, Rec and corec
    we also know if it is coinductive or inductive. The field ~nameStr~ in
    ~Constructor~ and ~Destructor~ are just for printing.

    #+name: syntax-for-expressions
    \begin{figure}
    \begin{lstlisting}
        expr :=
          rec Name<$C_1,\dots,C_n$> to typeExpr where
            match*
        | corec typeExpr to Name<$C_1,\dots,C_n$> where
            match*
        | expr @ expr | ()

        match := Name var* = expr

        typeExpr := Unit
                  | (var:typeExpr).typeExpr
                  | typeExpr @ expr
                  | Name
    \end{lstlisting}
    \caption{Syntax for expressions}
    \end{figure}

    #+name: abstract-syntax-tree-for-expressions
    #+caption: Abstract Syntax Tree for Expressions
    #+begin_src haskell
      data TypeExpr = UnitType
                    | TypeExpr :@ Expr
                    | LocalTypeVar Int Bool Text
                    | Parameter Int Bool Text
                    | GlobalTypeVar Text [TypeExpr]
                    | Abstr Text TypeExpr TypeExpr
                    | Ductive { openDuctive :: OpenDuctive
                              , parametersTyExpr :: [TypeExpr]}

      data Expr = UnitExpr
                | LocalExprVar Int Bool Text
                | GlobalExprVar Text [TypeExpr] [Expr]
                | Expr :@: Expr
                | Structor { ductive :: OpenDuctive
                           , parameters :: [TypeExpr]
                           , num :: Int
                           }
                | Iter { ductive :: OpenDuctive
                       , parameters :: [TypeExpr]
                       , motive :: TypeExpr
                       , matches :: [([Text],Expr)]
                       }
    #+end_src

** Substitution

** Typing rules
   We have to rewrite the typing rules of the paper, to get rules which are
   syntax directed. Here are the rules which have to be rewritten.
   + *(Ty-Inst)*
   + *(Param-Abstr)*
   This rules contain variables in the premises where their type isn't in the
   conclusion. So if we want to type-check something which is the conclusion of
   such a rule we have no way of knowing what this variables are.

   We don't need the weaking rules because we can lookup a variable in a
   context.

   So the following rules get removed.
   + *(TyVar-Weak)*
   + *(Ty-Weak)*
   The order in *TyCtx* isn't relevant so we use a Map for it.  The order
   of *Ctx* is relevant because types of later variables can refer to
   former variables and application instantiate the first varibale in
   *Ctx*

   We also rewrite the rules which are already syntax-directed to rules
   which work on our syntax

   We add a new Ctx for data types

   We will mark semantic differences in the rewritten rules gray.
*** Context rules
    The rules for valid contexts are already syntax directed so we take
    just them
    \begin{center}
    \AxiomC{}
    \UnaryInfC{$\vdash\emptyset$ \TyCtx}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\vdash\Theta$ \TyCtx}
    \AxiomC{$\vdash\Gamma$ \Ctx}
    \BinaryInfC{$\vdash\Theta,X:\Gamma\rat*$ \TyCtx}
    \DisplayProof
    \vskip 0.5em
    \AxiomC{}
    \UnaryInfC{$\vdash\emptyset$ \Ctx}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\emptyset|\Gamma\vdash A:*$}
    \UnaryInfC{$\vdash\Gamma,x:A$ \Ctx}
    \DisplayProof
    \end{center}

    We use the notation $\Theta(X)\rightsquigarrow\Gamma\rat*$ for
    looking up the type-variable $X$ in type-context $\Theta$ yields type
    $\Gamma\rat*$. We add 2 rules for looking up something in a
    type-context.  They are:
    \begin{center}
      \AxiomC{$\vdash \Theta$ \TyCtx}
      \AxiomC{$\vdash \Gamma$ \Ctx}
      \BinaryInfC{$\Theta,X:\Gamma\rat*(X)\rightsquigarrow\Gamma\rat*$}
      \DisplayProof
      \hskip 1.5em
      \AxiomC{$\vdash \Gamma_1$ \Ctx}
      \AxiomC{$\Theta(X) \rightsquigarrow\Gamma_2\rat*$}
      \BinaryInfC{$\Theta,Y:\Gamma_1\rat*(X)\rightsquigarrow\Gamma_2\rat*$}
      \DisplayProof
    \end{center}
    Here $Y$ and $X$ are different variables

    Respectively the notation $\Gamma(x)\rightsquigarrow A$ means looking
    up the termvariable $x$ in term-context $\Gamma$ yields type $A$. The
    rules for term-contexts are:
    \begin{center}
      \AxiomC{$\vdash \Gamma$ \Ctx}
      \AxiomC{$\Gamma\vdash A:*$}
      \BinaryInfC{$\Gamma,x:A(x)\rightsquigarrow A$}
      \DisplayProof
      \hskip 1.5em
      \AxiomC{$\Gamma(x) \rightsquigarrow A$}
      \AxiomC{$\Gamma\vdash B:*$}
      \BinaryInfC{$\Gamma,y:B(x)\rightsquigarrow A$}
      \DisplayProof
    \end{center}

*** Full evaluation
    We write $A \longrightarrow_T^* B$ for evaluating $A$ as long as it
    is possible yields $B$.

    The rules are
    \begin{center}
    \AxiomC{$\neg\exists B : A \longrightarrow_T B$}
    \UnaryInfC{$A \longrightarrow_T^* A$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$A \longrightarrow_T B$}
    \AxiomC{$B \longrightarrow_T^* C$}
    \BinaryInfC{$A \longrightarrow_T^* C$}
    \DisplayProof
    \end{center}
*** Beta-equivalence
    We introduce a new rule for beta-equivalence.
    \begin{center}
    \AxiomC{$A\longrightarrow_T^* A'$}
    \AxiomC{$B\longrightarrow_T^* B'$}
    \AxiomC{$A'\equiv_\alpha B'$}
    \TrinaryInfC{$A\equiv_\beta B$}
    \DisplayProof
    \end{center}
    In the implementation $\equiv_\alpha$ is trivial, because we use /de
    Bruijn indices/.

    We also add some rule to check if two contexts are the same.
    \begin{center}
    \AxiomC{}
    \UnaryInfC{$\emptyset\equiv_\beta\emptyset$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\Gamma_1\equiv_\beta \Gamma_2$}
    \AxiomC{$A[\Gamma_1]\equiv_\beta B[\Gamma_2]$}
    \BinaryInfC{$\Gamma_1,x:A\equiv_\beta\Gamma_2,y:B$}
    \DisplayProof
 %   \vskip 0.5em
 %   \AxiomC{$\Theta_1\equiv_\beta \Theta_2$}
 %   \AxiomC{$\Gamma_1\equiv_\beta \Gamma_2$}
 %   \BinaryInfC{$\Theta_1,X:\Gamma_1\rat*\equiv_\beta\Theta_2,X:\Gamma_2\rat*$}
 %   \DisplayProof
    \end{center}

*** Unit type introduction
    The rule
    \begin{prooftree}
      \AxiomC{}
      \RightLabel{\textbf{($\top$-I)}}
      \UnaryInfC{$\vdash\top:*$}
    \end{prooftree}
    gets rewritten to
     \begin{prooftree}
      \AxiomC{}
      \RightLabel{\textbf{(Unit-I)}}
      \UnaryInfC{\colorbox{gray}{$\Theta|\Gamma$}$\vdash$Unit:$*$}
    \end{prooftree}
    We change the syntax "$\top$" to "Unit" and add *Ctx* and *TyCtx*.
    We will do this for every rule which has empty contexts to subsume
    the rules with *TyVar-Weak*, *Ty-Weak* and *Term-Weak*.

*** Type Variable introduction

     The rule
     \begin{prooftree}
      \AxiomC{$\vdash \Theta$ \TyCtx}
      \AxiomC{$\vdash \Gamma$ \Ctx}
      \RightLabel{\textbf{(TyVar-I)}}
      \BinaryInfC{$\Theta,X:\Gamma\rat*|\emptyset\vdash X : \Gamma \rat *$}
    \end{prooftree}
    gets rewritten to

     \begin{prooftree}
      \AxiomC{\colorbox{gray}{$\Theta(X)\rightsquigarrow\Gamma\rat*$}}
      \AxiomC{\colorbox{gray}{$\vdash \Gamma_1$ \Ctx}}
      \RightLabel{\textbf{(TyVar-I)}}
      \BinaryInfC{$\Theta|$\colorbox{gray}{$\Gamma_1$}$\vdash X : \Gamma \rat *$}
    \end{prooftree}

*** Type instantiation
    The rule
    \begin{prooftree}
      \AxiomC{$\Theta|\Gamma_1\vdash A:(x:B,\Gamma_2)\rat*$}
      \AxiomC{$\Gamma_1\vdash t:B$}
      \BinaryInfC{$\Theta|\Gamma_1\vdash A@t:\Gamma_2[t/x]\rat*$}
    \end{prooftree}
    gets rewritten to
     \begin{prooftree}
      \AxiomC{$\Theta|\Gamma_1\vdash A:(x:B,\Gamma_2)\rat*$}
      \AxiomC{$\Gamma_1\vdash t:$\colorbox{gray}{$B'$}}
      \AxiomC{\colorbox{gray}{$B\equiv_\beta B'$}}
      \TrinaryInfC{$\Theta|\Gamma_1\vdash A@t:\Gamma_2[t/x]\rat*$}
    \end{prooftree}


*** Parameter abstraction
    The rule
    \begin{center}
      \AxiomC{$\Theta|\Gamma_1,x:A\vdash B:\Gamma_2\rat*$}
      \RightLabel{\textbf{(Param-Abstr)}}
      \UnaryInfC{$\Theta|\Gamma_1\vdash(x).B:(x:A,\Gamma_2)\rat*$}
      \DisplayProof
    \end{center}
    gets rewritten to
    \begin{center}
      \AxiomC{$\Theta|\Gamma_1,x:A\vdash B:\Gamma_2\rat*$}
      \RightLabel{\textbf{(Param-Abstr)}}
      \UnaryInfC{$\Theta|\Gamma_1\vdash(x$\colorbox{gray}{$:A$}$).B:(x:A,\Gamma_2)\rat*$}
      \DisplayProof
    \end{center}

*** (co)data definition
    The rule
    \begin{prooftree}
    \AxiomC{$\sigma_k:\Gamma_k\triangleright\Gamma$}
    \AxiomC{$\Theta,X:\Gamma\rat*|\Gamma_k\vdash A_k:*$}
    \RightLabel{(\textbf{FP-Ty})}
    \BinaryInfC{$\Theta | \emptyset \vdash \rho(X : \Gamma \rat *;\vv{\sigma};\vv{A}):\Gamma\rat *$}
    \end{prooftree}
    gets rewritten to
    \begin{prooftree}
    \AxiomC{$\sigma_k:\Gamma_k\triangleright\Gamma$}
    \AxiomC{$\Theta,X:\Gamma\rat*|\Gamma_k\vdash A_k:*$}
    \RightLabel{(\textbf{FP-Ty})}
    \BinaryInfC{$\Theta | $\colorbox{gray}{$\Gamma_1$} $\vdash$ data X $\Gamma$ -> Set where; $\vv{Constr_k : \Gamma_k\text{ -> }A_k\text{ -> }X \sigma_k}$}
    \end{prooftree}
    and
    \begin{prooftree}
    \AxiomC{$\sigma_k:\Gamma_k\triangleright\Gamma$}
    \AxiomC{$\Theta,X:\Gamma\rat*|\Gamma_k\vdash A_k:*$}
    \RightLabel{(\textbf{FP-Ty})}
    \BinaryInfC{$\Theta |$\colorbox{gray}{$\Gamma_1$} $ \vdash$ codata X $\Gamma$ -> Set where; $\vv{Destr_k : \Gamma_k \text{ -> } X \sigma_k \text{ -> } A_k}$}
    \end{prooftree}

*** Unit expression introduction
    The rule
    \begin{center}
      \AxiomC{}
      \RightLabel{\textbf{(}$\top$\textbf{-I)}}
      \UnaryInfC{$\lozenge:\top$}
      \DisplayProof
    \end{center}
    get rewritten to
    \begin{center}
      \AxiomC{}
      \RightLabel{\textbf{(}$\top$\textbf{-I)}}
      \UnaryInfC{():Unit}
      \DisplayProof
    \end{center}

*** Expression Instantiation
    The rule
    \begin{center}
      \AxiomC{$\Gamma_1\vdash t:(x:A,\Gamma_2)\rat B$}
      \AxiomC{$\Gamma_1\vdash s:A$}
      \RightLabel{\textbf{(Inst)}}
      \BinaryInfC{$\Gamma_1\vdash t@s:\Gamma_2[s/x]\rat B[s/x]$}
      \DisplayProof
    \end{center}
    gets rewritten to
    \begin{center}
      \AxiomC{$\Gamma_1\vdash t:(x:A,\Gamma_2)\rat B$}
      \AxiomC{$\Gamma_1\vdash s:$\colorbox{gray}{$A'$}}
      \AxiomC{\colorbox{gray}{$A\equiv_\beta A'$}}
      \RightLabel{\textbf{(Inst)}}
      \TrinaryInfC{$\Gamma_1\vdash t@s:\Gamma_2[s/x]\rat B[s/x]$}
      \DisplayProof
    \end{center}

*** Expression variable introduction
    The rule
    \begin{center}
      \AxiomC{$\Gamma\vdash A:*$}
      \RightLabel{\textbf{(Proj)}}
      \UnaryInfC{$\Gamma,x:A\vdash x:A$}
      \DisplayProof
    \end{center}
    gets rewritten to
    \begin{center}
      \AxiomC{\colorbox{gray}{$\Gamma(x)\rightsquigarrow A$}}
      \RightLabel{\textbf{(Proj)}}
      \UnaryInfC{$\Sigma|\Theta|\Gamma\vdash x:A$}
      \DisplayProof
    \end{center}


*** Constructor
    The rule
    \begin{center}
      \AxiomC{$\mu(X:\Gamma\rat*;\vv{\sigma};\vv{A}):\Gamma\rat*$}
      \AxiomC{$1\leq k\leq|\vv{A}|$}
      \RightLabel{\textbf{(Ind-I)}}
      \BinaryInfC{$\vdash\alpha_k^{\mu(X:\Gamma\rat*;\vv{\sigma};\vv{A})}:(\Gamma_k,y:A_k[\mu/X])\rat\mu@\sigma_k$}
      \DisplayProof
    \end{center}
    gets rewritten to
    \begin{center}
      \AxiomC{\colorbox{gray}{$\Sigma$(Constr)$\rightsquigarrow(\Gamma_k,y:A_k[\mu/X])\rat\mu@\sigma_k$}}
      \RightLabel{\textbf{(Ind-I)}}
      \UnaryInfC{\colorbox{gray}{$\Sigma|\Theta|\Gamma$}$\vdash$Constr$:(\Gamma_k,y:A_k[\mu/X])\rat\mu@\sigma_k$}
      \DisplayProof
    \end{center}


*** Destructor
    The rule
    \begin{center}
      \AxiomC{$\nu(X:\Gamma\rat*;\vv{\sigma};\vv{A}):\Gamma\rat*$}
      \AxiomC{$1\leq k\leq|\vv{A}|$}
      \RightLabel{\textbf{(Coind-E)}}
      \BinaryInfC{$\vdash\xi_k^{\nu(X;\Gamma\rat*;\vv{\sigma};\vv{A})}:(\Gamma_k,y:\nu@\sigma_k)\rat
        A_k[\nu/X]$}
      \DisplayProof
    \end{center}
    gets rewritten to
    \begin{center}
      \AxiomC{\colorbox{gray}{$\Sigma$(Destr)$\rightsquigarrow(\Gamma_k,y:\nu@\sigma_k)\rightarrow
        A_k[\nu/X]$}}
      \RightLabel{\textbf{(Ind-I)}}
      \UnaryInfC{\colorbox{gray}{$\Sigma|\Theta|\Gamma$}$\vdash$Destr$:(\Gamma_k,y:\nu@\sigma_k)\rat
        A_k[\nu/X]$}
      \DisplayProof
    \end{center}


*** Recursion
    \begin{center}
      \AxiomC{$\vdash C:\Gamma\rat*$}
      \AxiomC{$\Delta,\Gamma_k,y_k:A_k[C/X]\vdash g_k:(C@\sigma_k)$}
      \AxiomC{$\forall k=1,\dots,n$}
      \RightLabel{\textbf{(Ind-E)}}
      \TrinaryInfC{$\Delta\vdash$ rec
        $\vv{(\Gamma_k,y_k).g_k}:(\Gamma,y:\mu@id_\Gamma)\rat C@id_\Gamma$}
      \DisplayProof
    \end{center}

    \begin{prooftree}
      \AxiomC{$\vdash C:\Gamma\rat*$}
      \AxiomC{\colorbox{gray}{$\vv{\vdash B_k\equiv_\beta(C@\sigma_k)}$}}
      \AxiomC{\colorbox{gray}{$\vv{\Sigma \vdash\text{Constr}_k:(\Gamma_k,y:A_k[\mu/X])\rat\mu@\sigma_k}$}}
      \noLine
      \UnaryInfC{$\vv{\Delta,\Gamma_k,y_k:A_k[C/X]\vdash g_k:\text{\colorbox{gray}{$B_k$}}}$}
      \RightLabel{\textbf{(Ind-E)}}
      \TrinaryInfC{\colorbox{gray}{$\Sigma|\Theta|$}$\Delta\vdash$ rec \colorbox{gray}{$\mu$ to C};
        $\vv{\text{Constr}_k\vv{x_k}\text{ } y_k = g_k}:(\Gamma,y:\mu@id_\Gamma)\rat C@id_\Gamma$}
     \end{prooftree}


*** Corecursion
    \begin{center}
      \AxiomC{$\vdash C:\Gamma\rat*$}
      \AxiomC{$\Delta,\Gamma_k,y_k:(C@\sigma_k)\vdash g_k:A_k[C/X]$}
      \AxiomC{$\forall k=1,\dots,n$}
      \RightLabel{\textbf{(Coind-I)}}
      \TrinaryInfC{$\Delta\vdash$ corec
        $\vv{(\Gamma_k,y_k).g_k}:(\Gamma,y:C@id_\Gamma)\rat \nu@id_\Gamma$}
      \DisplayProof
    \end{center}

    \begin{prooftree}
      \AxiomC{$\vdash C:\Gamma\rat*$}
      \AxiomC{\colorbox{gray}{$\vv{\vdash B_k\equiv_\beta A_k[C/X]}$}}
      \AxiomC{\colorbox{gray}{$\vv{\Sigma \vdash\text{Destr}_k:(\Gamma_k,y:\nu@\sigma_k)\rat
        A_k[\nu/X]}$}}
      \noLine
      \UnaryInfC{$\vv{\Delta,\Gamma_k,y_k:(\Gamma_k,y:(C@\sigma_k))\vdash g_k:\text{\colorbox{gray}{$B_k$}}}$}
      \RightLabel{\textbf{(Coind-I)}}
      \TrinaryInfC{\colorbox{gray}{$\Sigma|\Theta|$}$\Delta\vdash$ rec \colorbox{gray}{C to $\nu$};
        $\vv{\text{Destr}_k\vv{x_k}\text{ } y_k = g_k}:(\Gamma,y:C@id_\Gamma)\rat \nu@id_\Gamma$}
     \end{prooftree}

** Type Actions
   \begin{definition}
     Let $n \in \mathbb{N}$ and $1 \leq i \leq n$.
     Let:
     \begin{align*}
       X_1 : \Gamma_1 \rat \ast,\ldots,X_n : \Gamma_n \rat \ast\ |\ \Gamma' \vdash C : \Gamma \rat \ast \\
       \Gamma_i \vdash A_i : \ast \\
       \Gamma_i \vdash B_i : \ast \\
       \Gamma_i, x : A_i \vdash t_i : B_i
     \end{align*}
     We will define the type action so that the following holds
     \begin{equation*}
       \Gamma',\Gamma,x:\hat{C}(\vv{A})\vdash\hat{C}(\vv{t}):\hat{C}(\vv{B})
     \end{equation*}
     Then we define the type action on terms inductively over $C$
     \begin{align*}
       \begin{array}{ll}
         \widehat{C}(\vv{t},t_{n+1}) = \widehat{C}(\vv{t})
         &\text{for \textbf{(TyVarWeak)}}\\
         \widehat{X_i}(\vv{t})=t_i\\
         \widehat{C'@s}(\vv{t})=\widehat{C'}(\vv{t})[s/y],
         &\text{for }\Theta\mid\Gamma'\vdash C':(y,\Gamma)\rat*\\
         \widehat{(y).C'}(\vv{t})=\widehat{C'}(\vv{t}),
         &\text{for }\Theta\mid(\Gamma',y)\vdash C':\Gamma\rat*\\
         \widehat{\mu(Y:\Gamma\rat*;\vv{\sigma};\vv{D}} =\text{rec}^{R_A}\vv{(\Delta_k,x).g_k}@\id{\Gamma}@x
         &\text{for } \Theta,Y:\Gamma\rat*\mid\Delta_k\vdash D_k:*\\
         \quad\text{with } g_k = \alpha_k^{R_B}@\id{\Delta_k}@\left(\widehat{D_k}(\vv{t},x)\right)\\
         \quad\text{and } R_A=\mu(Y:\Gamma\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).A}/\vv{X}])\\
         \quad\text{and } R_B=\mu(Y:\Gamma\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).B}/\vv{X}])\\
         \widehat{\nu(Y:\Gamma\rat*;\vv{\sigma};\vv{D}} =\text{corec}^{R_B}\vv{(\Delta_k,x).g_k}@\id{\Gamma}@x
         &\text{for } \Theta,Y:\Gamma\rat*\mid\Delta_k\vdash D_k:*\\
         \quad\text{with } g_k = \widehat{D_k}(\vv{t},x)[(\xi_k^{R_A}@\id{\Delta_k}@x)/x]\\
         \quad\text{and } R_A=\mu(Y:\Gamma\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).A}/\vv{X}])\\
         \quad\text{and } R_B=\mu(Y:\Gamma\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).B}/\vv{X}])\\
       \end{array}
     \end{align*}
   \end{definition}
   #+NAME: abstrid
   #+begin_theorem
    $(\Gamma).A@\id{\Gamma}\leftrightarrow_T A$
   #+end_theorem
   #+begin_proof
     We show this by induction on the length of $\Gamma$
     + $\Gamma=\epsilon$:
       \begin{equation*}
          A \longleftrightarrow_T A
       \end{equation*}
     + $\Gamma=x:B,\Gamma'$:
       \begin{equation*}
         (x:B,\Gamma').A@x@\id{\Gamma'}
         \longrightarrow_p(\Gamma').A@\id{\Gamma'}[x/x]
         = (\Gamma').A@\id{\Gamma'} \overset{IdH.}{\longleftrightarrow_T}A
       \end{equation*}
   #+end_proof
   #+NAME: ctxconv
   #+begin_theorem
    The following rule holds
    \begin{prooftree}
    \AxiomC{$x:A\vdash t:B$}
    \AxiomC{$A\longleftrightarrow_TA'$}
    \BinaryInfC{$x:A'\vdash t:B$}
    \end{prooftree}
   #+end_theorem
   #+begin_proof
     We show this by induction on t
   #+end_proof
   #+begin_theorem
   The typing rule (5) in the paper holds
   \begin{prooftree}
     \AxiomC{$X:\Gamma_1\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
     \AxiomC{$\Gamma_1,x:A\vdash t:B$}
     \BinaryInfC{$\Gamma',\Gamma,x:\widehat{C}(A)\vdash\widehat{C}(t):\widehat{C}(B) $}
   \end{prooftree}
   #+end_theorem
   #+begin_proof
   First we will generalize the rule to
   \begin{prooftree}
     \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
     \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
     \BinaryInfC{$\Gamma',\Gamma,x:\widehat{C}(\vv{A})\vdash\widehat{C}(\vv{t}):\widehat{C}(\vv{B}) $}
   \end{prooftree}
   Then we gonna show it by Induction on the derivation $\mathcal{D}$ of $C$
   +
     #+begin_export latex
       $\mathcal{D}$ =
         \AxiomC{}
         \topI{$\top:*$}
         \DisplayProof
     #+end_export

     Then the type actions got calculated as follows
     \begin{align*}
       &\widehat{\top}(\vv{A}) = \widehat{\top}() = \top\\
       &\widehat{\top}(\vv{t}) = \widehat{\top}() = x\\
       &\widehat{\top}(\vv{B}) = \widehat{\top}() = \top
     \end{align*}
     We than got the following prooftree
     \begin{prooftree}
       \AxiomC{$\vdash\top:*$}
       \RightLabel{\textbf{(Proj)}}
       \UnaryInfC{$x:\top\vdash x:\top$}
     \end{prooftree}
   +
     #+begin_export latex
       $\mathcal{D}$ =
         \Di{1}
         \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_{n-1}:\Gamma_{n-1}$\TyCtx}
         \Di{2}
         \UnaryInfC{$\Gamma_n$\Ctx}
         \TyVarI{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\emptyset\vdash X_n:\Gamma_n\rat*$}
         \DisplayProof
     #+end_export

     Again we calculate the type actions
     \begin{align*}
       &\widehat{X_n}(\vv{A}) = X_n[\vv{(\Gamma_i).A}/\vv{X}]@\id{\Gamma_n}= X_n[(\Gamma_n).A_n/X_n]@\id{\Gamma_n} = (\Gamma_n).A_n@\id{\Gamma_n}\\
       &\widehat{X_n}(\vv{t}) = t_n\\
       &\widehat{X_n}(\vv{B}) = X_n[\vv{(\Gamma_i).B}/\vv{X}]@\id{\Gamma_n}= X_n[(\Gamma_n).B_n/X_n]@\id{\Gamma_n} = (\Gamma_n).B_n@\id{\Gamma_n}\\
     \end{align*}
     We know from the first premise that $\Gamma=\Gamma_n$ and $\Gamma'=\emptyset$

     Here we got the prooftree
     \begin{prooftree}
     \AxiomC{$\Gamma_n,x:A\vdash t:B$}
     \AxiomC{}
     \RightLabel{Thrm. \ref{abstrid}}
     \UnaryInfC{$A\longleftrightarrow_T(\Gamma_n).A@\id{\Gamma_n}$}
     \RightLabel{Thrm. \ref{ctxconv}}
     \BinaryInfC{$\Gamma_n,x:(\Gamma_n).A@\id{\Gamma_n}\vdash t:B$}
     \AxiomC{}
     \RightLabel{Thrm. \ref{abstrid}}
     \UnaryInfC{$B\longleftrightarrow_T(\Gamma_n).B@\id{\Gamma_n}$}
     \RightLabel{Conv}
     \BinaryInfC{$\Gamma_n,x:(\Gamma_n).A@\id{\Gamma_n}\vdash t_n:(\Gamma_n).B@\id{\Gamma_1}$}
     \end{prooftree}

   +
     #+begin_export latex
     $\mathcal{D}$ =
       \Di{1}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\mid\Gamma'\vdash C:\Gamma\rat*$}
       \Di{2}
       \UnaryInfC{$\Gamma_n$\Ctx}
       \TyVarWeak{$X_1:\Gamma_1\rat*,\dots,X_{n+1}:\Gamma_{n+1}\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
       \DisplayProof
     #+end_export

     Here we got the prooftree
     \begin{prooftree}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_{n+1}:\Gamma_{n+1}\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
       \RightLabel{(*)}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
       \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
       \RightLabel{IdH.}
       \BinaryInfC{$\Gamma',\Gamma,x:\underbrace{\widehat{C}(\vv{A})}_{\overset{(**)}{=}\widehat{C}(\vv{A},A_{n+1})}\vdash\underbrace{\widehat{C}(\vv{t})}_{\overset{(***)}{=}\widehat{C}(\vv{t},t_{n+1})}:\underbrace{\widehat{C}(\vv{B})}_{\overset{(**)}{=}\widehat{C}(\vv{B},B_{n+1})} $}
     \end{prooftree}

     (=*=) Here we undo *(TyVar-Weak)*

     (=**=) $X_{n+1}$ doesn't occur free in C, otherwise $\mathcal{D}_1$ wouldn't be possible

     (=***=) Case for *(TyVar-Weak)* of type actions on terms

   +
     #+begin_export latex
     $\mathcal{D}$ =
       \Di{1}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\mid\Gamma'\vdash C:\Gamma\rat*$}
       \Di{2}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\mid\Gamma'\vdash D:*$}
       \TyWeak{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma',y:D\vdash C:\Gamma\rat*$}
       \DisplayProof
     #+end_export

     Here we got the prooftree
     \begin{scprooftree}{0.6}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma',y:D\vdash C:\Gamma\rat*$}
       \RightLabel{(*)}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
       \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
       \RightLabel{IdH.}
       \BinaryInfC{$\Gamma',\Gamma,x:\widehat{C}(\vv{A})\vdash\widehat{C}(\vv{t}):\widehat{C}(\vv{B})$}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\mid\Gamma'\vdash D:*$}
       \TermWeak{$\Gamma',\Gamma,x:\widehat{C}(\vv{A})y\vdash\widehat{C}(\vv{t}):\widehat{C}(\vv{B})$}
     \end{scprooftree}

     (=*=) Here we undo *(Ty-Weak)*

   +
     #+begin_export latex
     $\mathcal{D}= $
     \AxiomC{$X_1:\Gamma_1,\ldots,X_n:\Gamma_n\mid\Gamma'\vdash C':(y:D,\Gamma)\rat* $}
     \AxiomC{$\Gamma'\vdash s: D$}
     \TyInst{$X_1:\Gamma_1,\ldots,X_n:\Gamma_n\mid\Gamma'\vdash C'@s:\Gamma\rat* $}
     \DisplayProof
     #+end_export

     Then we got the following induction hypothesis
     \begin{prooftree}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma'\vdash C':(y:D,\Gamma)\rat*$}
       \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
       \BinaryInfC{$\Gamma',y:D,\Gamma,x:\widehat{C'}(\vv{A})\vdash\widehat{C'}(\vv{t}):\widehat{C'}(\vv{B}) $}
     \end{prooftree}

     Calculated type actions:
     \begin{align*}
       &\widehat{C'@s}(\vv{A})=C'@s[\vv{(\Gamma_i).A}/\vv{X}]@\id{\Gamma}=C'[\vv{(\Gamma_i).A}/\vv{X}]@s@\id{\Gamma}
       =\widehat{C'}(\vv{A})[s/y]\\
       &\widehat{C'@s}(\vv{t})=\widehat{C'}(\vv{t})[s/y]\\
       &\widehat{C'@s}(\vv{B})=C'@s[\vv{(\Gamma_i).B}/\vv{X}]@\id{\Gamma}=C'[\vv{(\Gamma_i).B}/\vv{X}]@s@\id{\Gamma}
       =\widehat{C'}(\vv{B})[s/y]\\
     \end{align*}

     We then got the following prooftree
     \begin{prooftree}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n\rat*\mid\Gamma_2'\vdash C'@s:\Gamma_2[s/y]\rat*$}
       \RightLabel{(*)}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma_2'\vdash C':(y:D,\Gamma_2)\rat*$}
       \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
       \RightLabel{IdH.}
       \BinaryInfC{$\Gamma_2',y:D,\Gamma_2,x:\widehat{C'}(\vv{A})\vdash\widehat{C'}(\vv{t}):\widehat{C'}(\vv{B}) $}
       \UnaryInfC{$\Gamma_2',\Gamma_2[s/y],x:\widehat{C'}(\vv{A})[s/y]\vdash\widehat{C'}(\vv{t})[s/y]:\widehat{C'}(\vv{B})[s/y] $}
     \end{prooftree}
     (=*=) This is the reverse of *(Ty-Inst)*.

   +
     #+begin_export latex
     $\mathcal{D}= $
     \AxiomC{$X_1:\Gamma_1,\ldots,X_n:\Gamma_n\mid\Gamma',y:D\vdash C':\Gamma\rat* $}
     \ParamAbstr{$X_1:\Gamma_1,\ldots,X_n:\Gamma_n\mid\Gamma'\vdash (y).C':(y:D,\Gamma)\rat* $}
     \DisplayProof

     #+end_export

     Calculated type actions:
     \begin{align*}
       \widehat{(y).C'}(\vv{A})&=(y).C'[\vv{(\Gamma_i.A)}/\vv{X}]@\id{\Gamma}\\
                          &=(y).(C'[\vv{(\Gamma_i.A)}/\vv{X}])@y@\id{\Gamma}\\
                          &\longleftrightarrow_T(C'[\vv{(\Gamma_i.A)}/\vv{X}])@\id{\Gamma}\\
                          &=\widehat{C'}(\vv{A})\\
       \widehat{(y).C'}(\vv{t})&=\widehat{C'}(\vv{t})\\
       \widehat{(y).C'}(\vv{B})&=(y).C'[\vv{(\Gamma_i.B)}/\vv{X}]@\id{\Gamma}\\
                          &=(y).(C'[\vv{(\Gamma_i.B)}/\vv{X}])@y@\id{\Gamma}\\
                          &\longleftrightarrow_T(C'[\vv{(\Gamma_i.B)}/\vv{X}])@\id{\Gamma}\\
                          &=\widehat{C'}(\vv{B})\\
     \end{align*}

     The prooftree then becomes the following
     \begin{prooftree}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma'\vdash (y).C':(y:D,\Gamma)\rat*$}
       \RightLabel{(*)}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid y:D,\Gamma'\vdash C':\Gamma\rat*$}
       \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
       \RightLabel{IdH.}
       \BinaryInfC{$y:D,\Gamma',\Gamma,x:\widehat{C'}(\vv{A})\vdash\widehat{C'}(\vv{t}):\widehat{C'}(\vv{B})$}
     \end{prooftree}
     (=*=) This is the reverse of *(Param-Abstr)*.

   +
      $\mathcal{D}$ =
              \begin{prooftree}
                  \Di{1}
                  \UnaryInfC{$\sigma_k:\Delta_k\triangleright\Gamma$}
                  \Di{2}
                  \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n\rat*,X:\Gamma\rat*\vdash A_k:*$}
                  \FPTy
                  \BinaryInfC{$\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}):\Gamma\rat*$}
              \end{prooftree}

          Calculated type actions:
          \begin{align*}
            &\widehat{\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{A})\\
            &=\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})[\vv{(\Gamma_1).A}/\vv{X}]@\id{\Gamma_2}\\
            &=\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_1).A}/\vv{X}])@\id{\Gamma_2}\\
            &\widehat{\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{t})\\
            &=\text{rec}^{\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[(\Gamma_1).A/X])}\vv{(\Delta_k,x).\alpha_k@\id{\Delta_k}@\widehat{D_k}(\vv{t},x)}@\id{\Gamma_2}@x\\
            &\widehat{\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{B})\\
            &=\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})[\vv{(\Gamma_1).B}/\vv{X}]@\id{\Gamma_2}\\
            &=\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_1).B}/\vv{X}])@\id{\Gamma_2}
          \end{align*}

         From the assumptions
          \begin{align*}
          &X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\emptyset\vdash \mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}):\Gamma_2\rat*\\
          &\Gamma_i,x:A_i\vdash t_i:B_i
          \end{align*}
         We have to proof that in *Ctx*
         \begin{equation*}
          \Gamma_2,x:\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[(\Gamma_1).A/X])@\id{\Gamma_2}
         \end{equation*}
         the expression
         \begin{equation*}
          \text{rec}^{\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).A}/\vv{X}])}\vv{(\Delta_k,y).\alpha_k@\id{\Delta_k}@\widehat{D_k}(t,y)}@\id{\Gamma_2}@x
         \end{equation*}
         has type
         \begin{equation*}
         \mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).B}/\vv{X}])@\id{\Gamma_2}
         \end{equation*}
         We can use the induction hypothesis
         \begin{prooftree}
           \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*,Y:\Gamma_{n+1}\rat*\mid\Delta_k\vdash D_k:*$}
           \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
           \BinaryInfC{$\Delta_k,x:\widehat{D_k}(\vv{A},A_{n+1})\vdash\widehat{D_k}(\vv{t},y):\widehat{D_k}(\vv{B},B_{n+1}) $}
         \end{prooftree}
    We than got the following proof
    \begin{prooftree}
     \AxiomC{$\Gamma_2,x:\widehat{C}(\vv{A}),\Delta_k,y_k:D_k[\mu/X]\vdash\widehat{D_k}(\vv{t},y):D_k[\vv{(\Gamma_i).B}/\vv{X}][(\Gamma_{n+1}).B_{n+1}/Y]$}
     \UnaryInfC{$\Gamma_2,x:\widehat{C}(\vv{A}),\Delta_k,y_k:D_k[\mu/X]\vdash\alpha_k@\id{\Delta_k}@\widehat{D_k}(\vv{t},y):\mu@\sigma_k$}
     \UnaryInfC{$\Gamma_2,x:\widehat{C}(\vv{A})\vdash\widehat{C}(t):\widehat{C}(\vv{B})$}
    \end{prooftree}

   + $C=\nu(Y:\Gamma\rat*;\vv{\sigma};\vv{D})$:

     Calculated type actions:
     \begin{align*}
       &\widehat{\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{A})\\
       &=\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})[\vv{(\Gamma_i).A}/\vv{X}]@\id{\Gamma_2}\\
       &=\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).A}/\vv{X}])@\id{\Gamma_2}\\
       &\widehat{\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{t})\\
       &=\text{corec}^{\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[(\vv{\Gamma_i).B}/\vv{X}])}\vv{(\Delta_k,x)\widehat{D_k}(\vv{t},x)[(\xi_k@\id{\Delta_k}@x)/x]}@\id{\Gamma_2}@x\\
       &\widehat{\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{B})\\
       &=\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})[\vv{(\Gamma_i).B}/\vv{X}]@\id{\Gamma_2}\\
       &=\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).B}/\vv{X}])@\id{\Gamma_2}
     \end{align*}

    From the assumptions
     \begin{align*}
     &X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma_2'\vdash \nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}):\Gamma_2\rat*\\
     &\Gamma_i,x:A_i\vdash t_i:B_i
     \end{align*}
    We have to proof that in *Ctx*
    \begin{equation*}
     \Gamma_2',\Gamma_2,x:\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[(\Gamma_1).A/X])@\id{\Gamma_2}
    \end{equation*}
    the expression
    \begin{equation*}
     \text{corec}^{\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[(\vv{\Gamma_i).B}/\vv{X}])}\vv{(\Delta_k,x)\widehat{D_k}(\vv{t},x)[(\xi_k@\id{\Delta_k}@x)/x]}@\id{\Gamma_2}@x\\
    \end{equation*}
    has type
    \begin{equation*}
    \nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).B}/\vv{X}])@\id{\Gamma_2}
    \end{equation*}
    We can use the induction hypothesis
    \begin{prooftree}
      \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*,Y:\Gamma_{n+1}\rat*\mid\Delta_k\vdash D_k:*$}
      \AxiomC{$\Gamma_i,y_k:A_i\vdash t_i:B_i$}
      \BinaryInfC{$\Delta_k,y_k:\widehat{D_k}(\vv{A},A_{n+1})\vdash\widehat{D_k}(\vv{t},y):\widehat{D_k}(\vv{B},B_{n+1}) $}
    \end{prooftree}
    We than got the following proof
    \begin{prooftree}
     \AxiomC{$\Gamma_2',\Gamma_2,x:\widehat{C}(\vv{A}),\Delta_k,y_k:\nu@\sigma_k\vdash\widehat{D_k}(\vv{t},x)[(\xi_k@\id{\Delta_k}@x)/x]:D_k[\vv{(\Gamma_i).A}/\vv{X}][\nu/X]$}
     \UnaryInfC{$\Gamma_2',\Gamma_2,x:\widehat{C}(\vv{A})\vdash\widehat{C}(t):\widehat{C}(\vv{B})$}
    \end{prooftree}

   #+end_proof

* Examples

  In this section we reiterate the examples from the paper.  We use our
  syntax, which is defined in [[Abstract Syntax]].

** Terminal Object

   The terminal object is a type which has exactly one value. In category
   theory every object in the category has a unique morphism to it. We define
   it as a coinductive type ~Terminal~ with one destructor ~Terminal~. It gets
   a Terminal and returns a Terminal. To get a Terminal value we use
   corecursion on the unit type, which is the first class terminal object.
   #+begin_example
   codata Terminal : Set where
      Terminal : Terminal -> Terminal
   terminal = corec Unit to Terminal where
                 { Terminal x = x } @ ()

   #+end_example

** Initial Object

   The initial object is a type which has no values. In category theory it is
   the object which has a unique morphism to every other object in the
   category. We define it inductively as ~Intial~ with one constructor
   ~Initial~. This constructor want's one value of the same type. We can't
   have a value of this type, because to get one we already need one. An
   shorter way to define this type would be a inductive type with no
   arguments. If we could get something of type ~Intial~, we could generate
   with ~exfalsum~ a value of arbitrary type ~C~.
   #+begin_example
   data Initial : Set where
      Initial : Intial -> Intial
   exfalsum<C : Set> = rec Initial to C where
                         Initial x = x
   #+end_example

** Natural Numbers

   We use the classical peano numbers to define natural numbers.  Therefor we use
   the inductive type ~Nat~ with the constructors ~Zero~ and ~Suc~. ~Zero~ is
   just the number zero. Every constructor has to have a argument, which can
   contain a recursive occurrence. Every Type ~A~ is isomorphic to the
   function type ~Terminal -> A~. So we use ~Terminal~ for this occurrence.
   ~Suc~ is the successor. So the meaning of ~Suc n~ is $n+1$
   #+begin_example
   data Nat : Set where
      Zero : Terminal -> Nat
      Suc : Nat -> Nat
   zero = Zero @ ()
   #+end_example

** Binary Product

   The product is defined as a coinductive type.  It has two destructors.
   The first gives back the first element.  And the second the second.
   The types A and B have to be concrete types.  We don't have type
   polymorphism in our language.
   #+begin_example
   codata Product<A : Set, B : Set> : Set where
      Fst : Product -> A
      Snd : Product -> B
   pair<A : Set, B : Set> (x:A, y:B) = corec Unit where
                                         { Fst _ -> x
                                         ; Snd _ -> y} @ ()
   #+end_example
   For different types we have to define different Products.  We will write
   ProductNat for a product of two nats. ProductNatUnit is the prodcuct,
   where the first element is a Nat and the second a Unit.

*** Swap funtion
    We use the swap function on a product of 2 numbers, to illustrate
    how evaluation on a coinductive type works.  The swap function is
    defined as follows.
    #+begin_example
    swap<A : Set, B : Set> =
      corec Product<A,B> to Product<B,A> where
             Fst x -> Snd x
             Snd x -> Fst x
    #+end_example
    This is a well typed function as shown by the following proof
    \begin{prooftree}
    \AxiomC{$(A : *, B : *)\vdash$ Product<A,B> : $*$}
    \AxiomC{(x:Nat) $\vdash$ Snd @ x : ProductNat \textcircled{a}}
    \noLine
    \UnaryInfC{(y : Nat) $\vdash$ Fst @ y : ProductNat \textcircled{b}}
    \BinaryInfC{swap : (y : ProductNat) $\rat$ ProductNat}
    \end{prooftree}
    We show \textcircled{a} in the following proof.  \textcircled{b} works analog
    \begin{prooftree}
    \AxiomC{ProductNat : $*$}
    \UnaryInfC{$\vdash$ Snd (y : Nat) $\rat$ ProductNat}
    \AxiomC{Nat : $*$}
    \BinaryInfC{(x : Nat) $\vdash$ Snd (y :Nat) $\rat$ ProductNat}
    \AxiomC{Nat : $*$}
    \UnaryInfC{(x : Nat) $\vdash$ x : Nat}
    \BinaryInfC{(x : Nat) $\vdash$ Snd @ x : ProductNat}
    \end{prooftree}

** Binary Coproduct

   The Binary Coproduct corresponds to the Eiher type in haskell.  It is defined
   as an inductive type.  It is either ~A~ or ~B~.  We have one constructor ~Left~
   for ~A~ and one constructor Right for ~B~
   #+begin_example
   data Coproduct<A,B> : Set where
      Left : A -> Coproduct
      Right : B -> Coproduct
   #+end_example

** Pi Type

   The pi type is a generalization of the function type to dependent types.
   The type of the codomain or result of a function can depend on the value
   We define it as a coinductive type.  To destruct a function we just apply
   it to a value.  So the Destructor is ~Apply~.  To construct a function we
   use corecursion on on ~Unit~.  This is a lambda so we call it ~lambda~.

   #+begin_example
   codata Pi<A : Set,B : (x : A) -> Set> : Set where
      Apply : (x : A) -> Pi x -> B
   #+end_example

*** identity function
     The identity function is defined like this
     #+begin_example
     id<A : Set> = corec Unit to Pi<A,(v:A).A> where
            { Apply v p = v } @ ()
     #+end_example

     Evaluation on one goes as follows

    \begin{lstlisting}
    apply = Apply<Nat,(v : Nat).Nat>
    one = S @ (Z @ ())
    apply @ id<Nat> @ one
    = apply @ one @ ((corec Unit to Pi<Nat,(x:Nat).Nat> where
                        Apply v p = v ) @ ())
    $\succ \widehat{\text{Nat}} \left(\underbrace{
       \begin{subarray}{c}
         \text{corec Unit to Pi where} \\
         \text{\{Apply' v \_ = v\} @ x}
       \end{subarray}}_t\right)$[v/x][one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ ($\widehat{()}$(t,x))
         Succ x = Suc @ ($\widehat{Y}$(t,x)))@x[v/x][one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ ($\widehat{()}$(t))
         Succ x = Suc @ x)@x[v/x][one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ ($\widehat{()}$())
         Succ x = Suc @ x) @ x[v/x][one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ x
         Succ x = Suc @ x) @ x[v/x][one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ x
         Succ x = Suc @ x) @ v[one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ x
         Succ x = Suc @ x) @ one
    = one
    \end{lstlisting}
** Sigma Type

   The sigma type is a dependent pair of two types.  The second type can depend on
   the value of the first type.  It corresponds to to exists in logic.  We define
   it as an inductive type and call the constructor ~Exists~.
   #+begin_example
   data Sigma<A : Set,B : (x : A) -> Set> : Set where
      Exists : (x:A) -> B x -> Sigma
   #+end_example

** Vectors

   Vectors are a standard example for dependent type.  There are like lists, except
   there type depends on there length. For example a vector ~[1;2]~ has type
   ~Vector<Nat> 2~, because it length is 2. It has 2 constructors ~Nil~ and
   ~Cons~ like lists. ~Nil~ gives back the empty vector. Because the length of
   the empty vector is zero its return type is ~Vector 0~. The second
   constructor ~Cons~ takes a natural number ~k~ and a pair. The pair consists
   of ~A~ and a vector of length ~k~, a ~Vector k~. It returns a new vector.
   Its head is the first argument of the pair and its tail the second. So the
   results length is one more then the second argument of the pair. Therefore it
   is ~Vector (Suc k)~

   #+begin_example
   data Vector<A : Set> : (n:Nat) -> Set where
     Nil : Unit -> Vector zero
     Cons : (k:Nat) -> Product<A,Vector @ k> -> Vector (Suc @ k)
   nil<A : Set> = Nil<A : Set> @ ()
   #+end_example

*** Extend Function

    We use a function, which extends a vector to the end of a
    vector, to show how evaluation on a vector works.
    This tail function returns the empty vector for the empty vector,
    because every function has to be total in our language.  To keep
    things simple we use Unit for $A$. We also simplify "Product Unit
    (VectorUnit k)" to just "VectorUnit k"
    #+begin_example
    extend<A : Set>(x : A) = 
      rec Vec<A> to ((x).Vec<A> @ (Suc x) where
        Nil u = Cons<A> @ x @ nil<A>
        Cons k v = Cons<A> @ (Suc @ k) @ v
    #+end_example
    The type checking of this function goes as follows
    \begin{scprooftree}{0.8}
    \AxiomC{$\vdash$ (x).(VecUnit @ (Suc @ x)) : (k: Nat)}
    \noLine
    \UnaryInfC{(\_ : Unit) $\vdash$ ConsUnit @ 0 @ (NilUnit' @ ()) : (x).(VecUnit @ (Suc @ x)) @ 0}
    \noLine
    \UnaryInfC{(k : Nat, v : (x).(Vec @ (Suc @ x)) @ k) $\vdash$ ConsUnit @ (Suc @ k) @ v : (x).(Vec @ (Suc @ x)) @ (Suc @ k)}
    \UnaryInfC{$\vdash$ app : (k:Nat,y : (x).Vec (Suc x)) $\rat$ (x).(Vec @ (Suc x)) @ k}
    \end{scprooftree}
    As an example we evaluate a vector of length 1 with this function.  We choose length one
    to see all rec cases.
    \begin{align*}
      &\text{extend}@ 1 @ (\text{ConsUnit} @ 0 @ (\text{NilUnit'} @ ()))\\
      &= \text{extend}@(\text{Suc} @ k \bullet 0) @ (\text{ConsUnit} @ 0 @ (\text{NilUnit'} @ ()))\\
      &\succ \text{ConsUnit} @ (\text{Suc} @ k) @ v \left[ \hat{X}(\text{extend} @ n @ x)/v \right][0,\text{NilUnit'} @ ()]\\
      &= \text{ConsUnit} @ (\text{Suc} @ k) @ v \left[ \text{extend} @ n @ x/v \right][0, \text{NilUnit'} @ ()]\\
      &= \text{ConsUnit} @ (\text{Suc} @ 0) @ (\text{extend} @ n @ x) [0,\text{NilUnit'} @ ()]\\
      &= \text{ConsUnit} @ (\text{Suc} @ 0) @ (\text{extend} @ 0 @ (\text{NilUnit'} @ ()))\\
      &= \text{ConsUnit} @ 1 @ (\text{extend} @ (0 \bullet 0) @ (\text{NilUnit'} @ ()))\\
      &\succ \text{ConsUnit} @ 1 @ (\text{ConsUnit} @ 0 @ (\text{NilUnit'} @ ()))\left[ \hat{()}(\text{extend} @ k @ x) / \_  \right][()]\\
      &= \text{ConsUnit} @ 1 @ (\text{ConsUnit} @ 0 @ (\text{NilUnit'} @ ()))[()]\\
      &= \text{ConsUnit} @ 1 @ (\text{ConsUnit} @ 0 @ (\text{NilUnit'} @ ()))
    \end{align*}

*** replicate function
    The following function gets a number $n$ and returns an vector of units
    with length $n$
    #+begin_example
    length = rec VectorUnit to Nat where
               NilUnit _ = zero
               ConsUnit k _ = Succ @ k
    replicate = lambda_Nat_VectorUnit n ((rec Nat to VectorUnit where
                                            Zero _ = NilUnit @ ()
                                            Suc   m = ConsUnit @ (length @ m) @ m) @ n)
    #+end_example
    The following shows the steps for evaluating /replicate/ on 1.  We omit
    the steps for /length/ and the inner /rec/, because we want to see how
    /corec/ evaluation works.  We will call the /rec/ part in the definition of
    /replicate/ /rep/.

    \begin{lstlisting}
    apply (lambda n (rep @ n)) 1
    = Apply @ 1 @ (lampda n (rep @ n))
    = Apply @ 1 @ ((corec Pi to Unit where
                     Apply n _ = rep @ n)@())
    $\succ$ $\widehat{\text{VecUnit}}(\underbrace{\text{corec Pi to Unit where \{ Apply n \_ = rep @ n \} @ x}}_t)$[rep@n/x][1,()]
    = (rec VecUnit to VecUnit where
         VecNil x = VecNil @ $\widehat{()}$(t,x)
         VecCons n x = VecCons @ n @ $\widehat{(y).Y}$(t,x))@n@x[rep@n/x][1,()]
    = (rec VecUnit to VecUnit where
         VecNil x = VecNil @ $\widehat{()}$(t,x)
         VecCons n x = VecCons @ n @ $\widehat{Y}$(t,x))@n@x[rep@n/x][1,()]
    = (rec VecUnit to VecUnit
         VecNil x = VecNil@()
         VecCons n x = VecCons'@n@x)@n@x[rep@n/x][1,()]
    = (rec VecUnit to VecUnit
         VecNil x = VecNil@x
         VecCons n x = VecCons@n@y)@n@(rep@n)[1,()]
    = (rec VecUnit to VecUnit
         VecNil x = VecNil@x
         VecCons n x = VecCons@n@x)@1@(rep@1)
    = ConsUnit @ (NilUnit @ ())
    \end{lstlisting}

** Maybe

** Extended Naturals

   We will now define extended naturals.  There are needed for the definitions
   of streams.  There are natural numbers with an additional value, infinty.  We
   define it coinductively with the predecessor as it only destructor.  The predecessor
   is either not defined (there is no predecessor of 0 in the natural numbers) or
   another natural number.  So we use a coproduct of ~Unit~ (which should mean: "has no
   predecessor") and another ~ExNat~.  We can define the successor as a corecursion.
   The predecessor of the successor of ~x~ is just ~x~.  So the only case of corec returns
   a ~Right x~ (remember Prec returns a coproduct not a number).
   #+begin_example
   codata ExNat : Set where
      Prec : ExNat -> Maybe<ExNat>
   succE = corec ExNat where
             Prec x -> Just<ExNat> @ x
   #+end_example

** Streams

   With extended naturals defined, we can now define partial streams.  This are streams
   which depend on there definition depth.  Like non-dependent
   streams they are coinductive and have 2 destructors for head and tail.
   #+begin_example
   codata PStr<A : Set>: (n: ExNat) -> Set where
      hd : (k : ExNat) -> PStr<A> (succE k) -> A
      tl : (k : ExNat) -> PStr<A> (succE k) -> PStr<A> @ k
   #+end_example
** List
   List A describes a list of type elements with type A.  It is defined
   as follows

   \begin{equation*}
   List A =  \mu(X:*;\epsilon_2;(\textbf{1},A\times X))
   \end{equation*}
   where $\Gamma_1=\emptyset$ and $\Gamma_2\vdash A:*$

   In the implemented syntax is written like this
   #+begin_example
   data List<A : Set> : Set where
      Nil : Terminal -> List
      Cons : Product<A,List> -> List
   nil<A : Set> = Nil<A> @ ()
   #+end_example

** Length function on lists of Units
   \begin{align*}
   \text{length} = \text{rec} &((y_k:\top).\alpha_1^\textbf{N}@\langle\rangle\\
                &,(x:\top,y_k:\mu(X:*;\epsilon_2(\mathbf{1},X)))).\alpha_2^\textbf{N} @ y_k\\
   \end{align*}
*** Type checking

    \begin{scprooftree}{0.6}
    \AxiomC{$\vdash\textbf{N}:*$}
    \AxiomC{$\vdash\alpha_1^\textbf{N}: (x:\textbf{1})\rightarrow\textbf{N}$}
    \RightLabel{\textbf{(Term-Weak)}}
    \UnaryInfC{$y_k:\textbf{1}\vdash\alpha_1^\textbf{N}: (x:\textbf{1})\rightarrow\textbf{N}$}
    \AxiomC{$\vdash\langle\rangle':\textbf{1}$}
    \RightLabel{\textbf{(Term-Weak)}}
    \UnaryInfC{$y_k:\textbf{1}\vdash\langle\rangle':\textbf{1}$}
    \RightLabel{\textbf{(Inst)}}
    \BinaryInfC{$y_k:\textbf{1}\vdash \alpha_1^\textbf{N}@\langle\rangle':\textbf{N}$}
    \AxiomC{$\vdash\alpha_2^\textbf{N}: (x:\textbf{N})\rightarrow\textbf{N}$}
    \RightLabel{\textbf{(Term-Weak)}}
    \UnaryInfC{$y_k:\textbf{N}\vdash\alpha_2^\textbf{N}: (x:\textbf{N})\rightarrow\textbf{N}$}
    \AxiomC{$\textbf{N}:*$}
    \RightLabel{\textbf{(Proj)}}
    \UnaryInfC{$y_k:\textbf{N}\vdash y_k:\textbf{N}$}
    \RightLabel{\textbf{(Inst)}}
    \BinaryInfC{$y_k:\textbf{N}\vdash \alpha_2^\textbf{N}@y_k:\textbf{N}$}
    \RightLabel{\textbf{(Ind-E)}}
    \TrinaryInfC{$\vdash \text{rec}((y_k).\alpha_1^\textbf{N}@\langle\rangle'
                    ,(y_k).\alpha_2^\textbf{N} @ y_k):(y:\text{List }\textbf{1})\rightarrow\textbf{N}$}
    \end{scprooftree}


** Rose Tree
   Rose Tree A = $\nu(X:*;\epsilon_2;(\textbf{1},List X)$

   #+begin_example
   data RoseTree<A : Set> : Set where
      Leaf : Terminal -> RoseTree
      Branch : List<RoseTree> -> RoseTree
   leaf<A : Set> = Leaf<A> @ ()
   #+end_example


\newpage   
\printbibliography
\newpage

* Appendix
** Type action derivation
\begin{landscape}
\begin{changemargin}{-1cm}{-1cm}
  \begin{prooftree}
   \AxiomC{$\Gamma_1\vdash\sigma:\Gamma_2$}
   \AxiomC{$\Gamma_3\vdash\tau:\Gamma_1$}
   \RightLabel{($*$)}
   \BinaryInfC{$\Gamma_3\vdash\sigma\circ\tau:\Gamma_2$}
  \end{prooftree}
  \begin{prooftree}
    \D
    \UnaryInfC{$\Delta,\Gamma_k,y_k:A_k[C/X]\vdash g_k:C@\sigma_k$}
    \RightLabel{TyAct}
    \UnaryInfC{$\Delta,\Gamma_k,x:A_k[\mu/X]\vdash g_k[\widehat{A_k}(\rec^\mu\overline{(\Gamma_k,y_k).g_k}@\id{\Gamma}@x/y_k]$}
    \D
    \UnaryInfC{$\Delta\vdash\tau:\Gamma_k$}
    \D
    \UnaryInfC{$\Delta\vdash u:A_k[\mu/X]$}
    \TrinaryInfC{$\Delta\vdash g_k[\widehat{A_k}(\rec^\mu\overline{(\Gamma_k,y_k).g_k}@\id{\Gamma}@x)/y_k][\tau,u]:C@\sigma_k$}
  \end{prooftree}
\begin{scprooftree}{0.93}
  \D
  \UnaryInfC{$\Delta,\Gamma_k,y_k:\Delta_k[C/X]\vdash g_k: C@\sigma_k$}
  \IndE
  \UnaryInfC{$\Delta\vdash\rec^\mu\overline{(\Gamma_k,y_k).g_k}:(\Gamma,x:\mu@\sigma_k)\rat C@\sigma_k$}
  \D
  \UnaryInfC{$\Gamma_k\vdash\sigma_k:\Gamma$}
  \D
  \UnaryInfC{$\Delta\vdash\tau:\Gamma_k$}
  \RightLabel{($*$)}
  \BinaryInfC{$\Delta\vdash\sigma_k\circ\tau:\Gamma$}
  \Inst{$\Delta\vdash(rec^\mu\overline{(\Gamma_k,y_k).g_k}@(\sigma_k\circ\tau)):(x:\mu@\sigma_k)\rat C@\sigma_k$}
  \AxiomC{}
  \IndI{$\Delta\vdash\alpha_k^\mu:(\Gamma_k,y:A_k[\mu/X])\rat\mu@\sigma_k$}
  \D
  \UnaryInfC{$\Delta\vdash\tau:\Gamma_k$}
  \Inst{$\Delta\vdash\alpha_k^\mu @\tau: (y:A_k[\mu/X])\rat\mu@\sigma_k$}
  \D
  \UnaryInfC{$\Delta\vdash u:A_k[\mu/X])$}
  \Inst{$\Delta\vdash\alpha_k^\mu @\tau@u:\mu@\sigma_k$}
  \Inst{$\Delta\vdash(\rec^\mu\overline{(\Gamma_k,y_k).g_k}@(\sigma_k\circ\tau))@(\alpha_k^\mu @\tau@u):C@\sigma_k$}
\end{scprooftree}
\begin{scprooftree}{0.63}
  \AxiomC{$\vdash\overbrace{\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}^C:\Gamma\rat*$}
  \AxiomC{}
  \IndI{$\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma},\Delta_k,y_k:A_k[\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]/X]\vdash\alpha_k^{\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}:(\Delta_k,y_k:A_k[\overline{(\Gamma_k).B_k}/\overline{X_k}])\rat\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]@\sigma_k$}
  \AxiomC{$\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma},\Delta_k,y_k:\overbrace{A_k[\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]/X]}^{=\widehat{A_k}(\overline{B},?)}\vdash\widehat{A_k}(\overline{t},y_k):\overbrace{A_k[\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]/X]}^{=\widehat{A_k}(\overline{B},?)}$}
  \Inst{$\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma},\Delta_k,y_k:A_k[\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]/X]\vdash\alpha_k^{\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}@\id{\Delta_k}@\widehat{A_k}(\overline{t},y_k):\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]@\sigma_k$}
  \IndE
  \BinaryInfC{$\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma}\vdash\rec^{\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}\overline{(\Delta_k,\overset{{\color{red}?}}{y_k}).\alpha_k^{\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}@\id{\Delta_k}@\widehat{A_k}(\overline{t},\overset{{\color{red}?}}{y_k})}:(\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma})\rat\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]@\id{\Gamma}$}
  \AxiomC{$\ldots$}
  \Inst{$\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma}\vdash\rec^{\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}\overline{(\Delta_k,\overset{{\color{red}?}}{y_k}).\alpha_k^{\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}@\id{\Delta_k}@\widehat{A_k}(\overline{t},\overset{{\color{red}?}}{y_k})}@\id{\Gamma}@x:\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]@\id{\Gamma}$}
\end{scprooftree}
\begin{scprooftree}{0.63}
  \AxiomC{$\vdash\overbrace{\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}^C:\Gamma\rat*$}
  \AxiomC{$\Gamma,x:\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma},\Delta_k,\overset{{\color{red}?}}{y_k}:\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\sigma_k\vdash\widehat{A_k}(\overline{t},\overset{{\color{red}?}}{y_k})[(\xi_k^{\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}@\id{\Delta_k}@\overset{{\color{red}?}}{y_k})/\overset{{\color{red}?}}{y_k}]:A_k[\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]/X]$}
  \IndE
  \BinaryInfC{$\Gamma,x:\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma}\vdash\corec^{\nu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}\overline{(\Delta_k,\overset{{\color{red}?}}{y_k}).\widehat{A_k}(\overline{t},\overset{{\color{red}?}}{y_k})[(\xi_k^{\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}@\id{\Delta_k}@\overset{{\color{red}?}}{y_k})/\overset{{\color{red}?}}{y_k}]}:(\Gamma,x:\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma})\rat\nu[\overline{(\Gamma_k).B_k}/\overline{X_k}]$}
  \AxiomC{$\ldots$}
  \Inst{$\Gamma,x:\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma}\vdash\corec^{\nu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}\overline{(\Delta_k,\overset{{\color{red}?}}{y_k}).\widehat{A_k}(\overline{t},\overset{{\color{red}?}}{y_k})[(\xi_k^{\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}@\id{\Delta_k}@\overset{{\color{red}?}}{y_k})/\overset{{\color{red}?}}{y_k}]}@\id{\Gamma}@x:\nu[\overline{(\Gamma_k).B_k}/\overline{X_k}]$}
\end{scprooftree}
\end{changemargin}
\end{landscape}
