#+LATEX_CLASS: scrbook
#+LATEX_CLASS_OPTIONs: [a4paper,cleardoubleempty,BCOR1cm]
#+LATEX_HEADER: \input{header}

#+TITLE: Implementation of Type Theory based on dependent Inductive and Coinductive Types
#+AUTHOR: Florian Engel

#+OPTIONS: toc:nil

\input{teaser}

\chapter*{Abstract}
  Dependent types are a useful tool to restrict possible type even further then
  types of strongly typed languages like Haskell. This gives us further type
  safety. With them we can also proof theorems. Coinductive types allow us to
  define types by their observations rather then by their constructors. This is
  useful for infinite types like streams. In many common dependently typed
  languages , like coq and agda, we can define inductive types which depend on
  values and coinductive types but not coinductive types, which depend on values

  In this work I will first give a survey of coinductive types in this languages
  and then implement the type theory from cite:basold2016type. This type theory
  has both dependent inductive types and dependent coinductive types. In this
  type theory the dependent function space becomes definable. This leads to a
  more symmetrical approach of coinduction in dependently typed languages.

#+TOC: headlines 2

* Introduction
  Through this work I will explain coinductive types at the examples of streams
  and functions. They will be generalized to partial streams and the Pi type in
  dependently typed languages. Streams are lists which are infinitely long. They
  are useful to modelling many IO interaction. For example a chat of a text
  messenger might be infinitely long. We can never know if the chat is finished.
  This is of course limited by the hardware, but we are interested in abstract
  models. Functions are used every where in functional programming. In most of
  this languages they are first-class objects. But in languages with coinductive
  types we can define them. If we only have types which are defined through
  induction or coinduction, we get a symmetrical language. This is useful,
  because than we can change a inductive type to a coinductive one and vice
  versa. It is straight forward to add function which destruct an inductive type
  by pattern matching on the constructor. But it is hard to add a new
  constructor. We then have to add this constructor to every pattern matching
  on that type. For coinductive types its the other way around. For more on this
  see cite:binder2019decomposition.  In the implemented syntax we can define
  partial streams like the following
  #+begin_example
  codata PStr<A : Set> : (n : Conat) -> Set where
    Hd : (k : Conat) -> PStr (succ @ k) -> A
    Tl : (k : Conat) -> PStr (succ @ k) -> PStr @ k
  #+end_example
  And the Pi type
  #+begin_example
  codata Pi<A : Set, B : (x : A) -> Set> : Set where
    Inst : (x : A) -> Pi -> B @ x
  #+end_example
  In chapter [[Coinductive Types]] we will see how coinductive can be defined. We
  will see in chapter [[Coinductive Types in dependent languages]] how they are
  defined in the dependently typed languages coq and agda.  In chapter
  [[Type Theory based on dependent Inductive and Coinductive Types]] we see how they
  are defined in cite:basold2016type.  We will then in chapter [[*Implementation]]
  explain how this theory is implemented.  At last we look at the examples from
  this paper in the implemented syntax.

* Coinductive Types
  Inductive types are defined via their constructors.  Coinductive types on
  the other hand are defined via their destructors.  In the paper cite:abel2013copatterns
  functions, which have coinductive types as their output, are implemented via
  copattern matching.  In this paper streams are defined like the following

  #+begin_example
  record Stream A = { head : A,
                      tail : Stream A }
  #+end_example

  The A in the definition should be a concrete type. The type system in the
  paper don't has dependent types. What differentiate this from regular record
  types (for example in Haskell), is the recursive field tail. So they call it a
  recursive record. In a strict language without inductive types we could never
  instantiate such a type, because to do this we already need something of type
  ~Stream A~ to fill in the field ~tail~. To remedy this the paper defines
  copattern matching. With the help of copattern matching we can define function
  which outputs expressions of type ~Stream A~. As an example we look at the
  definition of repeat. This function takes in an a of type Nat and generates an
  stream which just infinitely repeats it.

  #+begin_example
  repeat : Nat -> Stream Nat
  head (repeat x) = x
  tail (repeat x) = repeat x
  #+end_example

  As you can see copattern matching works via observations i.e. we define what
  should be the output of the fields applied to the function. This fields are
  also called observers, because we observe parts of the type. Because
  inhabitants of ~Stream~ are infinitely long we can't print out a stream.
  Because of this we also consider each expression with a coinductive type as a
  value. To get a subpart of this value we have to use observers . For example
  we can look at the third value of ~repeat 2~ via ~head (tail (tail (repeat
  2)))~ which should evaluate to 2. We can also implement a function which looks
  at the nth. value. Here it is.

  #+begin_example
  nth : Nat -> Stream A -> A
  nth 0     x = head x
  nth (S n) x = nth n (tail x)
  #+end_example

  As you can see we use ordinary pattern matching on the left hand side and
  observers on the right hand side. `nth 3 (repeat 2)` will output 2 as expected.
  Functions can also be defined via a recursive record.  It is defined like the
  following.

  #+begin_example
  record A -> B = { apply : A ~> B }
  #+end_example

  Here we differentiate between our defined function ~A -> B~ and ~~>~ in the
  destructor. Constructor application or, as is the case here, destructor
  application is not the same as function application, like in Haskell. In the
  paper ~f x~ means ~apply f x~. We will also use this convention in the
  following. In fact we already used in the definitions of the functions
  ~repeat~ and ~nth~. ~nth 0 x~ is just a nested copattern. We can also write it
  with `apply` like so: ~apply (apply nth 0) x = head x~. Here we use currying.
  So first apply is the sole observer of type ~Stream A -> A~ and the second of
  type ~Nat -> (Stream A -> A)~.

* Coinductive Types in dependent languages
  In this section we will look how coinductive types are implemented in
  dependently typed language. In dependently typed languages types can depend on
  values. The classical example for such a type is the vector. Vectors are like
  list, except their length is contained in their type. For example a vector of
  natural numbers of length 2 has type ~Vec Nat 2~. This type depends on two
  things. Namely the type ~Nat~ and the value ~2~, which is itself of type ~Nat~.
  We can define vectors in coq like follows.
  #+begin_src coq
  Inductive Vec (A : Set) : nat -> Set :=
    | Nil : Vec A 0
    | Cons : forall {k : nat}, A -> Vec A k -> Vec A (S k).
  #+end_src
  A Vector has two constructors.  One for the empty vector called ~Nil~ and one to append a
  element at the front of a vector called ~Cons~.  The difference to list is the second argument
  to the type constructor ~Vec~. It is 0 for ~Nil~.  And ~Cons~ gets an ~A~ and a vector of length ~k~.  It
  returns a vector of length ~S k~ (~S~ is just the successor of k).
  They can also be defined in agda like follows.
  #+begin_src agda
  data Vec (A : Set) : ℕ → Set where
    Nil : Vec A 0
    Cons : {k : ℕ} → A → Vec A k → Vec A (suc k)
  #+end_src
  One advantage over of vectors over list is that we can define a total function
  (a function which is defined for every input) which takes the head of a
  vector. This function can't be total for lists, because we can't know if the
  input list is empty. A empty list has no head. For vectors we can enforce in
  coq like follow.
  #+begin_src coq
  Definition hd {A : Set} {k : nat} (v : Vec A (S k)) : A :=
    match v with
    | Cons _ x _ => x
    end.
  #+end_src
  We just pattern match on ~v~.  The only patter is for the ~Cons~ constructor.  The ~Nil~ constructor
  is a vector of length 0.  But ~v~ has type ~Vec A (S k)~.  So it can't be a vector of length 0.
  In agda the function looks like follow.
  #+begin_src agda
  hd : {A : Set} {k : ℕ} → Vec A (suc k) → A
  hd (cons x _) = x
  #+end_src
  That terms can occur in types makes it necessary to ensure that function
  terminate. Otherwise type checking wouldn't be decidable. If we have a
  function ~f : Nat -> Nat~ and we want to check a value ~a~ against a type ~Vec
  (f 1)~ we have to know what ~f 1~ evaluates to. So ~f~ has to terminate.  We check
  termination in coq via a structural decreasing argument.  A argument is structural decreasing, if
  it is structural smaller in a recursive call.  Structural smaller means it is a recursive occurrence
  in a constructor.  As an example we look at the definition of the natural numbers and the add function
  on them.  We define the natural numbers in coq like follows.
  #+begin_src coq
  Inductive nat : Set :=
  | O : nat
  | S : nat -> nat.
  #+end_src
  ~O~ is the constructor for 0 and ~S~ is the successor of its argument. Here
  the recursive argument to ~S~ is structural smaller than S applied to it i.e.
  ~n~ is structural smaller than ~S n~. Then we can define addition like follows
  #+begin_src coq
  Fixpoint add (n m:nat) : nat :=
  match n with
  | O => m
  | S p => S (add p m)
  end.
  #+end_src
  In the recursive call the first argument is structural decreasing. ~p~ is
  smaller than ~s p~. So coq accepts this definition.  The classical example
  for a function where a argument is decreasing, but not structural decreasing
  is quicksort.  A naive implementation would be the following.
  #+begin_src coq
  Fixpoint quicksort (l : list nat) : list nat :=
  match l with
  | nil => nil
  | cons x xs => match split x xs with
                | (lower, upper) => app (quicksort lower) (cons x (quicksort upper))
                end
  end.
  #+end_src
  Here ~split~ is just a function which gets a number and a list of numbers.
  It gives back a pair of two lists where the left list are all elements of
  the input list which are smaller than the input number and the right this
  which are bigger.  It is clear that this lists can't be longer than the
  input list.  So ~lower~ and ~upper~ can't be longer than ~xs~.  Here ~xs~ is
  structural smaller than the input ~cons x xs~.  So ~lower~ and ~upper~ are smaller
  than the input.  Therefore we know that ~quicksort~ is terminating.  But coq won't
  accept our code, because no argument is structural decreasing.


  For coinductive types termination means that functions which produce them
  should be productive. If a function is productive it produces in each step a
  new part of the infinitely large coinductive type.

  In section [[Coinductive Types in Coq]] we will look at the implementation in coq.
  There are two ways to define them. The older way uses positive coinductive
  types. This is known to violate subject reduction. Therefore it is highly
  discouraged to use them. To fix this the new way uses negative coinductive
  types the new way uses negative coinductive types. In section [[Coinductive
  Types in Agda]] we look at the implementation in agda. Agda also has the two
  ways of defining such types. On special thing about it, is that it implements
  copattern matching. To help agda with termination checking we can use sized
  types.
** Coinductive Types in Coq
   There are two approaches to define coinductive types in coq. The older one is
   described in [[Postive Coinductive Types]]. It works over constructors. Therefore
   they are called positive coinductive types. The newer and recommended one is
   described in section [[Negative Coinductive Types]]. They are defined over
   primitive records (a relatively new feature of coq). Therefore they are
   called negative coinductive Types.

*** Postive Coinductive Types
   Positive coinductive types are defined over constructors in coq.  The keyword
   ~CoInductive~ is used to indicate that we about to define a coinductive type.
   This is the only syntactical difference from the definition of inductive
   types. For example streams are defined like the following.

   #+begin_src coq
     CoInductive Stream (A:Set): Set :=
       Cons : A -> Stream A -> Stream A.
   #+end_src

   If this was a inductive type we couldn't generate of this type.  To generate values
   of coinductive types coq uses guarded recursion.  This checks if the recursive call
   to the function occurs as a argument to a coinductive constructor.  In addition to the
   guard condition the constructor can only nested in other constructors, fun or match
   expressions.  With all of this in mind we can define
   ~repeat~ like the following.

   #+begin_src coq
     CoFixpoint repeat (A:Set) (x:A) : Stream A := Cons A x (repeat A x).
   #+end_src

   Then we can produce the constant zero stream with ~repeat nat 0~. If we used
   a normal coq function i.e. write ~Fixpoint~ instead of ~CoFixpoint~ coq
   wouldn't except our code. It rejects it, because there is no argument which
   is structural decreasing. ~x~ stays always the same. ~CoFixpoint~ on the
   other hand only checks the previously mentioned conditions. It sees the
   recursive call ~repeat A x~ occurs as an argument to constructor ~LCons~ of
   the coinductive type ~Stream~. This constructor is also not nested. So our
   definition is accepted.

   We can use the normal pattern matching of coq to destruct a coinductive type.
   We define ~nth~ like the following.

   #+begin_src coq
     Fixpoint nth (A:Set) (n:nat) (s:Stream A) {struct n} : A :=
       match s with
         Cons _ a s' =>
         match n with 0 => a | S p => nth A p s' end
       end.
   #+end_src

   The guard condition is necessary to ensure every expression is terminating.
   If we didn't have the guard condition we could define the following.

   #+begin_src coq
     CoFixpoint loop (A : Set) : Stream A = loop A.
   #+end_src

   Here the recursive call doesn't occur in a constructor.  So the guard
   condition is violated.  With this definition the expression ~nth 0 loop~
   wouldn't terminate.  ~nth~ would try to pattern match on ~loop~.  But to
   succeed in that ~loop~ has to come has to unfold to something of the form
   ~Cons a ?~ which it never does.  So ~nth 0 loop~ will never evaluate to a
   value.  This would lead to undecidable type checking.

   We illustrate the purpose of the other conditions on a example taken from
   cite:chlipala2013certified.  First we implement the function ~tl~ like so.

   #+begin_src coq
     Definition tl A (s : Stream A) : Stream A :=
       match s with
       | Cons _ _ s' => s'
       end.
   #+end_src

   This is just one normal pattern match on ~Stream~.  If we didn't had the
   other condition we could define the following.

   #+begin_src coq
     CoFixpoint bad : Stream nat := tl nat (Cons nat 0 bad).
   #+end_src

   This doesn't violate the guard condition.  The recursive call ~bad~ is a
   argument to the constructor ~Cons~.  But the constructor is nested in a
   function.  If we would allow this, ~nth 0 bad~ would loop forever.  To
   understand why, we first unfold ~tl~ in ~bad~.  So we get

   #+begin_src coq
     nth 0 (cofix bad : Stream nat :=
              match (Cons 0 bad) with
              | Cons _ s' => s'
              end)
   #+end_src

   We can now simplify this to just

   #+begin_src coq
     nth 0 (cofix bad : Stream nat := bad)
   #+end_src

   After that ~bad~ isn't anymore an argument to a constructor.  Here we can also
   see easily that the expression ~cofix bad : Stream nat := bad~ loops for ever.
   So we never get the value at position ~0~.

   An important property of typed languages is subject reduction. Subject
   reduction says if we evaluate a expression $e_1$ of type $t$ to a expression
   $e_2$, $e_2$ should also be of type $t$. With positive coinductive types subject
   reduction is no longer valid. We illustrate this by Oury's counterexample
   cite:oury2008. First we define the codata type ~U~ as follows

   #+begin_src coq
    CoInductive U : Set := In : U -> U.
   #+end_src

   We can now define a value of u with the following ~Cofixpoint~ like so

   #+begin_src coq
     CoFixpoint u : U := In u.
   #+end_src

   This generates an infinite succession of ~In~.  We use the function ~force~
   to force ~U~ to evaluate one step i.e. ~x~ becomes ~In y~

   #+begin_src coq
     Definition force (x: U) : U :=
       match x with
         In y => In y
       end.
   #+end_src

   The same trick will be used to define ~eq~ which sates that ~x~ is
   definitional equal to ~force x~

   #+begin_src coq
     Definition eq (x : U) : x = force x :=
       match x with
         In y => eq_refl
       end.
   #+end_src

   This first matches on x to force it, to reduce to ~In y~. Then the new goal
   becomes ~In y = force (In y)~. ~force (In y)~ evaluates to just ~In y~, as it
   is just pattern matching on ~In y~. So the final goal is ~In y = In y~ which
   can be shown by ~eq_refl~. ~eq_refl~ is a constructor for ~=~, where both
   sides of ~=~ are exactly the same. If we now instantiate ~eq~ with ~u~ we
   become ~eq u~

   #+begin_src coq
     Definition eq_u : u = In u := eq u
   #+end_src

   But ~u~ is not definitional equal to ~In u~.  As mentioned above expression
   with a coinductive type are always values to prevent inifinite evaluation.
   So ~In u~ is a value and ~u~ is also a value.  But values are only
   definitional equal, if they are exactly the same.  The next section will
   solve this problem through negative coinductive types.

*** Negative Coinductive Types
    In coq 8.5. primitive record were introduced.
    With this it is now possible to define types over there destructors.  So we
    can have negative , especially negative coinductive, types in coq.  With
    primitive records we can define streams like the following

    #+begin_src coq
      CoInductive Stream (A : Set) : Set :=
        Seq { hd : A; tl : Stream A }.
    #+end_src

    Now we cant define ~repeat~ over the fields of ~Stream~

    #+begin_src coq
      CoFixpoint repeat (A:Set) (x:A) : Stream A :=
        {| hd := x; tl := repeat A x|}.
    #+end_src

    To define ~repeat~ we must define what is the head of the constructed stream
    and what it is tail.  The guard conditions say now that corecursive
    occurrences must be guarded by a record field.  We can see that the
    corecursive call ~repeat~ is a direct argument to the field ~tl~ of the
    corecursive type ~Stream A~.  This means coq accepts the above definition.
    If we want to access parts of a stream we use the destructors ~hd~ and
    ~tl~.  With them we can define nth again for the negative stream.

    #+begin_src coq
      Fixpoint nth (A : Set) (n : nat) (s : Stream A) : list A :=
        match n with
        | 0 => nil
        | S n' => s.(hd A) :: nth A n' s.(tl A)
        end.
    #+end_src

    With negative coinductive types we can't form the above mentioned
    counterexample to subject reduction anymore, because we can't pattern match
    on negative types. Oury's example becomes.

    #+begin_src coq
      CoInductive U := { out : U }.
    #+end_src

    ~U~ is now defined over its destructor ~out~, instead of its constructor ~in~.
     Then ~in~ becomes just a function.  In Fact its just a definition, because
     we don't recurse or corecurse on it.

    #+begin_src coq
      Definition In (y : U) : U := {| out := y |}.
    #+end_src

    We define it over the only field ~out~.  When we put a ~y~ in then we get
    the same ~y~ out.  We can also again define ~u~

    #+begin_src coq
      CoFixpoint u : U := {| out := u |}.
    #+end_src

    ~u~ With coinductive types it is know possible to define the pi type.

    #+begin_src coq
      CoInductive Pi (A : Set) (B : A -> Set) := { Apply (x : A) : B x }.
    #+end_src

    The Pi type is defined over its destructor ~Apply~.  If we evaluate ~Apply~
    on a value of Pi (which is a function) and an argument, we get the result
    i.e. we apply the value to the function.  It looks like the Pi type becomes definable
    in coq.  But we are cheating.  The type of ~Apply~ is already a Pi type.  This is because
    we identify constructors and destructors with functions.  We will see that the theory of
    the paper avoids this identification. To define a function we use
    ~CoFixpoint~.  As a simple non recursive, non dependent example we use the
    function ~plus2~.

    #+begin_src coq
      CoFixpoint plus2 : Pi nat (fun _ => nat) :=
        {| Apply x  := S (S x) |}.
    #+end_src

    If we apply (i.e. call the destructor ~Apply~) an ~x~ to plus2 we give back
    ~S (S x)~.  Which is twice the successor on ~x~.  So we add 2 to ~x~.  We
    use ~_~ here because ~plus2~ is not a dependent function i.e. the result
    type ~nat~ doesn't depend on the input value.  To define function with more
    than one argument we just use currying i.e. we use the type ~Pi~ as the
    second argument ~Pi~. For example a 2-ary non-dependent function from ~A~
    and ~B~ to ~C~ would have type ~Pi A (fun _ => Pi B (fun _ => C))~.  It
    would be fortunate if we could define ~plus~ like the following.

    #+begin_src coq
      CoFixpoint plus : Pi nat (fun _ => Pi nat (fun _ => nat)) :=
        {| Apply := fun (n : nat)  =>
             match n with
             | O => {| Apply (m : nat) := m |}
             | S n' => {| Apply m := S (Apply _ _ (Apply _ _  plus n') m) |}
             end
        |}.
    #+end_src

    But coq doesn't accept this definition.  The guard condition is violated.
    ~plus n'~ is not a direct argument of the field ~Apply~.  The definition
    should terminate because we are decreasing ~n~ and the case for ~0~ is
    accepted.  In the case for ~0~, there is no recursive call.

    We can also define a dependent function.  We define append2Units like
    follows
    #+begin_src coq
    CoFixpoint append2Units : Pi nat
                                 (fun n => Pi (Vec unit n)
                                           (fun _ => Vec unit (S (S n)))) :=
      {| Apply n := {| Apply v := Cons _ tt (Cons _ tt v) |} |}.
    #+end_src
    This just appends 2 units at a vector of length ~n~.

** Coinductive Types in Agda
   In agda coinductive types where first also introduced as positive types.
   In the section [[Positive Coinductive Types in Agda]] we will look at them in
   detail.  In section [[Negative Coinductive Types in Agda]] we describe the
   correct way to implement coinductive types in agda. There are function which
   terminate but are rejected by the type checker. In fact in any total language
   there have to be such functions. We can show that by trying to list all
   total functions. The following table lists functions per row. The columns say
   what the output of the functions to the given input is
   |          |        1 |        2 |        3 |        4 | $\dots$  |
   |----------+----------+----------+----------+----------+----------|
   | $f_1$    |        2 |        7 |        8 |        6 | $\dots$  |
   | $f_2$    |        4 |        4 |        6 |       19 | $\dots$  |
   | $f_3$    |        6 |      257 |        1 |        2 | $\dots$  |
   | $f_4$    |        7 |      121 |    23188 |     2313 | $\dots$  |
   | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\ddots$ |
   We can now define a function $g(n)=f_n(n)+1$ this function is total and not
   in the list, because it is different to any function in the list for at least
   on input To allow more functions we can use a unique feature of agda, sized
   types. They are described in section [[Termination Checking with Sized Types]].

*** Positive Coinductive Types in Agda
   Agda doesn't has a special keyword to define coinductive types like coq.  It
   uses the symbol $\infty$ to mark arguments to constructors as coinductive.
   This symbol says that the computation of arguments of this type are suspended.
   $\infty$ is just a type constructor.  So agda ensures productivity over type
   checking. We define streams like so

   #+begin_src agda
     data Stream (A : Set) : Set where
       cons : A → ∞ (Stream A) → Stream A
   #+end_src

   Here the second argument to cons is marked with $\infty$. This is the tail of
   the stream. Because it is infinitely long (we don't have a constructor of an
   empty stream) we can't compute it completely, so we suspend the computation.
   We can delay a computation with the constructor $\sharp$ and force it with
   the function $\flat$. They're types are given below

   #+begin_src agda
     ♯_ : ∀ {a} {A : Set a} → A → ∞ A
     ♭  : ∀ {a} {A : Set a} → ∞ A → A
     #+end_src

   We can now again define our usual functions.  We begin with ~repeat~

   #+begin_src agda
     repeat : {A : Set} → A → Stream A
     repeat x = cons x (♯ (repeat x))
   #+end_src

   We first apply ~Cons~ to ~x~. So the head of the stream is ~x~. We then apply
   it to the corecursive call ~repeat~. So the tail will be a repetition of xs.
   We have to call the ~repeat~ with $\sharp$ to suspend the computation.
   Otherwise the code doesn't type check. If we would write this function
   without $\sharp$ on a stream which has no $\infty$ on the second argument of
   ~cons~, the function would run forever. In fact the termination checker won't
   allow us to write such an function. We can also write ~nth~ again, which
   consumes a stream

   #+begin_src agda
     nth : {A : Set} → ℕ → Stream A → A
     nth 0       (cons x _)  = x
     nth (suc n) (cons _ xs) = nth n (♭ xs)
   #+end_src

   Here we have to use $\flat$ on the right hand side of the second case, to
   force the computation of the tail of the input stream.  We have to do that
   because ~nth~ wants a stream.  It doesn't want a suspended stream.
   Productivity on coinductive types like stream is checked by only allowing non
   decreasing recursive calls behind the $\sharp$ constructor
**** TODO Look up and cite it

*** Negative Coinductive Types in Agda
    In agda we can also define negative coinductive types.  This is the
    recommended way.  Agda implements the previously mentioned copattern matching.
    We can define a record with the keyword ~record~.  We use the keyword ~coinductive~
    to make it possible to define recursive fields.  Stream is defined like the
    following.

    #+begin_src agda
      record Stream (A : Set) : Set where
        coinductive
        field
          hd : A
          tl : Stream A
    #+end_src

    A Stream has 2 field. ~hd~ is the head of the stream. It has type ~A~. ~tl~
    is the tail of the stream. It is another stream, so it has type ~Stream A~.
    ~tl~ is a recursive field. So agda wouldn't accept the definition without
    ~coinductive~. Stream can never be empty. Every stream has a head (a field
    ~hd~) and an empty stream wouldn't have an head. So the tail of a stream can
    never be empty. Therefor every stream is infinitely long. We can now define
    ~repeat~ with copattern matching.

    #+begin_src agda
      repeat : ∀ {A : Set} → A → Stream A
      hd (repeat x) = x
      tl (repeat x) = repeat x
    #+end_src

    We have to copattern match on every field of ~Stream~, namely ~hd~ and ~tl~.
    Because agda is total it won't accept non-exhaustive (co)pattern matches
    like Haskell.  First we define what the head of ~repeat x~ is.  We just
    repeat ~x~ infinitely often.  So every element of the steam is ~x~, including
    the head.  Therefor we just write ~x~.  In the second and last copattern we
    define what the tail of the stream is.  The tail is just ~repeat x~.
    Infinitely often repeated ~x~ is the same as x and then infinitely repeated
    ~x~.  We can use normal pattern matchings and the destructors for functions
    which consume streams.  We define ~nth~ like the following.

   #+begin_src agda
     nth : ∀ {A : Set} → ℕ → Stream A → A
     nth zero s = hd s
     nth (suc n) s = nth n (tl s)
   #+end_src

   Here we just pattern match on the first argument (excluding the implicit
   argument of the type).  If it is zero the result is just the head of the
   stream.  If it is $n+1$ the result is the recursive call of ~nth~ on ~n~ and
   ~tl s~.  Agda accepts this code, because it is structural decreasing on the
   first (or second if we count the implicit) argument.

   We can also define the Pi type.  We use ~_$_~ as the apply operator.  This
   operator is taken from Haskell.

   #+begin_src agda
   record Pi (A : Set) (B : A → Set) : Set where
     field _$_ : (x : A) → B x
     infixl 20 _$_
   open Pi
   #+end_src

   like in coq we are using the first-class pi type to define the pi type. We
   can also define a function which adds 2 to a number ~plus2~ in agda.

   #+begin_src agda
    plus2 : ℕ →' ℕ
    plus2 $ x = suc (suc x)
   #+end_src

   We just use copattern matching to define it. If we apply an ~x~ to ~plus2~ we
   get ~suc (suc x)~. ~_→'_~ is just the non-dependent function it is defined
   using our pi type. Here it is

   #+begin_src agda
     _→'_ : Set → Set → Set
     A →' B = Pi A (λ _ → B)
     infixr 20 _→'_
   #+end_src

   In agda it becomes possible to define plus. We just use nested copattern
   matching.

   #+begin_src agda
    plus : ℕ →' ℕ →' ℕ
    plus $ 0       $ m = m
    plus $ (suc n) $ m = suc (plus $ n $ m)
   #+end_src

   If we change ~→'~ to ~→~ and remove ~$~ we get the standard definition for
   plus in agda.  We can also define a dependent function ~repeatUnit~ like follow
   #+begin_src agda
   repeatUnit : Pi ℕ (λ n → Vec ⊤ n)
   repeatUnit $ 0     = nil
   repeatUnit $ suc n = tt :: (repeatUnit $ n)
   #+end_src
   This function gives back a vector with the length of the input, where every element
   is unit.

*** Termination Checking with Sized Types
    They are many function, which are total but are not accepted by agda's
    termination checker.  For example we could try to define  division with
    rest on natural numbers like the following.

   #+begin_src agda
   _/_ :  ℕ → ℕ → ℕ
   zero / y = zero
   suc x / y = suc ( (x - y) / y)
   #+end_src

   The problem with this definition is that agda doesn't know that $x-y$ is
   smaller than $x+1$, which is clearly the case (x and y are positive).  This
   definition would work perfectly fine in a language without termination
   checking (like haskell).  Agda only checks an argument is structurally
   decreasing.  Here it is neither the case for l~x~ nor for ~y~.

   To remedy this problem sized types where introduced first to mini-agda (a
   language specifically developed to explore them) by cite:abel2010miniagda.
   Later they got introduced to agda itself. Sized types allow us to annote data
   with their size. Functions can use this sizes to check termination and
   productivity.

   We can now define the natural numbers depending on a size argument.
   #+begin_src agda
   data ℕ (i : Size) : Set where
     zero : ℕ i
     suc : ∀{j : Size< i} → ℕ j → ℕ i
   #+end_src
   The natural number now depends on a size ~i~.  The constructor ~zero~ is of
   arbitrary size ~i~.  ~suc~ gets an size ~j~ which is smaller than ~i~, a
   natural number of size ~j~ and gives back a natural number of size ~i~.  This
   means the size of the input is smaller than the size of the output.  For
   inductive types, a size is upper bound on the number of constructors.  With
   ~suc~ we add an constructor so the size has to increase  ~i~.  This means the
   size of the input is smaller than the size of the output.  For inductive
   types, a size is upper bound on the number of constructors.  With ~suc~ we
   add an constructor so the size has to increase.  We can now define
   subtraction on this sized nats.
   #+begin_src agda
   _-_ : {i : Size} → ℕ i → ℕ ∞ → ℕ i
   zero    - _      = zero
   n       - zero   = n
   (suc n) - (suc m) = n - m
   #+end_src
   Through the sized annotations, we know now that the result isn't larger than
   the first input.  $\infty$ means that the size isn't bound.  If the first
   argument is zero the result is also zero, which has the same type.  If the
   second argument is zero we return just the first.  In the last case both
   arguments are non-zero.  We call subtraction recursively on the predecessors
   of the inputs.  Here the size and both arguments are smaller.  So the
   function terminates.  Tough the type is smaller then $i$, the result type
   checks because sizes are upper bounds.  We can now define definition.
   #+begin_src agda
   _/_ : {i : Size} → ℕ i → ℕ ∞ → ℕ i
   zero  / _ = zero
   suc x / y = suc ( (x - y) / y)
   #+end_src
   From the definition of ~suc~ we now that the size of ~x~ is smaller than ~i~.
   Because the result of ~-~ has the same size as it's first input (here ~x~),
   we also now that ~(x - y)~ has the same size as ~x~. Therefor ~(x - y)~ is
   smaller than ~suc x~ and the function is decreasing on the first argument.
   Also, agda accepts this definition.

* Type Theory based on dependent Inductive and Coinductive Types
  In the paper cite:basold2016type a type theory, where inductive types and
  coinductive types can depend on values, is developed. For example we can, in
  contrast to the coinductive types of coq and agda, define streams which depend
  on their definition length. The theory differentiates types from terms. We
  don't have infinite universes, where a term in universe $n$ has a type in
  universe $n+1$(This is how it is done in coq cite:sozeau2014universe and agda
  cite:agdadocuniverselevels). Therefore types can only depend on values, not on
  other types. We only have functions on the type level. This function abstract
  over terms. For example $\lambda x.A$ is a type where all occurences of the
  term variable $x$ in $A$ are bound . We will see that functions are definable
  on the term level. We can apply types to terms. For example $A @ t$ means we
  apply the term $A$ to $x$. Every type has a kind. A kind is either $*$ or
  $\Gamma\rat*$. Here $\Gamma$ is a context, which states to what terms we can
  apply the type. For example we can apply $A$ of kind $(x:B)\rat*$ only to a
  term of type $B$. If we apply it to $t$ of type $B$, we get a type of kind
  $*$. We write $\rat$ instead of $\rightarrow$ to indicate, that this are not
  functions. We can also apply a term to annother term. For example $t@s$ means
  we apply the term $t$ to the term $s$.  Terms also can depend on contexts.
  For example if we have a term $t$ of type $(x:A)\rat B$ and apply it to a term
  $s$ of type $A$ we get a term of type $B$.  We can also define our own types.
  $\mu(X:\Gamma\rat*;\vv{\sigma};\vv{A})$ is an inductive type and
  $\nu(X:\Gamma\rat*;\vv{\sigma};\vv{A})$ is an coinductive type. $X$ is a
  variable which stands for the recursive occurrence of the type.  The $\vv{A}$ can
  contain this variable. There are also contexts $\vv{\Gamma}$, which are implicit
  in the paper.  $\sigma_k$ and $A_k$ can contain variables from $\Gamma_k$.
  $\sigma_k$ is a context morphism from $\Gamma_k$ to $\Gamma$.  A context
  morphism is a sequence of terms, which depend on $\Gamma_k$ and instantiate
  $\Gamma$. $\vv{\sigma}$, $\vv{A}$ and $\vv{\Gamma}$ are of same length.

  In this theory we can define partial streams on some type $A$ like the following.
  \begin{align*}
  &\text{PStr }A := \nu(X:(n:\text{Conat})\rat*;(\text{succ} @ n, \text{succ} @ n);(A, X @ n))\\
  &\text{with } \Gamma_1 = (n:\text{Conat}) \text{ and } \Gamma_n = (n:\text{Conat})
  \end{align*}
  Here ~succ~ is the successor on conats.  Conats are natural numbers with one
  additional element, infinity.  See [[Extended Naturals]] for their definition.
  Here the first destructor is the head.  It becomes a stream with lenght
  $\text{succ} @ N$ and returns an $A$.  The second destructor is the tail.  It
  becomes also an stream of length $\text{succ} @ N$.  It gives back an $X @ n$,
  which is a stream of length $n$.  We can also define the Pi type from $A$ to $B$,
  where $B$ can depend on $A$
  \begin{align*}
  &\Pi x:A.B := \nu(_:*;\epsilon_1;B)\\
  &\text{with } \Gamma_1 = (x:A)
  \end{align*}
  By ~_~ we mean, we are ignoring this variable. $\epsilon_1$ is one empty
  context morphism.  So the only destructor gives back an $B$ which can depend
  on $x$ of type $A$.  It is the function application.

  To construct a inductive types we use constructors (written
  $\alpha_k^{\mu(X:\Gamma\rat*;\vv{\sigma};\vv{A})}$ in the paper, which is the k'st
  constructor of the given type).  We can destruct it with recursion (written
  rec $\vv{(\Gamma_k.y_k).g_k}$).  Coinductive type work the other way around.
  We destruct them with destructors (written
  $\xi_k^{\nu(X:\Gamma\rat*;\vv{\sigma};\vv{A})}$) and construct them with
  corecursion (written corec $\vv{(\Gamma_k.y_k).g_k}$).

  We will give the rules for the theory in section [[Typing rules]] and a detailed
  explanation of the reduction in [[Evaluation]].

* Implementation
  In this section we look at the implementation details.  We use the functional
  programming language haskell.  Haskell is a pure language.  This means
  functions which aren't in the IO monad have no side effect.  The only IO we
  are doing is reading a file and as the last step printing it.  Because
  everything between is pure, we can test it without bordering on side effects.
  Another feature of haskell, which will be get useful in our implementation is
  pattern matching.  We will see its usefulness in section [[Typing rules]].

  In section [[Abstract Syntax]] we will develop the abstract syntax of our language
  from the rare syntax in the paper.  Then we rewrite the typing rules in [[*Typing rules]].
  At last we look at the implementation of the reduction in [[*Evaluation]]
** Abstract Syntax
   In the following we will scratch out the abstract syntax. We will give every
   inductive and coinductive type a name. They will be defined via statements.
   We will also be able to bind expressions to names. This will be described in
   section [[Statements]] . In section [[Expressions]] we will define the syntax of
   expressions. This will mostly be in one to one correspondence with the syntax of
   the paper. Note however that we use the names of the constructors instead of anonymous
   constructors together with their type and number.  Also the order of the matches in
   rec and corec is irrelevant.  We use the names of the Con/Destructors to identify them.
   Here we can't write anonymous inductive and coinductive types.  We have to refer to the
   previously defined types.  In the following section [[Examples]] we will see how the examples
   from the paper look in our syntax.
*** Statements
    The abstract syntax is given in figure [[syntax-for-statements]].
    In the syntax "/Name/" , "$Constr_1\dots Contr_m$" and
    "$Destr_1\dots Destr_m$" are arbitrary distinct names. With the
    keywords data and codata we define inductive and coinductive types
    respectively. After that we will write the name. Behind that we can give a
    parameter context. This is a type context. This types are not polymorphic.
    They are merely macros to make the code more precise. If we want to use this
    Type we have to fully instantiate this context. This types can occur
    everywhere in the definition where a type is expected. A (co)inductive Type
    can have a context, which is written before an arrow. ~Set~ stands for type
    (or * in the paper). If a type don't has a context we omit the arrow. We
    will also give names to every constructor and destructor. Constructors and
    destructors also have contexts. Additionally they have one argument which
    can has a recursive occurrence of the type we are defining. A constructor
    gives back a value of the type, where its context is instantiated. This
    instantiation corresponds to the sigmas in the paper. If we write a name
    before a equal sign we can bind the following expression to the name. Every
    such defined name can depend on a parameter context and an argument context.
    We write the parameter context like in the case for data types behind the
    name. After that we can give a term context between round parenthesis.

    #+name: syntax-for-statements
    \begin{figure}
    \begin{lstlisting}
    statement =
      data Name<$C_1 : \Gamma_1$ -> *,$\dots$ ,$ C_n : \Gamma_n$ -> *> : $(x_1 : B_1,\dots,x_n : B_n)$ -> Set where
        $Constr_1$ : $(x_{1_1}:B_{1_1},\dots,x_{n_1}: B_{n_1})$ -> $A_1[Name/X]$ -> Name $\sigma_{1_1}\dots \sigma_{1_n}$
               $\vdots$                $\vdots$             $\vdots$            $\vdots$
        $Constr_m$ : $(x_{1_m}:B_{1_m},\dots,x_{n_m}: B_{n_m})$ -> $A_i[Name/X]$ -> Name $\sigma_{m_1}\dots \sigma_{m_n}$
     | codata Name<$C_1 : \Gamma_1$ -> *,$\dots$ ,$ C_n : \Gamma_n$ -> *> : $(x_1 : B_1,\dots,x_n : B_n)$ -> Set where
        $Destr_1$ : $(x_{1_1}:B_{i_1},\dots,x_{n_1}: B_{n_1})$ -> Name $\sigma_{1_1}\dots \sigma_{1_n}$ -> $A_1[Name/X]$
               $\vdots$                $\vdots$             $\vdots$            $\vdots$
        $Destr_m$ : $(x_{1_m}:B_{1_m},\dots,x_{n_m}: B_{n_m})$ -> Name $\sigma_{m_1}\dots \sigma_{m_n}$ -> $A_i[Name/X]$
     | name<$C_1 : Gamma_1$ -> *,$\dots$ ,$ C_n : Gamma_n$ -> *> $(x_1:A_1,\dots,x_n:A_n)$ = expr
    \end{lstlisting}
    \caption{Syntax for statements}
    \end{figure}


    The statements in Figure [[syntax-for-statements]] correspond to $\rho(X:\Gamma\rat*;\vv\sigma;\vv{A}):\Gamma\rat*$ as follows.
    + $x_1: B_1,\dots,x_n: B_n$ is $\Gamma$
    + /Name/ is X
    + $Constr_1,\dots, Contr_m$ stands for
      $\alpha_1^{\mu(X:\Gamma\rat *;\vv\sigma;\vv A)},\dots,\alpha_m^{\mu(X:\Gamma\rat *;\vv\sigma;\vv A)}$
    + $Destr_1,\dots, Destr_m$ stands for
      $\xi_1^{\mu(X:\Gamma\rat *;\vv\sigma;\vv A)},\dots,\xi_m^{\mu(X:\Gamma\rat *;\vv\sigma;\vv A)}$
    + $Name_i$ is $A_i[\Gamma/X]$
    + $(x_{1_1}:B_{1_1},\dots,x_{n_1}: B_{n_1}),\dots,(x_{1_m}:B_{1_m},\dots,x_{n_m}:B_{n_m})$
      stands for $\Gamma_1,\dots,\Gamma_m$
    + <$C_1 : Gamma_1$ -> *,$\dots$ ,$ C_n : Gamma_n$ -> *> are the parameter contexts.
      If we call a constructor we have to give this types, to relate the right type to it.

    To parse the abstract syntax we use megaparsec. The parser generates an
    abstract syntax tree, which is given for statements in Listing
    [[Abstract Syntax Tree for Statements]]. The field ~ty~ in ~ExprDef~ is used later in
    type checking. The parser just fills them in with ~Nothing~. data and codata
    definitions are both saved in ~TypeDef~. The haskell type ~OpenDuctive~ contains all the
    information for inductive and coinductive types. It corresponds to $\mu$ and`
    $\nu$ in the paper. We use an ~OpenDuctive~ where the field ~inOrCoin~ is ~IsIn~
    for $ \mu$ and an ~OpenDuctive~ where the field ~inOrCoin~ is ~IsCoin~ for
    $\nu$.  The haskell type ~StrDef~ ensures that the sigmas as and gamma1s have the
    same length.  We omit the implementation details for the parser, because we
    are manly focused on type checking.

    #+caption: Abstract Syntax Tree for Statements
    #+NAME: Abstract Syntax Tree for Statements
    #+begin_src haskell
      data Statement = ExprDef { name :: Text
                               , tyParameterCtx :: TyCtx
                               , exprParameterCtx :: Ctx
                               , expr :: Expr
                               , ty :: Maybe Type
                               }
                     | TypeDef OpenDuctive
                     | Expression Expr

      data OpenDuctive = OpenDuctive { nameDuc :: Text
                                     , inOrCoin :: InOrCoin
                                     , parameterCtx :: TyCtx
                                     , gamma :: Ctx
                                     , strDefs :: [StrDef]
                                     }

      data StrDef = StrDef { sigma :: [Expr]
                           , a :: TypeExpr
                           , gamma1 :: Ctx
                           , strName :: Text
                           }
    #+end_src
*** Expressions
    The abstract syntax for expression is given in figure [[syntax-for-expressions]].
    We will separate expression in expressions for terms and in expressions for
    types.  There are given as regular expressions in ~expr~ and ~typeExpr~ respectively.

    An ~expr~ is either the unit expression ~()~, an con/destructor, an
    application ~@~, an ~rec~ or an ~corec~. All con/destructors have to be
    instantiate with all variables in the parameter contexts of their types.
    This is done by giving types of the expected kinds separated by ',' enclosed
    in '<' and '>'. With the keyword ~rec~ we can destruct an inductive type.
    We write ~Type to typeExrp~, where ~Type~ is an previously defined inductive
    type after ~rec~ to facilitate type checking. It says we want to destruct a
    inductive type to some other type . We have to list all the constructor
    above one another. For each constructor we write an expression behind the
    equal sign, which should be of type ~TypeExpr~ which we have given above. In
    this expression we can use variables given in the match expression. The last
    one is the recursive occurrence. With the keyword ~corec~ we can do the same
    thing to construct a coinductive type. Here we have to swap the ~Type~ and
    the ~TypeExpr~ and list the destructors. We can also apply a expression to
    another with ~@~. The only primitive expression we have is the unit
    expression ~()~.

    The ~typeExpr~ is either the unit type ~Unit~, a lambda abstraction on
    types, an application or a variable. In the lambda expression we have to
    give the type of the variable. We apply a type to a term (types can only
    depend on terms) with ~@~. The unit type is the only primitive type
    expression.

    The generated abstract syntax tree is given in listing
    [[abstract-syntax-tree-for-expressions]]. The variables for expressions are
    separated in ~LocalExprVar~ and ~GlobalExprVar~. ~LocalExprVar~ should refer
    to variables which are only locally defined i.e. in ~Rec~ and ~Corec~. We
    use de-Brujin indexes for them.  This facilitates substitution which we will
    describe in section [[Substitution]].  ~GlobalExprVar~ refers to variables from
    definitions. Here we just use names. We do the same thing for ~LocalTypeVar~
    and ~GlobalTypeVar~. In the abstract syntax tree we use anonymous
    constructors like in the paper.  We combine them to the haskell constructor
    ~Iter~.  We now from the field ~ductive~ if it is a constructor or a destructor.
    The types in field ~parameters~ are to fill in the parameter context of the
    field ~ductive~
    this type. Here we know that it has to be an inductive type, because we
    don't have constructors for coinductive types. For Destructor, Rec and corec
    we also know if it is coinductive or inductive. The field ~nameStr~ in
    ~Constructor~ and ~Destructor~ are just for printing.

    #+name: syntax-for-expressions
    \begin{figure}
    \begin{lstlisting}
        expr :=
          rec Name<$C_1,\dots,C_n$> to typeExpr where
            match*
        | corec typeExpr to Name<$C_1,\dots,C_n$> where
            match*
        | expr @ expr | ()

        match := Name var* = expr

        typeExpr := Unit
                  | (var:typeExpr).typeExpr
                  | typeExpr @ expr
                  | Name
    \end{lstlisting}
    \caption{Syntax for expressions}
    \end{figure}

    #+name: abstract-syntax-tree-for-expressions
    #+caption: Abstract Syntax Tree for Expressions
    #+begin_src haskell
      data TypeExpr = UnitType
                    | TypeExpr :@ Expr
                    | LocalTypeVar Int Bool Text
                    | Parameter Int Bool Text
                    | GlobalTypeVar Text [TypeExpr]
                    | Abstr Text TypeExpr TypeExpr
                    | Ductive { openDuctive :: OpenDuctive
                              , parametersTyExpr :: [TypeExpr]}

      data Expr = UnitExpr
                | LocalExprVar Int Bool Text
                | GlobalExprVar Text [TypeExpr] [Expr]
                | Expr :@: Expr
                | Structor { ductive :: OpenDuctive
                           , parameters :: [TypeExpr]
                           , num :: Int
                           }
                | Iter { ductive :: OpenDuctive
                       , parameters :: [TypeExpr]
                       , motive :: TypeExpr
                       , matches :: [([Text],Expr)]
                       }
    #+end_src

** Substitution
   In the following we will write $t[s/x]$ for "substitute every free
   occurrences of $x$ in $t$ by $s$". Substitution is done in the module
   ~Subst.hs~. We use de-Bruijn indexes for bound variables to facilitate
   substitution. With this method every bound variable is a number instead of a
   string. The number says where the variable is bound. To find the binder of a
   variable we go outwards from and count every bounder until we reach the
   number of the variable. For example $\lambda.\lambda.\lambda.1$ says that the
   variable is bound by the second binder (we start counting at zero). This
   would be the same as $\lambda x.\lambda y. \lambda z.y$. This means we never
   have to generate fresh names. We just shift the free variables in the term
   with which we substitute by one, every time we encounter an binder. This
   shifting is done in the module ~ShiftFreeVars.hs~. We also want to be able to
   substitute multiple variables simultaneously. If we would just substitute one
   term after another we could substitute into a previous term. For example the
   substitution $x[y/x][z/y]$ would yield $z$ if we substitute sequential and
   $y$ if we substitute simultaneously.  To make simultaneous substitution
   possible every local variable has a boolean flag.  If this flag is set to
   true substitution won't substitute for that variable.  So for simultaneous
   substitution we just set this flag to true for all terms with witch we want
   to substitute.  Then we substitute with them.  In the last step we just have
   to set the flags to false in the result.  This setting(marking of the
   variables) is done in the module ~Mark.hs~.

** Typing rules
   A typing rule says that some expression or statement is of some type, given
   some premises. If we can for every statement or expression form a tree of
   such rules with no open premises, our program type checks. We have to rewrite
   the typing rules of the paper, to get rules which are syntax directed. Syntax
   directed means we can infer from the syntax alone what we have to check next
   i. e. which rule with which premises we have to apply. Here are the rules
   which have to be rewritten.
   + *(Ty-Inst)*
   + *(Param-Abstr)*
   This rules contain variables in the premises where their type isn't in the
   conclusion. So if we want to type-check something which is the conclusion of
   such a rule we have no way of knowing what this variables are.

   We don't need the weakening rules because we can lookup a variable in a
   context. So the following rules get removed.
   + *(TyVar-Weak)*
   + *(Ty-Weak)*
   The order in *TyCtx* isn't relevant so we use a Map for it.  The order
   of *Ctx* is relevant because types of later variables can refer to
   former variables and application instantiate the first varibale in
   *Ctx*.  We add a new Ctx for data types.  We also need a context for the
   parameters. *Ctx* can contain variables from this context, but not from
   *TyCtx*.

   We also rewrite the rules which are already syntax-directed to rules which
   work on our syntax.   We will mark semantic differences in the rewritten rules
   gray. We use variables $\Phi,\Phi',\Phi_1,\Phi_2,\dots$ for parameter contexts,
   $\Theta,\Theta',\Theta_1,\Theta_2,\dots$ for type variable contexts and
   $\Gamma,\Gamma',\Gamma_1,\Gamma_2,\dots$ for term variable contexts.

   In the module ~TypeChecker~ we will implement the following rules.  It
   defines a monad ~TI~ which can throw errors and has a reader on the contexts
   in which we are type checking.  To add something to a context we use the
   function ~local~.

*** Context rules
    The rules for valid contexts are already syntax directed so we take
    just them
    \begin{center}
    \AxiomC{}
    \UnaryInfC{$\vdash\emptyset$ \TyCtx}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\vdash\Theta$ \TyCtx}
    \AxiomC{$\vdash\Gamma$ \Ctx}
    \BinaryInfC{$\vdash\Theta,X:\Gamma\rat*$ \TyCtx}
    \DisplayProof
    \vskip 0.5em
    \AxiomC{}
    \UnaryInfC{$\vdash\emptyset$ \Ctx}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$|\emptyset|\Gamma\vdash A:*$}
    \UnaryInfC{$\vdash\Gamma,x:A$ \Ctx}
    \DisplayProof
    \end{center}
    In the rules for valid contexts we ensure that the types in the context can
    not depend on *TyCtx*.  Note however that they can depend on *ParCtx*.  This
    ensures that only strictly positive types are possible.

    We also need new rules for checking if a parameter context is valid.
    \begin{center}
    \AxiomC{}
    \UnaryInfC{$\vdash\emptyset$ \ParCtx}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\vdash\Phi$ \ParCtx}
    \AxiomC{$\vdash\Gamma$ \Ctx}
    \BinaryInfC{$\vdash\Phi,X:\Gamma\rat*$ \ParCtx}
    \DisplayProof
    \end{center}
    This is structural the same rule as \TyCtx.  The difference is that \ParCtx
    and \TyCtx are used differently in the other rules, as we have already seen
    in the rule \Ctx.

    We use the notation $\Theta(X)\rightsquigarrow\Gamma\rat*$ for looking up
    the type-variable $X$ in type-context $\Theta$ yields type $\Gamma\rat*$. We
    add 2 rules for looking up something in a type-context. They are:
    \begin{center}
      \AxiomC{$\vdash \Theta$ \TyCtx}
      \AxiomC{$\vdash \Gamma$ \Ctx}
      \BinaryInfC{$\Theta,X:\Gamma\rat*(X)\rightsquigarrow\Gamma\rat*$}
      \DisplayProof
      \hskip 1.5em
      \AxiomC{$\vdash \Gamma_1$ \Ctx}
      \AxiomC{$\Theta(X) \rightsquigarrow\Gamma_2\rat*$}
      \BinaryInfC{$\Theta,Y:\Gamma_1\rat*(X)\rightsquigarrow\Gamma_2\rat*$}
      \DisplayProof
    \end{center}
    Here $Y$ and $X$ are different variables

    The rules for looking up someting in a parameter context are principally the
    same.
    \begin{center}
      \AxiomC{$\vdash \Phi$ \ParCtx}
      \AxiomC{$\vdash \Gamma$ \Ctx}
      \BinaryInfC{$\Phi,X:\Gamma\rat*(X)\rightsquigarrow\Gamma\rat*$}
      \DisplayProof
      \hskip 1.5em
      \AxiomC{$\vdash \Gamma_1$ \Ctx}
      \AxiomC{$\Phi(X) \rightsquigarrow\Gamma_2\rat*$}
      \BinaryInfC{$\Phi,Y:\Gamma_1\rat*(X)\rightsquigarrow\Gamma_2\rat*$}
      \DisplayProof
    \end{center}

    Respectively the notation $\Gamma(x)\rightsquigarrow A$ means looking
    up the termvariable $x$ in term-context $\Gamma$ yields type $A$. The
    rules for term-contexts are:
    \begin{center}
      \AxiomC{$\vdash \Gamma$ \Ctx}
      \AxiomC{$\Gamma\vdash A:*$}
      \BinaryInfC{$\Gamma,x:A(x)\rightsquigarrow A$}
      \DisplayProof
      \hskip 1.5em
      \AxiomC{$\Gamma(x) \rightsquigarrow A$}
      \AxiomC{$\Gamma\vdash B:*$}
      \BinaryInfC{$\Gamma,y:B(x)\rightsquigarrow A$}
      \DisplayProof
    \end{center}

*** Full evaluation
    We write $A \longrightarrow_T^* B$ for evaluating $A$ as long as it
    is possible yields $B$.

    The rules are
    \begin{center}
    \AxiomC{$\neg\exists B : A \longrightarrow_T B$}
    \UnaryInfC{$A \longrightarrow_T^* A$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$A \longrightarrow_T B$}
    \AxiomC{$B \longrightarrow_T^* C$}
    \BinaryInfC{$A \longrightarrow_T^* C$}
    \DisplayProof
    \end{center}
*** Beta-equivalence
    We introduce a new rule for beta-equivalence.
    \begin{center}
    \AxiomC{$A\longrightarrow_T^* A'$}
    \AxiomC{$B\longrightarrow_T^* B'$}
    \AxiomC{$A'\equiv_\alpha B'$}
    \TrinaryInfC{$A\equiv_\beta B$}
    \DisplayProof
    \end{center}
    In the implementation $\equiv_\alpha$ is trivial, because we use /de
    Bruijn indices/.

    We also add some rule to check if two contexts are the same.
    \begin{center}
    \AxiomC{}
    \UnaryInfC{$\emptyset\equiv_\beta\emptyset$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\Gamma_1\equiv_\beta \Gamma_2$}
    \AxiomC{$A[\Gamma_1]\equiv_\beta B[\Gamma_2]$}
    \BinaryInfC{$\Gamma_1,x:A\equiv_\beta\Gamma_2,y:B$}
    \DisplayProof
 %   \vskip 0.5em
 %   \AxiomC{$\Theta_1\equiv_\beta \Theta_2$}
 %   \AxiomC{$\Gamma_1\equiv_\beta \Gamma_2$}
 %   \BinaryInfC{$\Theta_1,X:\Gamma_1\rat*\equiv_\beta\Theta_2,X:\Gamma_2\rat*$}
 %   \DisplayProof
    \end{center}

*** Unit type introduction
    The rule
    \begin{prooftree}
      \AxiomC{}
      \RightLabel{\textbf{($\top$-I)}}
      \UnaryInfC{$\vdash\top:*$}
    \end{prooftree}
    gets rewritten to
     \begin{prooftree}
      \AxiomC{}
      \RightLabel{\textbf{(Unit-I)}}
      \UnaryInfC{\graybox{$\Phi|\Theta|\Gamma$}$\vdash$Unit:$*$}
    \end{prooftree}
    We change the syntax "$\top$" to "Unit" and add *Ctx* and *TyCtx*.
    We will do this for every rule which has empty contexts to subsume
    the rules with *TyVar-Weak*, *Ty-Weak* and *Term-Weak*.

*** Type Variable introduction

     The rule
     \begin{prooftree}
      \AxiomC{$\vdash \Theta$ \TyCtx}
      \AxiomC{$\vdash \Gamma$ \Ctx}
      \TyVarI{$\Theta,X:\Gamma\rat*|\emptyset\vdash X : \Gamma \rat *$}
    \end{prooftree}
    gets rewritten to

     \begin{prooftree}
      \AxiomC{\graybox{$\Theta(X)\rightsquigarrow\Gamma\rat*$}}
      \AxiomC{\graybox{$\vdash \Gamma_1$ \Ctx}}
      \TyVarI{\graybox{$\Phi$}$|\Theta|$\graybox{$\Gamma_1$}$\vdash X : \Gamma \rat *$}
    \end{prooftree}
    In the rule from the paper we can only type the last variable in the type
    context.  In our rule we just look up the variable in the context.  This
    rules can check the same thing if we take the weakening rules into account.
    With them we can just weaken the context until we get to the desired
    variable.

*** Type instantiation
    The rule
    \begin{prooftree}
      \AxiomC{$\Theta|\Gamma_1\vdash A:(x:B,\Gamma_2)\rat*$}
      \AxiomC{$\Gamma_1\vdash t:B$}
      \TyInst{$\Theta|\Gamma_1\vdash A@t:\Gamma_2[t/x]\rat*$}
    \end{prooftree}
    gets rewritten to
     \begin{prooftree}
      \AxiomC{\graybox{$\Phi$}$|\Theta|\Gamma_1\vdash A:(x:B,\Gamma_2)\rat*$}
      \AxiomC{\graybox{$\Phi|\Theta$}$|\Gamma_1\vdash t:$\graybox{$B'$}}
      \AxiomC{\graybox{$B\equiv_\beta B'$}}
      \TyInstTrinary{\graybox{$\Phi$}$|\Theta|\Gamma_1\vdash A@t:\Gamma_2[t/x]\rat*$}
    \end{prooftree}
    For this rule we have to check if $t$ has the expected type for the first
    variable in the context of $A$.  In our version we just infer the type for $A$ and $t$.
    Then we check if the first variable in the context is beta-equal to the type
    of $t$.  If that isn't the case type checking fails.  Otherwise we just
    substitute in the remaining context.

*** Parameter abstraction
    The rule
    \begin{center}
      \AxiomC{$\Theta|\Gamma_1,x:A\vdash B:\Gamma_2\rat*$}
      \ParamAbstr{$\Theta|\Gamma_1\vdash(x).B:(x:A,\Gamma_2)\rat*$}
      \DisplayProof
    \end{center}
    gets rewritten to
    \begin{center}
      \AxiomC{\graybox{$\Phi$}|$\Theta|\Gamma_1,x:A\vdash B:\Gamma_2\rat*$}
      \ParamAbstr{\graybox{$\Phi$}$|\Theta|\Gamma_1\vdash(x$\graybox{$:A$}$).B:(x:A,\Gamma_2)\rat*$}
      \DisplayProof
    \end{center}
    Here we just at the argument of the lambda to the expression context.  Then
    we check the body of the lambda.  In the syntax directed version we have to
    annotate the variable with its type, so we know which type we have to add to
    the context.

*** (co)inductive types
    We have to separate the rule
    \begin{prooftree}
    \AxiomC{$\sigma_k:\Gamma_k\triangleright\Gamma$}
    \AxiomC{$\Theta,X:\Gamma\rat*|\Gamma_k\vdash A_k:*$}
    \FPTy
    \BinaryInfC{$\Theta | \emptyset \vdash \rho(X : \Gamma \rat *;\vv{\sigma};\vv{A}):\Gamma\rat *$}
    \end{prooftree}
    into multiple rules.  First we need rules to check the definitions of
    (co)inductive types.  This are
    \begin{prooftree}
    \AxiomC{$\sigma_k:\Gamma_k\triangleright\Gamma$}
    \AxiomC{\graybox{$\Phi$}$|X:\Gamma\rat*|\Gamma_k\vdash A_k:*$}
    \AxiomC{\graybox{$\vdash \phi$ \ParCtx}}
    \FPTy
    \TrinaryInfC{$\vdash$ data X<$\Phi$> $\Gamma$ -> Set where; $\vv{Constr_k : \Gamma_k\text{ -> }A_k\text{ -> }X \sigma_k}$}
    \end{prooftree}
    and
    \begin{prooftree}
    \AxiomC{$\sigma_k:\Gamma_k\triangleright\Gamma$}
    \AxiomC{\graybox{$\Phi$}$|X:\Gamma\rat*|\Gamma_k\vdash A_k:*$}
    \AxiomC{\graybox{$\vdash \phi$ \ParCtx}}
    \FPTy
    \TrinaryInfC{$\vdash$ codata X<$\Phi$> : $\Gamma$ -> Set where; $\vv{Destr_k : \Gamma_k \text{ -> } X \sigma_k \text{ -> } A_k}$}
    \end{prooftree}
    Because we only allow top level definitions of (co)inductive types our rules
    have empty contexts.  We first have to check if $\sigma_k$ is  a context
    morphism from $\Gamma_k$ to $\Gamma$.  This basically means that the terms
    in $\sigma_k$ are of the types in $\Gamma$, if we check them in $\Gamma_k$.
    After that we have to check if the $\vv{A}$ (the arguments where we can have
    a recursive occurrence) are of kind $*$.  Because this is a top level
    definition the context $\phi$ is provided by the code.  So we have to check
    if it is valid.  We will now have to rewrite the rules for context morphism.
    Here we just add the parameter context to the rules of the paper.
    \begin{center}
    \AxiomC{}
    \UnaryInfC{\graybox{$\Phi\vdash$}$() : \Gamma_1 \triangleright \emptyset$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{\graybox{$\Phi\vdash$}$\sigma : \Gamma_1 \triangleright \Gamma_2$}
    \AxiomC{\graybox{$\Phi|$}$\Gamma_1\vdash t : A[\sigma]$}
    \BinaryInfC{\graybox{$\Phi\vdash$}$(\sigma,t):\Gamma_1\triangleright(\Gamma_2,x:A)$}
    \DisplayProof
    \end{center}
    We also need a rule for the cases in which we are using this defined
    variables.  This is.
    \begin{prooftree}
    \AxiomC{$\Phi|\Theta|\Gamma'\vdash \vv{A}:\Gamma_i \rat *$}
    \UnaryInfC{$\Phi|\Theta|\Gamma'\vdash X<\vv{A}> : \Gamma[\vv{A}]\rat *$}
    \end{prooftree}
    Here X is a data or codata definition.  The parser can decide if a variable
    is a such an definition or a local definition. Because we are type checking
    on the abstract syntax tree we also know $\Gamma$ and $\Phi'$. $\Gamma$ is
    just the context from the definition and $\Phi$ is the parameter context.
    Because we already typed checked this definition we just have to check if
    the types given for the parameters have the right kind.  Then we substitute
    this parameters in its type.  We will now give the rules for checking if a
    list of parameters matches a parameter context.
    \begin{center}
    \AxiomC{}
    \UnaryInfC{$\Phi|\Theta|\Gamma\vdash () : ()$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\Phi|\Theta|\Gamma\vdash A : \Gamma'\rat*$}
    \AxiomC{$\Phi|\Theta|\Gamma\vdash \vv{A} : \Phi'[A/X]$}
    \BinaryInfC{$\Phi|\Theta|\Gamma\vdash A,\vv{A} : (X:\Gamma'\rat*,\Phi'$)}
    \DisplayProof
    \end{center}
    We just check every variable for the kinds in $\Phi'$ one after the other.
    We also have to substitute the type into the context.  Because kinds in
    a parameter context can depend on variables previously defined in this context.

*** Unit expression introduction
    The rule
    \begin{center}
      \AxiomC{}
      \topI{$\vdash\lozenge:\top$}
      \DisplayProof
    \end{center}
    get rewritten to
    \begin{center}
      \AxiomC{}
      \topI{\graybox{$\Phi|\Theta|\Gamma$}$\vdash$():Unit}
      \DisplayProof
    \end{center}
    The unit term always has the unity type as its type.

*** Expression Instantiation
    The rule
    \begin{center}
      \AxiomC{$\Gamma_1\vdash t:(x:A,\Gamma_2)\rat B$}
      \AxiomC{$\Gamma_1\vdash s:A$}
      \RightLabel{\textbf{(Inst)}}
      \BinaryInfC{$\Gamma_1\vdash t@s:\Gamma_2[s/x]\rat B[s/x]$}
      \DisplayProof
    \end{center}
    gets rewritten to
    \begin{center}
      \AxiomC{\graybox{$\Phi|\Theta$}$|\Gamma_1\vdash t:(x:A,\Gamma_2)\rat B$}
      \AxiomC{\graybox{$\Phi|\Theta$}$|\Gamma_1\vdash s:$\graybox{$A'$}}
      \AxiomC{\graybox{$A\equiv_\beta A'$}}
      \RightLabel{\textbf{(Inst)}}
      \TrinaryInfC{\graybox{$\Phi|\Theta$}$|\Gamma_1\vdash t@s:\Gamma_2[s/x]\rat B[s/x]$}
      \DisplayProof
    \end{center}
    This rules are similar to the rules in [[Type instantiation]].  Here we have to
    check(or infer) a term instead of a type.  We also have to substitute $s$ in
    the result type of $t$(in the case of types its always $*$, which obviously
    has no free variables).

*** Expression variable introduction
    The rule
    \begin{center}
      \AxiomC{$\Gamma\vdash A:*$}
      \RightLabel{\textbf{(Proj)}}
      \UnaryInfC{$\Gamma,x:A\vdash x:A$}
      \DisplayProof
    \end{center}
    gets rewritten to
    \begin{center}
      \AxiomC{\graybox{$\Gamma(x)\rightsquigarrow A$}}
      \RightLabel{\textbf{(Proj)}}
      \UnaryInfC{\graybox{$\Phi|\Theta|$}$\Gamma\vdash x:A$}
      \DisplayProof
    \end{center}
    This works analog to [[Type Variable introduction]].  Here we just look up a
    expression variable in the expression variable context.

*** Constructor
    The rule
    \begin{center}
      \AxiomC{$\mu(X:\Gamma\rat*;\vv{\sigma};\vv{A}):\Gamma\rat*$}
      \AxiomC{$1\leq k\leq|\vv{A}|$}
      \IndIBinary{$\vdash\alpha_k^{\mu(X:\Gamma\rat*;\vv{\sigma};\vv{A})}:(\Gamma_k,y:A_k[\mu/X])\rat\mu@\sigma_k$}
      \DisplayProof
    \end{center}
    gets rewritten to
    \begin{center}
      \AxiomC{\graybox{$\Phi|\Theta|\Gamma\vdash \vv{B} : \Phi'$}}
      \IndI{\graybox{$\Phi|\Theta|\Gamma$}$\vdash$Constr\graybox{$<\vv{B}>$}$:(\Gamma_k\graybox{$\graybox{$[\vv{B}]$}$},y:A_k[\mu/X]\graybox{$\graybox{$[\vv{B}]$}$})\rat\mu@\sigma_k\graybox{$\graybox{$[\vv{B}]$}$}$}
      \DisplayProof
    \end{center}
    We just have to check the parameters.  Every term we need is in the haskell
    representation of the constructor.  The constructor has the type which we
    have defined in the data definition.  We just substitute the type itself for
    the free variable.  At last we need to substitute the parameters for the
    respective variables.

*** Destructor
    The rule
    \begin{center}
      \AxiomC{$\nu(X:\Gamma\rat*;\vv{\sigma};\vv{A}):\Gamma\rat*$}
      \AxiomC{$1\leq k\leq|\vv{A}|$}
      \RightLabel{\textbf{(Coind-E)}}
      \BinaryInfC{$\vdash\xi_k^{\nu(X;\Gamma\rat*;\vv{\sigma};\vv{A})}:(\Gamma_k,y:\nu@\sigma_k)\rat
        A_k[\nu/X]$}
      \DisplayProof
    \end{center}
    gets rewritten to
    \begin{center}
      \AxiomC{\graybox{$\Phi|\Theta|\Gamma\vdash \vv{B} : \Phi'$}}
      \RightLabel{\textbf{(Ind-I)}}
      \UnaryInfC{\graybox{$\Phi|\Theta|\Gamma$}$\vdash$Destr\graybox{$<\vv{B}>$}$:(\Gamma_k$\graybox{$[\vv{B}]$}$,y:\nu@\sigma_k)$\graybox{$[\vv{B}]$}$\rat
        A_k[\nu/X]$\graybox{$[\vv{B}]$}$$}
      \DisplayProof
    \end{center}
    This works analog to [[Constructor]].

*** Recursion
    \begin{center}
      \AxiomC{$\vdash C:\Gamma\rat*$}
      \AxiomC{$\Delta,\Gamma_k,y_k:A_k[C/X]\vdash g_k:(C@\sigma_k)$}
      \AxiomC{$\forall k=1,\dots,n$}
      \RightLabel{\textbf{(Ind-E)}}
      \TrinaryInfC{$\Delta\vdash$ rec
        $\vv{(\Gamma_k,y_k).g_k}:(\Gamma,y:\mu@id_\Gamma)\rat C@id_\Gamma$}
      \DisplayProof
    \end{center}

    \begin{prooftree}
      \AxiomC{$\vdash C:\Gamma\rat*$}
      \AxiomC{\graybox{$\vdash\Gamma\equiv_\beta \Gamma'[\vv{D}]$}}
      \noLine
      \UnaryInfC{\graybox{$\vv{\vdash B_k\equiv_\beta(C@\sigma_k[\vv{D}])}$}}
      \AxiomC{\graybox{$\Phi|\Theta|\Delta\vdash \vv{D}:\Phi'$}}
      \noLine
      \UnaryInfC{$\vv{$\graybox{$\Phi||$}$\Delta,\Gamma_k$\graybox{$[\vv{D}]$}$,y_k:A_k$\graybox{$[\vv{D}]$}$[C/X]\vdash g_k:\text{\graybox{$B_k$}}}$}
      \RightLabel{\textbf{(Ind-E)}}
      \TrinaryInfC{\graybox{$\Phi|\Theta|$}$\Delta\vdash$ rec \graybox{$\mu<\vv{D}>$ to C};
        $\vv{\text{Constr}_k\vv{x_k}\text{ } y_k = g_k}:(\Gamma,y:\mu$\graybox{$[\vv{D}]$}$@id_\Gamma)\rat C@id_\Gamma$}
     \end{prooftree}

     We are recursing over some previously inductive defined type $\mu$ to some
     type $C$.  This types must have the same context.  Recursing is done by
     listing each constructor with the result which the hole expression should
     have if we apply it to this constructor.  This result can refer to the
     arguments of the constructor via the variables $\vv{x_k},y_k$.  The type
     must be the result type $C$ applied to the $\sigma_k$ of this constructor.
     In the syntax directed version we also have to check the parameters.  We
     check if the types match by inferring them and compare them on beta
     equality.

*** Corecursion
    \begin{center}
      \AxiomC{$\vdash C:\Gamma\rat*$}
      \AxiomC{$\Delta,\Gamma_k,y_k:(C@\sigma_k)\vdash g_k:A_k[C/X]$}
      \AxiomC{$\forall k=1,\dots,n$}
      \RightLabel{\textbf{(Coind-I)}}
      \TrinaryInfC{$\Delta\vdash$ corec
        $\vv{(\Gamma_k,y_k).g_k}:(\Gamma,y:C@id_\Gamma)\rat \nu@id_\Gamma$}
      \DisplayProof
    \end{center}

    \begin{prooftree}
      \AxiomC{$\vdash C:\Gamma\rat*$}
      \AxiomC{\graybox{$\vdash\Gamma\equiv_\beta \Gamma'[\vv{D}]$}}
      \noLine
      \UnaryInfC{\graybox{$\vv{\vdash B_k\equiv_\beta A_k[\vv{D}][C/X]}$}}
      \AxiomC{\graybox{$\Phi|\Theta|\Delta\vdash \vv{D}:\Phi'$}}
      \noLine
      \UnaryInfC{$\vv{$\graybox{$\Phi||$}$\Delta,\Gamma_k$\graybox{$[\vv{D}]$}$,y_k:(C@\sigma_k$\graybox{$[\vv{D}]$}$)\vdash g_k:\text{\graybox{$B_k$}}}$}
      \RightLabel{\textbf{(Coind-I)}}
      \TrinaryInfC{\graybox{$\Phi|\Theta|$}$\Delta\vdash$ corec \graybox{C to $\nu<\vv{D}>$};
        $\vv{\text{Destr}_k\vv{x_k}\text{ } y_k = g_k}:(\Gamma,y:C@id_\Gamma)\rat \nu$\graybox{$[\vv{D}]$}$@id_\Gamma$}
     \end{prooftree}

     A corecursion produces an coinductive type $\nu$.  We have to give it a
     type $C$ and list the destructors together with they should be destruct to.
     We get the syntax directed rule analog as in [[Recursion]].

** Evaluation
   #+name: reduction-steps
   \begin{figure}
     \begin{align*}
       ((x).A) @ t \longrightarrow_p A[t/x]\\
       \rec \vv{(\Gamma_k,y_k).g_k}@(\sigma_k\bullet\tau)@(\alpha_k@\tau@u)\succ g_k\left[ \hat{A_k}(\rec\vv{(\Gamma_k,y_k).g_k}@\id{\Gamma}@x)/y_k \right][\tau,u]\\
       \xi_k@\tau@(\corec \vv{(\Gamma_k,y_k).g_k}@(\sigma_k\bullet\tau)@u)\succ \hat{A_k}(\corec\vv{(\Gamma_k,y_k).g_k}@\id{\Gamma}@x)[g_k/x] [\tau,u]
     \end{align*}
   \caption{Reduction steps}
   \end{figure}
   Their are 3 kinds of reduction steps in this system. There are given in
   figure [[reduction-steps]]. The implementation of this is in ~Eval.hs~. One is
   standard beta reduction on the type level. If we apply a lambda to a term we
   substitute the term for the binding variable in the body. This body is then
   the result of the reduction. The other two are reductions on the term level,
   for the (co)inductive types. Here $\sigma_k \bullet\tau$ is a context
   morphism, where we first substitute with $\tau$ and then with $\sigma_k$. If
   we apply a recursion to this context morphism and a constructor, which is
   fully applied, we lookup the case for this constructor. In this case we
   substitute $\tau$ for the variables from $\Gamma_k$ and $u$, where we apply
   the recursion to all recursive occurrences, for $y_k$. So a recursion is
   destructing an inductive type and all its recursive occurrences to another
   type, while we use different cases for the different constructors of the
   type. On the contrary corecurison is constructing a coinductive type. If we
   apply a destructor on such a corecursion, we are taking the case of this
   destructor. In this case we are applying the corecurison to all recursive
   occurrences. $\tau$ and $u$ are substituted as in recursion. The type action
   is responsible for the applying to the recursive occurrences. The variables
   from $\id{\Gamma}$ get substituted by the type action.  The type action is
   implemented in the module ~TypeAction.hs~.  Both the type action and the
   evaluation are done in the ~Eval~ monad.  This monad has access to the
   previously defined statements. We will now define the type action.

   \begin{definition}
     Let $n \in \mathbb{N}$ and $1 \leq i \leq n$.
     Let:
     \begin{align*}
       X_1 : \Gamma_1 \rat \ast,\ldots,X_n : \Gamma_n \rat \ast\ |\ \Gamma' \vdash C : \Gamma \rat \ast \\
       \Gamma_i \vdash A_i : \ast \\
       \Gamma_i \vdash B_i : \ast \\
       \Gamma_i, x : A_i \vdash t_i : B_i
     \end{align*}
     Then we define the type action on terms inductively over $C$
     \begin{align*}
       \begin{array}{ll}
         \widehat{C}(\vv{t},t_{n+1}) = \widehat{C}(\vv{t})
         &\text{for \textbf{(TyVarWeak)}}\\
         \widehat{X_i}(\vv{t})=t_i\\
         \widehat{C'@s}(\vv{t})=\widehat{C'}(\vv{t})[s/y],
         &\text{for }\Theta\mid\Gamma'\vdash C':(y,\Gamma)\rat*\\
         \widehat{(y).C'}(\vv{t})=\widehat{C'}(\vv{t}),
         &\text{for }\Theta\mid(\Gamma',y)\vdash C':\Gamma\rat*\\
         \widehat{\mu(Y:\Gamma\rat*;\vv{\sigma};\vv{D})} =\text{rec}^{R_A}\vv{(\Delta_k,x).g_k}@\id{\Gamma}@x
         &\text{for } \Theta,Y:\Gamma\rat*\mid\Delta_k\vdash D_k:*\\
         \quad\text{with } g_k = \alpha_k^{R_B}@\id{\Delta_k}@\left(\widehat{D_k}(\vv{t},x)\right)\\
         \quad\text{and } R_A=\mu(Y:\Gamma\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).A}/\vv{X}])\\
         \quad\text{and } R_B=\mu(Y:\Gamma\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).B}/\vv{X}])\\
         \widehat{\nu(Y:\Gamma\rat*;\vv{\sigma};\vv{D})} =\text{corec}^{R_B}\vv{(\Delta_k,x).g_k}@\id{\Gamma}@x
         &\text{for } \Theta,Y:\Gamma\rat*\mid\Delta_k\vdash D_k:*\\
         \quad\text{with } g_k = \widehat{D_k}(\vv{t},x)[(\xi_k^{R_A}@\id{\Delta_k}@x)/x]\\
         \quad\text{and } R_A=\mu(Y:\Gamma\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).A}/\vv{X}])\\
         \quad\text{and } R_B=\mu(Y:\Gamma\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).B}/\vv{X}])\\
       \end{array}
     \end{align*}
     And the type action on types as follow
     \begin{equation*}
       \hat{C}(\vv{A})=C[\vv{(\Gamma_i).A}/\vv{X}]@\id{\Gamma}
     \end{equation*}
   \end{definition}
   The type action generates a term with a free variable $x$.  In the type of
   this term we have changed all the free variables to the types of $\vv{t}$.
   Therefore the following holds
   \begin{prooftree}
     \AxiomC{$X : \Gamma_1\rat*|\Gamma_2'\vdash C:\Gamma_2\rat*$}
     \AxiomC{$\Gamma_1, x:A\vdash t:B$}
     \BinaryInfC{$\Gamma_2',\Gamma_2,x:\hat{C}(\vv{A})\vdash\hat{C}(\vv{t}):\hat{C}(\vv{B})$}
   \end{prooftree}

   #+NAME: abstrid
   #+begin_theorem
    $(\Gamma).A@\id{\Gamma}\leftrightarrow_T A$
   #+end_theorem
   #+begin_proof
     We show this by induction on the length of $\Gamma$
     + $\Gamma=\epsilon$:
       \begin{equation*}
          A \longleftrightarrow_T A
       \end{equation*}
     + $\Gamma=x:B,\Gamma'$:
       \begin{equation*}
         (x:B,\Gamma').A@x@\id{\Gamma'}
         \longrightarrow_p(\Gamma').A@\id{\Gamma'}[x/x]
         = (\Gamma').A@\id{\Gamma'} \overset{IdH.}{\longleftrightarrow_T}A
       \end{equation*}
   #+end_proof
   #+NAME: ctxconv
   #+begin_theorem
    The following rule holds
    \begin{prooftree}
    \AxiomC{$x:A\vdash t:B$}
    \AxiomC{$A\longleftrightarrow_TA'$}
    \BinaryInfC{$x:A'\vdash t:B$}
    \end{prooftree}
   #+end_theorem
   #+begin_proof
     We show this by induction on t
   #+end_proof
   #+begin_theorem
   The typing rule (5) in the paper holds
   \begin{prooftree}
     \AxiomC{$X:\Gamma_1\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
     \AxiomC{$\Gamma_1,x:A\vdash t:B$}
     \BinaryInfC{$\Gamma',\Gamma,x:\widehat{C}(A)\vdash\widehat{C}(t):\widehat{C}(B) $}
   \end{prooftree}
   #+end_theorem
   #+begin_proof
   First we will generalize the rule to
   \begin{prooftree}
     \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
     \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
     \BinaryInfC{$\Gamma',\Gamma,x:\widehat{C}(\vv{A})\vdash\widehat{C}(\vv{t}):\widehat{C}(\vv{B}) $}
   \end{prooftree}
   Then we gonna show it by Induction on the derivation $\mathcal{D}$ of $C$
   +
     #+begin_export latex
       $\mathcal{D}$ =
         \AxiomC{}
         \topI{$\top:*$}
         \DisplayProof
     #+end_export

     Then the type actions got calculated as follows
     \begin{align*}
       &\widehat{\top}(\vv{A}) = \widehat{\top}() = \top\\
       &\widehat{\top}(\vv{t}) = \widehat{\top}() = x\\
       &\widehat{\top}(\vv{B}) = \widehat{\top}() = \top
     \end{align*}
     We than got the following prooftree
     \begin{prooftree}
       \AxiomC{$\vdash\top:*$}
       \RightLabel{\textbf{(Proj)}}
       \UnaryInfC{$x:\top\vdash x:\top$}
     \end{prooftree}
   +
     #+begin_export latex
       $\mathcal{D}$ =
         \Di{1}
         \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_{n-1}:\Gamma_{n-1}$\TyCtx}
         \Di{2}
         \UnaryInfC{$\Gamma_n$\Ctx}
         \TyVarI{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\emptyset\vdash X_n:\Gamma_n\rat*$}
         \DisplayProof
     #+end_export

     Again we calculate the type actions
     \begin{align*}
       &\widehat{X_n}(\vv{A}) = X_n[\vv{(\Gamma_i).A}/\vv{X}]@\id{\Gamma_n}= X_n[(\Gamma_n).A_n/X_n]@\id{\Gamma_n} = (\Gamma_n).A_n@\id{\Gamma_n}\\
       &\widehat{X_n}(\vv{t}) = t_n\\
       &\widehat{X_n}(\vv{B}) = X_n[\vv{(\Gamma_i).B}/\vv{X}]@\id{\Gamma_n}= X_n[(\Gamma_n).B_n/X_n]@\id{\Gamma_n} = (\Gamma_n).B_n@\id{\Gamma_n}\\
     \end{align*}
     We know from the first premise that $\Gamma=\Gamma_n$ and $\Gamma'=\emptyset$

     Here we got the prooftree
     \begin{prooftree}
     \AxiomC{$\Gamma_n,x:A\vdash t:B$}
     \AxiomC{}
     \RightLabel{Thrm. \ref{abstrid}}
     \UnaryInfC{$A\longleftrightarrow_T(\Gamma_n).A@\id{\Gamma_n}$}
     \RightLabel{Thrm. \ref{ctxconv}}
     \BinaryInfC{$\Gamma_n,x:(\Gamma_n).A@\id{\Gamma_n}\vdash t:B$}
     \AxiomC{}
     \RightLabel{Thrm. \ref{abstrid}}
     \UnaryInfC{$B\longleftrightarrow_T(\Gamma_n).B@\id{\Gamma_n}$}
     \RightLabel{Conv}
     \BinaryInfC{$\Gamma_n,x:(\Gamma_n).A@\id{\Gamma_n}\vdash t_n:(\Gamma_n).B@\id{\Gamma_1}$}
     \end{prooftree}

   +
     #+begin_export latex
     $\mathcal{D}$ =
       \Di{1}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\mid\Gamma'\vdash C:\Gamma\rat*$}
       \Di{2}
       \UnaryInfC{$\Gamma_n$\Ctx}
       \TyVarWeak{$X_1:\Gamma_1\rat*,\dots,X_{n+1}:\Gamma_{n+1}\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
       \DisplayProof
     #+end_export

     Here we got the prooftree
     \begin{prooftree}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_{n+1}:\Gamma_{n+1}\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
       \RightLabel{(*)}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
       \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
       \RightLabel{IdH.}
       \BinaryInfC{$\Gamma',\Gamma,x:\underbrace{\widehat{C}(\vv{A})}_{\overset{(**)}{=}\widehat{C}(\vv{A},A_{n+1})}\vdash\underbrace{\widehat{C}(\vv{t})}_{\overset{(***)}{=}\widehat{C}(\vv{t},t_{n+1})}:\underbrace{\widehat{C}(\vv{B})}_{\overset{(**)}{=}\widehat{C}(\vv{B},B_{n+1})} $}
     \end{prooftree}

     (=*=) Here we undo *(TyVar-Weak)*

     (=**=) $X_{n+1}$ doesn't occur free in C, otherwise $\mathcal{D}_1$ wouldn't be possible

     (=***=) Case for *(TyVar-Weak)* of type actions on terms

   +
     #+begin_export latex
     $\mathcal{D}$ =
       \Di{1}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\mid\Gamma'\vdash C:\Gamma\rat*$}
       \Di{2}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\mid\Gamma'\vdash D:*$}
       \TyWeak{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma',y:D\vdash C:\Gamma\rat*$}
       \DisplayProof
     #+end_export

     Here we got the prooftree
     \begin{scprooftree}{0.6}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma',y:D\vdash C:\Gamma\rat*$}
       \RightLabel{(*)}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma'\vdash C:\Gamma\rat*$}
       \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
       \RightLabel{IdH.}
       \BinaryInfC{$\Gamma',\Gamma,x:\widehat{C}(\vv{A})\vdash\widehat{C}(\vv{t}):\widehat{C}(\vv{B})$}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\mid\Gamma'\vdash D:*$}
       \TermWeak{$\Gamma',\Gamma,x:\widehat{C}(\vv{A})y\vdash\widehat{C}(\vv{t}):\widehat{C}(\vv{B})$}
     \end{scprooftree}

     (=*=) Here we undo *(Ty-Weak)*

   +
     #+begin_export latex
     $\mathcal{D}= $
     \AxiomC{$X_1:\Gamma_1,\ldots,X_n:\Gamma_n\mid\Gamma'\vdash C':(y:D,\Gamma)\rat* $}
     \AxiomC{$\Gamma'\vdash s: D$}
     \TyInst{$X_1:\Gamma_1,\ldots,X_n:\Gamma_n\mid\Gamma'\vdash C'@s:\Gamma\rat* $}
     \DisplayProof
     #+end_export

     Then we got the following induction hypothesis
     \begin{prooftree}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma'\vdash C':(y:D,\Gamma)\rat*$}
       \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
       \BinaryInfC{$\Gamma',y:D,\Gamma,x:\widehat{C'}(\vv{A})\vdash\widehat{C'}(\vv{t}):\widehat{C'}(\vv{B}) $}
     \end{prooftree}

     Calculated type actions:
     \begin{align*}
       &\widehat{C'@s}(\vv{A})=C'@s[\vv{(\Gamma_i).A}/\vv{X}]@\id{\Gamma}=C'[\vv{(\Gamma_i).A}/\vv{X}]@s@\id{\Gamma}
       =\widehat{C'}(\vv{A})[s/y]\\
       &\widehat{C'@s}(\vv{t})=\widehat{C'}(\vv{t})[s/y]\\
       &\widehat{C'@s}(\vv{B})=C'@s[\vv{(\Gamma_i).B}/\vv{X}]@\id{\Gamma}=C'[\vv{(\Gamma_i).B}/\vv{X}]@s@\id{\Gamma}
       =\widehat{C'}(\vv{B})[s/y]\\
     \end{align*}

     We then got the following prooftree
     \begin{prooftree}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n\rat*\mid\Gamma_2'\vdash C'@s:\Gamma_2[s/y]\rat*$}
       \RightLabel{(*)}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma_2'\vdash C':(y:D,\Gamma_2)\rat*$}
       \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
       \RightLabel{IdH.}
       \BinaryInfC{$\Gamma_2',y:D,\Gamma_2,x:\widehat{C'}(\vv{A})\vdash\widehat{C'}(\vv{t}):\widehat{C'}(\vv{B}) $}
       \UnaryInfC{$\Gamma_2',\Gamma_2[s/y],x:\widehat{C'}(\vv{A})[s/y]\vdash\widehat{C'}(\vv{t})[s/y]:\widehat{C'}(\vv{B})[s/y] $}
     \end{prooftree}
     (=*=) This is the reverse of *(Ty-Inst)*.

   +
     #+begin_export latex
     $\mathcal{D}= $
     \AxiomC{$X_1:\Gamma_1,\ldots,X_n:\Gamma_n\mid\Gamma',y:D\vdash C':\Gamma\rat* $}
     \ParamAbstr{$X_1:\Gamma_1,\ldots,X_n:\Gamma_n\mid\Gamma'\vdash (y).C':(y:D,\Gamma)\rat* $}
     \DisplayProof

     #+end_export

     Calculated type actions:
     \begin{align*}
       \widehat{(y).C'}(\vv{A})&=(y).C'[\vv{(\Gamma_i.A)}/\vv{X}]@\id{\Gamma}\\
                          &=(y).(C'[\vv{(\Gamma_i.A)}/\vv{X}])@y@\id{\Gamma}\\
                          &\longleftrightarrow_T(C'[\vv{(\Gamma_i.A)}/\vv{X}])@\id{\Gamma}\\
                          &=\widehat{C'}(\vv{A})\\
       \widehat{(y).C'}(\vv{t})&=\widehat{C'}(\vv{t})\\
       \widehat{(y).C'}(\vv{B})&=(y).C'[\vv{(\Gamma_i.B)}/\vv{X}]@\id{\Gamma}\\
                          &=(y).(C'[\vv{(\Gamma_i.B)}/\vv{X}])@y@\id{\Gamma}\\
                          &\longleftrightarrow_T(C'[\vv{(\Gamma_i.B)}/\vv{X}])@\id{\Gamma}\\
                          &=\widehat{C'}(\vv{B})\\
     \end{align*}

     The prooftree then becomes the following
     \begin{prooftree}
       \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma'\vdash (y).C':(y:D,\Gamma)\rat*$}
       \RightLabel{(*)}
       \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid y:D,\Gamma'\vdash C':\Gamma\rat*$}
       \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
       \RightLabel{IdH.}
       \BinaryInfC{$y:D,\Gamma',\Gamma,x:\widehat{C'}(\vv{A})\vdash\widehat{C'}(\vv{t}):\widehat{C'}(\vv{B})$}
     \end{prooftree}
     (=*=) This is the reverse of *(Param-Abstr)*.

   +
      $\mathcal{D}$ =
              \begin{prooftree}
                  \Di{1}
                  \UnaryInfC{$\sigma_k:\Delta_k\triangleright\Gamma$}
                  \Di{2}
                  \UnaryInfC{$X_1:\Gamma_1\rat*,\dots,X_n\rat*,X:\Gamma\rat*\vdash A_k:*$}
                  \FPTy
                  \BinaryInfC{$\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}):\Gamma\rat*$}
              \end{prooftree}

          Calculated type actions:
          \begin{align*}
            &\widehat{\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{A})\\
            &=\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})[\vv{(\Gamma_1).A}/\vv{X}]@\id{\Gamma_2}\\
            &=\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_1).A}/\vv{X}])@\id{\Gamma_2}\\
            &\widehat{\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{t})\\
            &=\text{rec}^{\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[(\Gamma_1).A/X])}\vv{(\Delta_k,x).\alpha_k@\id{\Delta_k}@\widehat{D_k}(\vv{t},x)}@\id{\Gamma_2}@x\\
            &\widehat{\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{B})\\
            &=\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})[\vv{(\Gamma_1).B}/\vv{X}]@\id{\Gamma_2}\\
            &=\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_1).B}/\vv{X}])@\id{\Gamma_2}
          \end{align*}

         From the assumptions
          \begin{align*}
          &X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\emptyset\vdash \mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}):\Gamma_2\rat*\\
          &\Gamma_i,x:A_i\vdash t_i:B_i
          \end{align*}
         We have to proof that in *Ctx*
         \begin{equation*}
          \Gamma_2,x:\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[(\Gamma_1).A/X])@\id{\Gamma_2}
         \end{equation*}
         the expression
         \begin{equation*}
          \text{rec}^{\mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).A}/\vv{X}])}\vv{(\Delta_k,y).\alpha_k@\id{\Delta_k}@\widehat{D_k}(t,y)}@\id{\Gamma_2}@x
         \end{equation*}
         has type
         \begin{equation*}
         \mu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).B}/\vv{X}])@\id{\Gamma_2}
         \end{equation*}
         We can use the induction hypothesis
         \begin{prooftree}
           \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*,Y:\Gamma_{n+1}\rat*\mid\Delta_k\vdash D_k:*$}
           \AxiomC{$\Gamma_i,x:A_i\vdash t_i:B_i$}
           \BinaryInfC{$\Delta_k,x:\widehat{D_k}(\vv{A},A_{n+1})\vdash\widehat{D_k}(\vv{t},y):\widehat{D_k}(\vv{B},B_{n+1}) $}
         \end{prooftree}
    We than got the following proof
    \begin{prooftree}
     \AxiomC{$\Gamma_2,x:\widehat{C}(\vv{A}),\Delta_k,y_k:D_k[\mu/X]\vdash\widehat{D_k}(\vv{t},y):D_k[\vv{(\Gamma_i).B}/\vv{X}][(\Gamma_{n+1}).B_{n+1}/Y]$}
     \UnaryInfC{$\Gamma_2,x:\widehat{C}(\vv{A}),\Delta_k,y_k:D_k[\mu/X]\vdash\alpha_k@\id{\Delta_k}@\widehat{D_k}(\vv{t},y):\mu@\sigma_k$}
     \UnaryInfC{$\Gamma_2,x:\widehat{C}(\vv{A})\vdash\widehat{C}(t):\widehat{C}(\vv{B})$}
    \end{prooftree}

   + $C=\nu(Y:\Gamma\rat*;\vv{\sigma};\vv{D})$:

     Calculated type actions:
     \begin{align*}
       &\widehat{\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{A})\\
       &=\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})[\vv{(\Gamma_i).A}/\vv{X}]@\id{\Gamma_2}\\
       &=\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).A}/\vv{X}])@\id{\Gamma_2}\\
       &\widehat{\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{t})\\
       &=\text{corec}^{\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[(\vv{\Gamma_i).B}/\vv{X}])}\vv{(\Delta_k,x)\widehat{D_k}(\vv{t},x)[(\xi_k@\id{\Delta_k}@x)/x]}@\id{\Gamma_2}@x\\
       &\widehat{\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})}(\vv{B})\\
       &=\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D})[\vv{(\Gamma_i).B}/\vv{X}]@\id{\Gamma_2}\\
       &=\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).B}/\vv{X}])@\id{\Gamma_2}
     \end{align*}

    From the assumptions
     \begin{align*}
     &X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*\mid\Gamma_2'\vdash \nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}):\Gamma_2\rat*\\
     &\Gamma_i,x:A_i\vdash t_i:B_i
     \end{align*}
    We have to proof that in *Ctx*
    \begin{equation*}
     \Gamma_2',\Gamma_2,x:\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[(\Gamma_1).A/X])@\id{\Gamma_2}
    \end{equation*}
    the expression
    \begin{equation*}
     \text{corec}^{\nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[(\vv{\Gamma_i).B}/\vv{X}])}\vv{(\Delta_k,x)\widehat{D_k}(\vv{t},x)[(\xi_k@\id{\Delta_k}@x)/x]}@\id{\Gamma_2}@x\\
    \end{equation*}
    has type
    \begin{equation*}
    \nu(Y:\Gamma_2\rat*;\vv{\sigma};\vv{D}[\vv{(\Gamma_i).B}/\vv{X}])@\id{\Gamma_2}
    \end{equation*}
    We can use the induction hypothesis
    \begin{prooftree}
      \AxiomC{$X_1:\Gamma_1\rat*,\dots,X_n:\Gamma_n\rat*,Y:\Gamma_{n+1}\rat*\mid\Delta_k\vdash D_k:*$}
      \AxiomC{$\Gamma_i,y_k:A_i\vdash t_i:B_i$}
      \BinaryInfC{$\Delta_k,y_k:\widehat{D_k}(\vv{A},A_{n+1})\vdash\widehat{D_k}(\vv{t},y):\widehat{D_k}(\vv{B},B_{n+1}) $}
    \end{prooftree}
    We than got the following proof
    \begin{prooftree}
     \AxiomC{$\Gamma_2',\Gamma_2,x:\widehat{C}(\vv{A}),\Delta_k,y_k:\nu@\sigma_k\vdash\widehat{D_k}(\vv{t},x)[(\xi_k@\id{\Delta_k}@x)/x]:D_k[\vv{(\Gamma_i).A}/\vv{X}][\nu/X]$}
     \UnaryInfC{$\Gamma_2',\Gamma_2,x:\widehat{C}(\vv{A})\vdash\widehat{C}(t):\widehat{C}(\vv{B})$}
    \end{prooftree}

   #+end_proof

* Examples

  In this section we reiterate the example types from the paper.  We use our
  syntax, which is defined in [[Abstract Syntax]].  We will also show some functions
  on this types.  On some of them we will show the reduction steps in detail.

** Terminal Object

   The terminal object is a type which has exactly one value. In category
   theory every object in the category has a unique morphism to it. We define
   it as a coinductive type ~Terminal~ with one destructor ~Terminal~. It gets
   a Terminal and returns a Terminal. To get a Terminal value we use
   corecursion on the unit type, which is the first class terminal object.
   #+begin_example
   codata Terminal : Set where
      Terminal : Terminal -> Terminal
   terminal = corec Unit to Terminal where
                 { Terminal x = x } @ ()

   #+end_example

** Initial Object

   The initial object is a type which has no values. In category theory it is
   the object which has a unique morphism to every other object in the
   category. We define it inductively as ~Intial~ with one constructor
   ~Initial~. This constructor want's one value of the same type. We can't
   have a value of this type, because to get one we already need one. An
   shorter way to define this type would be a inductive type with no
   arguments. If we could get something of type ~Intial~, we could generate
   with ~exfalsum~ a value of arbitrary type ~C~.
   #+begin_example
   data Initial : Set where
      Initial : Intial -> Intial
   exfalsum<C : Set> = rec Initial to C where
                         Initial x = x
   #+end_example

** Natural Numbers

   We use the classical peano numbers to define natural numbers.  Therefor we use
   the inductive type ~Nat~ with the constructors ~Zero~ and ~Suc~. ~Zero~ is
   just the number zero. Every constructor has to have a argument, which can
   contain a recursive occurrence. Every Type ~A~ is isomorphic to the
   function type ~Terminal -> A~. So we use ~Terminal~ for this occurrence.
   ~Suc~ is the successor. So the meaning of ~Suc n~ is $n+1$
   #+begin_example
   data Nat : Set where
      Zero : Terminal -> Nat
      Suc : Nat -> Nat
   zero = Zero @ ()
   #+end_example

** Binary Product

   The product is defined as a coinductive type.  It has two destructors.
   The first gives back the first element.  And the second the second.
   The types A and B have to be concrete types.  We don't have type
   polymorphism in our language.  We also define a pair expression which
   generates a pair over corecursion.
   #+begin_example
   codata Product<A : Set, B : Set> : Set where
      Fst : Product -> A
      Snd : Product -> B
   pair<A : Set, B : Set> (x:A, y:B) = corec Unit where
                                         { Fst _ -> x
                                         ; Snd _ -> y} @ ()
   #+end_example
   For types with other contexts we have to define different Products.  For
   example if ~B~ depends on ~Nat~, we define product like the following.
   #+begin_example
   codata Pair<A : Set, B : (n : Nat) -> Set> : (n : Nat) -> Set where
     First : (n : Nat) -> Pair n -> A
     Second : (n : Nat) -> Pair n -> B @ n
   #+end_example
   Here the product also depends on ~Nat~. If ~A~ or ~B~ depends on valuse the
   product must also depend on this values. This is the product we need for the
   definition of vectors

*** Swap funtion
    On this type we can define the swap function.
    #+begin_example
    swap<A : Set, B : Set> =
      corec Product<A,B> to Product<B,A> where
             Fst x -> Snd x
             Snd x -> Fst x
    #+end_example
    This is a well typed function as shown by the following proof
    \begin{prooftree}
    \AxiomC{$(A : *, B : *)||\vdash$ Product<A,B> : $*$}
    \AxiomC{$(A : *, B : *)||(x:A) \vdash$ Snd @ x : Product<A,B> \textcircled{a}}
    \noLine
    \UnaryInfC{$(A : *, B : *)||(y : B) \vdash$ Fst @ y : Product<A,B> \textcircled{b}}
    \BinaryInfC{$(A : *, B : *)|| \vdash$ swap : (p : Product<A,B>) $\rat$ Product<B,A>}
    \end{prooftree}
    We show \textcircled{a} in the following proof.  \textcircled{b} works analog
    \begin{prooftree}
    \AxiomC{$(A : *, B : *)||(x : A) \vdash$ Snd : $(x : A) \rat$ Product<A,B>}
    \AxiomC{$(x : A )(x)\rightsquigarrow A$}
    \UnaryInfC{$(x : A) \vdash x : A$}
    \BinaryInfC{$(A : *, B : *)||(x : A) \vdash$ Snd @ x : Product<A,B>}
    \end{prooftree}
    For brevity we omitted the beta equality premises and the checking for of
    the parameters. The beta equality premises wouldn't be interesting because
    they all already syntactically identical.

** Binary Coproduct

   The Binary Coproduct corresponds to the Eiher type in haskell.  It is defined
   as an inductive type.  It is either ~A~ or ~B~.  We have one constructor ~Left~
   for ~A~ and one constructor Right for ~B~
   #+begin_example
   data Coproduct<A,B> : Set where
      Left : A -> Coproduct
      Right : B -> Coproduct
   #+end_example

** Pi Type

   The pi type is a generalization of the function type to dependent types.
   The type of the codomain or result of a function can depend on the value
   We define it as a coinductive type.  To destruct a function we just apply
   it to a value.  So the Destructor is ~Apply~.  To construct a function we
   use corecursion on on ~Unit~.  This is a lambda so we call it ~lambda~.

   #+begin_example
   codata Pi<A : Set,B : (x : A) -> Set> : Set where
      Apply : (x : A) -> Pi x -> B
   #+end_example

*** identity function
     The identity function is defined like this
     #+begin_example
     id<A : Set> = corec Unit to Pi<A,(v:A).A> where
            { Apply v p = v } @ ()
     #+end_example

     Evaluation on one goes as follows

    #+begin_example
    apply = Apply<Nat,(v : Nat).Nat>
    one = S @ (Z @ ())
    apply @ id<Nat> @ one
    = apply @ one @ ((corec Unit to Pi<Nat,(x:Nat).Nat> where
                        Apply v p = v ) @ ())
    $\succ \widehat{\text{Nat}} \left(\underbrace{
       \begin{subarray}{c}
         \text{corec Unit to Pi where} \\
         \text{\{Apply' v \_ = v\} @ x}
       \end{subarray}}_t\right)$[v/x][one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ ($\widehat{()}$(t,x))
         Succ x = Suc @ ($\widehat{Y}$(t,x)))@x[v/x][one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ ($\widehat{()}$(t))
         Succ x = Suc @ x)@x[v/x][one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ ($\widehat{()}$())
         Succ x = Suc @ x) @ x[v/x][one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ x
         Succ x = Suc @ x) @ x[v/x][one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ x
         Succ x = Suc @ x) @ v[one,()]
    = (rec Nat to Nat where
         Zero x = Zero @ x
         Succ x = Suc @ x) @ one
    = one
    #+end_example
** Sigma Type

   The sigma type is a dependent pair of two types.  The second type can depend on
   the value of the first type.  It corresponds to to exists in logic.  We define
   it as an inductive type and call the constructor ~Exists~.
   #+begin_example
   data Sigma<A : Set,B : (x : A) -> Set> : Set where
      Exists : (x:A) -> B x -> Sigma
   #+end_example

** Vectors

   Vectors are a standard example for dependent type.  There are like lists, except
   there type depends on there length. For example a vector ~[1;2]~ has type
   ~Vector<Nat> 2~, because it length is 2. It has 2 constructors ~Nil~ and
   ~Cons~ like lists. ~Nil~ gives back the empty vector. Because the length of
   the empty vector is zero its return type is ~Vector 0~. The second
   constructor ~Cons~ takes a natural number ~k~ and a pair. The pair consists
   of ~A~ and a vector of length ~k~, a ~Vector k~. It returns a new vector.
   Its head is the first argument of the pair and its tail the second. So the
   results length is one more then the second argument of the pair. Therefore it
   is ~Vector (Suc k)~

   #+begin_example
   data Vector<A : Set> : (n:Nat) -> Set where
     Nil : Unit -> Vector zero
     Cons : (k:Nat) -> Product<A,Vector @ k> -> Vector (Suc @ k)
   nil<A : Set> = Nil<A : Set> @ ()
   #+end_example

*** Extend Function

    We use a function, which extends a vector to the end of a
    vector, to show how evaluation on a vector works.
    This tail function returns the empty vector for the empty vector,
    because every function has to be total in our language.  To keep
    things simple we use Unit for $A$. We also simplify "Product Unit
    (VectorUnit k)" to just "VectorUnit k"
    #+begin_example
    extend<A : Set>(x : A) =
      rec Vec<A> to ((x).Vec<A> @ (Suc x) where
        Nil u = Cons<A> @ x @ nil<A>
        Cons k v = Cons<A> @ (Suc @ k) @ v
    #+end_example
    The type checking of this function goes as follows
    \begin{scprooftree}{0.8}
    \AxiomC{$\vdash$ (x).(VecUnit @ (Suc @ x)) : (k: Nat)}
    \noLine
    \UnaryInfC{(\_ : Unit) $\vdash$ ConsUnit @ 0 @ (NilUnit' @ ()) : (x).(VecUnit @ (Suc @ x)) @ 0}
    \noLine
    \UnaryInfC{(k : Nat, v : (x).(Vec @ (Suc @ x)) @ k) $\vdash$ ConsUnit @ (Suc @ k) @ v : (x).(Vec @ (Suc @ x)) @ (Suc @ k)}
    \UnaryInfC{$\vdash$ app : (k:Nat,y : (x).Vec (Suc x)) $\rat$ (x).(Vec @ (Suc x)) @ k}
    \end{scprooftree}
    As an example we evaluate a vector of length 1 with this function.  We choose length one
    to see all rec cases.
    \begin{align*}
      &\text{extend}@ 1 @ (\text{ConsUnit} @ 0 @ (\text{NilUnit'} @ ()))\\
      &= \text{extend}@(\text{Suc} @ k \bullet 0) @ (\text{ConsUnit} @ 0 @ (\text{NilUnit'} @ ()))\\
      &\succ \text{ConsUnit} @ (\text{Suc} @ k) @ v \left[ \hat{X}(\text{extend} @ n @ x)/v \right][0,\text{NilUnit'} @ ()]\\
      &= \text{ConsUnit} @ (\text{Suc} @ k) @ v \left[ \text{extend} @ n @ x/v \right][0, \text{NilUnit'} @ ()]\\
      &= \text{ConsUnit} @ (\text{Suc} @ 0) @ (\text{extend} @ n @ x) [0,\text{NilUnit'} @ ()]\\
      &= \text{ConsUnit} @ (\text{Suc} @ 0) @ (\text{extend} @ 0 @ (\text{NilUnit'} @ ()))\\
      &= \text{ConsUnit} @ 1 @ (\text{extend} @ (0 \bullet 0) @ (\text{NilUnit'} @ ()))\\
      &\succ \text{ConsUnit} @ 1 @ (\text{ConsUnit} @ 0 @ (\text{NilUnit'} @ ()))\left[ \hat{()}(\text{extend} @ k @ x) / \_  \right][()]\\
      &= \text{ConsUnit} @ 1 @ (\text{ConsUnit} @ 0 @ (\text{NilUnit'} @ ()))[()]\\
      &= \text{ConsUnit} @ 1 @ (\text{ConsUnit} @ 0 @ (\text{NilUnit'} @ ()))
    \end{align*}

*** replicate function
    The following function gets a number $n$ and returns an vector of units
    with length $n$
    #+begin_example
    length = rec VectorUnit to Nat where
               NilUnit _ = zero
               ConsUnit k _ = Succ @ k
    replicate = lambda_Nat_VectorUnit n ((rec Nat to VectorUnit where
                                            Zero _ = NilUnit @ ()
                                            Suc   m = ConsUnit @ (length @ m) @ m) @ n)
    #+end_example
    The following shows the steps for evaluating /replicate/ on 1.  We omit
    the steps for /length/ and the inner /rec/, because we want to see how
    /corec/ evaluation works.  We will call the /rec/ part in the definition of
    /replicate/ /rep/.

    #+begin_example
    apply (lambda n (rep @ n)) 1
    = Apply @ 1 @ (lampda n (rep @ n))
    = Apply @ 1 @ ((corec Pi to Unit where
                     Apply n _ = rep @ n)@())
    $\succ$ $\widehat{\text{VecUnit}}(\underbrace{\text{corec Pi to Unit where \{ Apply n \_ = rep @ n \} @ x}}_t)$[rep@n/x][1,()]
    = (rec VecUnit to VecUnit where
         VecNil x = VecNil @ $\widehat{()}$(t,x)
         VecCons n x = VecCons @ n @ $\widehat{(y).Y}$(t,x))@n@x[rep@n/x][1,()]
    = (rec VecUnit to VecUnit where
         VecNil x = VecNil @ $\widehat{()}$(t,x)
         VecCons n x = VecCons @ n @ $\widehat{Y}$(t,x))@n@x[rep@n/x][1,()]
    = (rec VecUnit to VecUnit
         VecNil x = VecNil@()
         VecCons n x = VecCons'@n@x)@n@x[rep@n/x][1,()]
    = (rec VecUnit to VecUnit
         VecNil x = VecNil@x
         VecCons n x = VecCons@n@y)@n@(rep@n)[1,()]
    = (rec VecUnit to VecUnit
         VecNil x = VecNil@x
         VecCons n x = VecCons@n@x)@1@(rep@1)
    = ConsUnit @ (NilUnit @ ())
    #+end_example

** Maybe

** Extended Naturals

   We will now define extended naturals.  There are needed for the definitions
   of streams.  There are natural numbers with an additional value, infinty.  We
   define it coinductively with the predecessor as it only destructor.  The predecessor
   is either not defined (there is no predecessor of 0 in the natural numbers) or
   another natural number.  So we use a coproduct of ~Unit~ (which should mean: "has no
   predecessor") and another ~ExNat~.  We can define the successor as a corecursion.
   The predecessor of the successor of ~x~ is just ~x~.  So the only case of corec returns
   a ~Right x~ (remember Prec returns a coproduct not a number).
   #+begin_example
   codata ExNat : Set where
      Prec : ExNat -> Maybe<ExNat>
   succE = corec ExNat where
             Prec x -> Just<ExNat> @ x
   #+end_example

** Streams

   With extended naturals defined, we can now define partial streams.  This are streams
   which depend on there definition depth.  Like non-dependent
   streams they are coinductive and have 2 destructors for head and tail.
   #+begin_example
   codata PStr<A : Set>: (n: ExNat) -> Set where
      hd : (k : ExNat) -> PStr<A> (succE k) -> A
      tl : (k : ExNat) -> PStr<A> (succE k) -> PStr<A> @ k
   #+end_example
** List
   List A describes a list of type elements with type A.  It is defined
   as follows

   \begin{equation*}
   List A =  \mu(X:*;\epsilon_2;(\textbf{1},A\times X))
   \end{equation*}
   where $\Gamma_1=\emptyset$ and $\Gamma_2\vdash A:*$

   In the implemented syntax is written like this
   #+begin_example
   data List<A : Set> : Set where
      Nil : Terminal -> List
      Cons : Product<A,List> -> List
   nil<A : Set> = Nil<A> @ ()
   #+end_example

** Length function on lists of Units
   \begin{align*}
   \text{length} = \text{rec} &((y_k:\top).\alpha_1^\textbf{N}@\langle\rangle\\
                &,(x:\top,y_k:\mu(X:*;\epsilon_2(\mathbf{1},X)))).\alpha_2^\textbf{N} @ y_k\\
   \end{align*}
*** Type checking

    \begin{scprooftree}{0.6}
    \AxiomC{$\vdash\textbf{N}:*$}
    \AxiomC{$\vdash\alpha_1^\textbf{N}: (x:\textbf{1})\rightarrow\textbf{N}$}
    \RightLabel{\textbf{(Term-Weak)}}
    \UnaryInfC{$y_k:\textbf{1}\vdash\alpha_1^\textbf{N}: (x:\textbf{1})\rightarrow\textbf{N}$}
    \AxiomC{$\vdash\langle\rangle':\textbf{1}$}
    \RightLabel{\textbf{(Term-Weak)}}
    \UnaryInfC{$y_k:\textbf{1}\vdash\langle\rangle':\textbf{1}$}
    \RightLabel{\textbf{(Inst)}}
    \BinaryInfC{$y_k:\textbf{1}\vdash \alpha_1^\textbf{N}@\langle\rangle':\textbf{N}$}
    \AxiomC{$\vdash\alpha_2^\textbf{N}: (x:\textbf{N})\rightarrow\textbf{N}$}
    \RightLabel{\textbf{(Term-Weak)}}
    \UnaryInfC{$y_k:\textbf{N}\vdash\alpha_2^\textbf{N}: (x:\textbf{N})\rightarrow\textbf{N}$}
    \AxiomC{$\textbf{N}:*$}
    \RightLabel{\textbf{(Proj)}}
    \UnaryInfC{$y_k:\textbf{N}\vdash y_k:\textbf{N}$}
    \RightLabel{\textbf{(Inst)}}
    \BinaryInfC{$y_k:\textbf{N}\vdash \alpha_2^\textbf{N}@y_k:\textbf{N}$}
    \RightLabel{\textbf{(Ind-E)}}
    \TrinaryInfC{$\vdash \text{rec}((y_k).\alpha_1^\textbf{N}@\langle\rangle'
                    ,(y_k).\alpha_2^\textbf{N} @ y_k):(y:\text{List }\textbf{1})\rightarrow\textbf{N}$}
    \end{scprooftree}

** Rose Tree
   Rose Tree A = $\nu(X:*;\epsilon_2;(\textbf{1},List X)$

   #+begin_example
   data RoseTree<A : Set> : Set where
      Leaf : Terminal -> RoseTree
      Branch : List<RoseTree> -> RoseTree
   leaf<A : Set> = Leaf<A> @ ()
   #+end_example

* Conclusion
  We have implemented a depend type theory with inductive and coinductive types.
  In this theory, contrary to coq and agda, coinductive types can also depend on
  values.  The downside is that we don't have universes.  This prevents type
  polymorphism.  Further work needs do be done to solve this.

  Another problem is, that each constructor or destructor has at least one
  argument. The argument with the recursive occurrence. For example we have to
  apply an unit to the constructors of a boolean types. We could allow recursive
  occurrences in the contexts of the constructors and destructors. This makes it
  possible to remove the argument the argument with the recursive occurrence. We
  then have to be change the evaluation rules.

\appendix
* Type action derivation
\begin{landscape}
\begin{changemargin}{-1cm}{-1cm}
  \begin{prooftree}
   \AxiomC{$\Gamma_1\vdash\sigma:\Gamma_2$}
   \AxiomC{$\Gamma_3\vdash\tau:\Gamma_1$}
   \RightLabel{($*$)}
   \BinaryInfC{$\Gamma_3\vdash\sigma\circ\tau:\Gamma_2$}
  \end{prooftree}
  \begin{prooftree}
    \D
    \UnaryInfC{$\Delta,\Gamma_k,y_k:A_k[C/X]\vdash g_k:C@\sigma_k$}
    \RightLabel{TyAct}
    \UnaryInfC{$\Delta,\Gamma_k,x:A_k[\mu/X]\vdash g_k[\widehat{A_k}(\rec^\mu\overline{(\Gamma_k,y_k).g_k}@\id{\Gamma}@x/y_k]$}
    \D
    \UnaryInfC{$\Delta\vdash\tau:\Gamma_k$}
    \D
    \UnaryInfC{$\Delta\vdash u:A_k[\mu/X]$}
    \TrinaryInfC{$\Delta\vdash g_k[\widehat{A_k}(\rec^\mu\overline{(\Gamma_k,y_k).g_k}@\id{\Gamma}@x)/y_k][\tau,u]:C@\sigma_k$}
  \end{prooftree}
\begin{scprooftree}{0.93}
  \D
  \UnaryInfC{$\Delta,\Gamma_k,y_k:\Delta_k[C/X]\vdash g_k: C@\sigma_k$}
  \IndE
  \UnaryInfC{$\Delta\vdash\rec^\mu\overline{(\Gamma_k,y_k).g_k}:(\Gamma,x:\mu@\sigma_k)\rat C@\sigma_k$}
  \D
  \UnaryInfC{$\Gamma_k\vdash\sigma_k:\Gamma$}
  \D
  \UnaryInfC{$\Delta\vdash\tau:\Gamma_k$}
  \RightLabel{($*$)}
  \BinaryInfC{$\Delta\vdash\sigma_k\circ\tau:\Gamma$}
  \Inst{$\Delta\vdash(rec^\mu\overline{(\Gamma_k,y_k).g_k}@(\sigma_k\circ\tau)):(x:\mu@\sigma_k)\rat C@\sigma_k$}
  \AxiomC{}
  \IndI{$\Delta\vdash\alpha_k^\mu:(\Gamma_k,y:A_k[\mu/X])\rat\mu@\sigma_k$}
  \D
  \UnaryInfC{$\Delta\vdash\tau:\Gamma_k$}
  \Inst{$\Delta\vdash\alpha_k^\mu @\tau: (y:A_k[\mu/X])\rat\mu@\sigma_k$}
  \D
  \UnaryInfC{$\Delta\vdash u:A_k[\mu/X])$}
  \Inst{$\Delta\vdash\alpha_k^\mu @\tau@u:\mu@\sigma_k$}
  \Inst{$\Delta\vdash(\rec^\mu\overline{(\Gamma_k,y_k).g_k}@(\sigma_k\circ\tau))@(\alpha_k^\mu @\tau@u):C@\sigma_k$}
\end{scprooftree}
\begin{scprooftree}{0.63}
  \AxiomC{$\vdash\overbrace{\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}^C:\Gamma\rat*$}
  \AxiomC{}
  \IndI{$\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma},\Delta_k,y_k:A_k[\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]/X]\vdash\alpha_k^{\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}:(\Delta_k,y_k:A_k[\overline{(\Gamma_k).B_k}/\overline{X_k}])\rat\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]@\sigma_k$}
  \AxiomC{$\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma},\Delta_k,y_k:\overbrace{A_k[\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]/X]}^{=\widehat{A_k}(\overline{B},?)}\vdash\widehat{A_k}(\overline{t},y_k):\overbrace{A_k[\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]/X]}^{=\widehat{A_k}(\overline{B},?)}$}
  \Inst{$\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma},\Delta_k,y_k:A_k[\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]/X]\vdash\alpha_k^{\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}@\id{\Delta_k}@\widehat{A_k}(\overline{t},y_k):\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]@\sigma_k$}
  \IndE
  \BinaryInfC{$\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma}\vdash\rec^{\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}\overline{(\Delta_k,\overset{{\color{red}?}}{y_k}).\alpha_k^{\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}@\id{\Delta_k}@\widehat{A_k}(\overline{t},\overset{{\color{red}?}}{y_k})}:(\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma})\rat\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]@\id{\Gamma}$}
  \AxiomC{$\ldots$}
  \Inst{$\Gamma,x:\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma}\vdash\rec^{\mu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}\overline{(\Delta_k,\overset{{\color{red}?}}{y_k}).\alpha_k^{\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}@\id{\Delta_k}@\widehat{A_k}(\overline{t},\overset{{\color{red}?}}{y_k})}@\id{\Gamma}@x:\mu[\overline{(\Gamma_k).B_k}/\overline{X_k}]@\id{\Gamma}$}
\end{scprooftree}
\begin{scprooftree}{0.63}
  \AxiomC{$\vdash\overbrace{\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}^C:\Gamma\rat*$}
  \AxiomC{$\Gamma,x:\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma},\Delta_k,\overset{{\color{red}?}}{y_k}:\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\sigma_k\vdash\widehat{A_k}(\overline{t},\overset{{\color{red}?}}{y_k})[(\xi_k^{\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}@\id{\Delta_k}@\overset{{\color{red}?}}{y_k})/\overset{{\color{red}?}}{y_k}]:A_k[\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]/X]$}
  \IndE
  \BinaryInfC{$\Gamma,x:\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma}\vdash\corec^{\nu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}\overline{(\Delta_k,\overset{{\color{red}?}}{y_k}).\widehat{A_k}(\overline{t},\overset{{\color{red}?}}{y_k})[(\xi_k^{\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}@\id{\Delta_k}@\overset{{\color{red}?}}{y_k})/\overset{{\color{red}?}}{y_k}]}:(\Gamma,x:\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma})\rat\nu[\overline{(\Gamma_k).B_k}/\overline{X_k}]$}
  \AxiomC{$\ldots$}
  \Inst{$\Gamma,x:\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]@\id{\Gamma}\vdash\corec^{\nu[\overline{(\Gamma_k).B_k}/\overline{X_k}]}\overline{(\Delta_k,\overset{{\color{red}?}}{y_k}).\widehat{A_k}(\overline{t},\overset{{\color{red}?}}{y_k})[(\xi_k^{\nu[\overline{(\Gamma_k).A_k}/\overline{X_k}]}@\id{\Delta_k}@\overset{{\color{red}?}}{y_k})/\overset{{\color{red}?}}{y_k}]}@\id{\Gamma}@x:\nu[\overline{(\Gamma_k).B_k}/\overline{X_k}]$}
\end{scprooftree}
\end{changemargin}
\end{landscape}


\bibliographystyle{alpha}
\bibliography{references}
